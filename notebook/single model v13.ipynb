{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Single Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "import gc\n",
    "import lightgbm as lgb\n",
    "import xgboost as xgb\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import time\n",
    "import warnings\n",
    "import xlearn as xl\n",
    "import scipy.sparse as sp\n",
    "\n",
    "from contextlib import contextmanager\n",
    "from pandas.core.common import SettingWithCopyWarning\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import KFold, StratifiedKFold\n",
    "import numba\n",
    "import pickle\n",
    "\n",
    "\n",
    "warnings.simplefilter(action='ignore', category=SettingWithCopyWarning)\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "\n",
    "pd.set_option('display.max_rows', 500)\n",
    "pd.set_option('display.max_columns', 500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "@contextmanager\n",
    "def timer(title):\n",
    "    t0 = time.time()\n",
    "    yield\n",
    "    print(\"***{} - done in {:.0f}s\".format(title, time.time() - t0))\n",
    "\n",
    "# rmse\n",
    "@numba.jit\n",
    "def rmse(y_true, y_pred):\n",
    "    return np.sqrt(mean_squared_error(y_true, y_pred))\n",
    "\n",
    "# One-hot encoding for categorical columns with get_dummies\n",
    "def one_hot_encoder(df, nan_as_category = True):\n",
    "    original_columns = list(df.columns)\n",
    "    categorical_columns = [col for col in df.columns if df[col].dtype == 'object']\n",
    "    df = pd.get_dummies(df, columns= categorical_columns, dummy_na= nan_as_category)\n",
    "    new_columns = [c for c in df.columns if c not in original_columns]\n",
    "    return df, new_columns\n",
    "    \n",
    "# Display/plot feature importance\n",
    "def display_importances(feature_importance_df_, model, straified_opt, figsize=(16, 50)):\n",
    "    # cols = feature_importance_df_[[\"feature\", \"importance\"]].groupby(\"feature\").mean().sort_values(by=\"importance\", ascending=False)[:40].index\n",
    "    cols = feature_importance_df_[[\"feature\", \"importance\"]].groupby(\"feature\").mean().sort_values(by=\"importance\", ascending=False).index\n",
    "    best_features = feature_importance_df_.loc[feature_importance_df_.feature.isin(cols)]\n",
    "\n",
    "    plt.figure(figsize=figsize)\n",
    "    sns.barplot(x=\"importance\", y=\"feature\", data=best_features.sort_values(by=\"importance\", ascending=False))\n",
    "    if model == 'lgb':\n",
    "        model_name = 'Lightgbm'\n",
    "    elif model == 'xgb':\n",
    "        model_name = 'Xgboost'\n",
    "    plt.title(model_name + ' Features (avg over folds)')\n",
    "    plt.tight_layout()\n",
    "    if straified_opt:\n",
    "        plt.savefig('../img/single_model_v'+str(write_ver)+'_importances_'+model+'_straified.png')\n",
    "    else:\n",
    "        plt.savefig('../img/single_model_v'+str(write_ver)+'_importances_'+model+'.png')\n",
    "\n",
    "# reduce memory\n",
    "def reduce_mem_usage(df, verbose=True):\n",
    "    numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n",
    "    start_mem = df.memory_usage().sum() / 1024**2\n",
    "    for col in df.columns:\n",
    "        col_type = df[col].dtypes\n",
    "        if col_type in numerics:\n",
    "            c_min = df[col].min()\n",
    "            c_max = df[col].max()\n",
    "            if str(col_type)[:3] == 'int':\n",
    "                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n",
    "                    df[col] = df[col].astype(np.int8)\n",
    "                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n",
    "                    df[col] = df[col].astype(np.int16)\n",
    "                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n",
    "                    df[col] = df[col].astype(np.int32)\n",
    "                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n",
    "                    df[col] = df[col].astype(np.int64)\n",
    "            else:\n",
    "                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n",
    "                    df[col] = df[col].astype(np.float16)\n",
    "                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n",
    "                    df[col] = df[col].astype(np.float32)\n",
    "                else:\n",
    "                    df[col] = df[col].astype(np.float64)\n",
    "\n",
    "    end_mem = df.memory_usage().sum() / 1024**2\n",
    "    print('Memory usage after optimization is: {:.2f} MB'.format(end_mem))\n",
    "    print('Decreased by {:.1f}%'.format(100 * (start_mem - end_mem) / start_mem))\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "def cleaning(train_df, test_df):\n",
    "    train_df = train_df.replace([np.inf, -np.inf], np.nan)\n",
    "    test_df = test_df.replace([np.inf, -np.inf], np.nan)\n",
    "\n",
    "    train_df = train_df.fillna(0)\n",
    "    test_df = test_df.fillna(0)\n",
    "    return train_df, test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocessing train & test\n",
    "def train_test(num_rows=None, debug=False):\n",
    "\n",
    "    # load csv\n",
    "    train_df = pd.read_csv('../input/train.csv', index_col=['card_id'], nrows=num_rows)\n",
    "    test_df = pd.read_csv('../input/test.csv', index_col=['card_id'], nrows=num_rows)\n",
    "\n",
    "    print(\"Train samples: {}, test samples: {}\".format(len(train_df), len(test_df)))\n",
    "\n",
    "    # outlier\n",
    "    train_df['outliers'] = 0\n",
    "    train_df.loc[train_df['target'] < -30, 'outliers'] = 1\n",
    "\n",
    "    # set target as nan\n",
    "    test_df['target'] = np.nan\n",
    "\n",
    "    # merge\n",
    "    df = train_df.append(test_df)\n",
    "\n",
    "    del train_df, test_df\n",
    "    gc.collect()\n",
    "\n",
    "    feat_list = ['feature_1', 'feature_2', 'feature_3']\n",
    "    for feat in feat_list:\n",
    "        df[feat + '_orig'] = df[feat]\n",
    "    \n",
    "    df['feature_1_2_cross'] = df['feature_1_orig'].astype(str).add('-').add(df['feature_2_orig'].astype(str))\n",
    "    df['feature_1_3_cross'] = df['feature_1_orig'].astype(str).add('-').add(df['feature_3_orig'].astype(str))\n",
    "    df['feature_2_3_cross'] = df['feature_2_orig'].astype(str).add('-').add(df['feature_3_orig'].astype(str))\n",
    "    \n",
    "    if debug:\n",
    "        print_train_test_feat_cross(df)\n",
    "    \n",
    "    df['feature_1_2_cross'] = df['feature_1_2_cross'].map({'1-1': 0, '1-2': 1, '1-3': 2,\n",
    "                                                           '2-1': 3, '2-2': 4, '2-3': 5, \n",
    "                                                           '3-1': 6, '3-2': 7, '3-3': 8, \n",
    "                                                           '4-1': 9, '4-2': 10, '4-3': 11,\n",
    "                                                           '5-1': 12, '5-2': 13\n",
    "                                                          }).astype(int)\n",
    "    df['feature_1_3_cross'] = df['feature_1_3_cross'].map({'1-0': 0, '2-0': 1, '3-1': 2, '4-0': 3, '5-1': 4\n",
    "                                                          }).astype(int)\n",
    "    df['feature_2_3_cross'] = df['feature_2_3_cross'].map({'1-0': 0, '1-1': 1, '2-0': 2, '2-1': 3, '3-0': 4, '3-1': 5\n",
    "                                                          }).astype(int)\n",
    "    \n",
    "    df = pd.get_dummies(df, columns=['feature_1', 'feature_2', 'feature_3', \n",
    "                                     'feature_1_2_cross', 'feature_1_3_cross', 'feature_2_3_cross'])\n",
    "    for feat in feat_list:\n",
    "        df[feat] = df[feat + '_orig'] \n",
    "    \n",
    "    # to datetime\n",
    "    df['first_active_month'] = pd.to_datetime(df['first_active_month'])\n",
    "\n",
    "    # datetime features\n",
    "    # df['quarter'] = df['first_active_month'].dt.quarter\n",
    "    df['elapsed_time'] = (datetime.datetime.today() - df['first_active_month']).dt.days\n",
    "\n",
    "    df['days_feature1'] = df['elapsed_time'] * df['feature_1']\n",
    "    df['days_feature2'] = df['elapsed_time'] * df['feature_2']\n",
    "    df['days_feature3'] = df['elapsed_time'] * df['feature_3']\n",
    "\n",
    "    df['days_feature1_ratio'] = df['feature_1'] / df['elapsed_time']\n",
    "    df['days_feature2_ratio'] = df['feature_2'] / df['elapsed_time']\n",
    "    df['days_feature3_ratio'] = df['feature_3'] / df['elapsed_time']\n",
    "\n",
    "    # one hot encoding\n",
    "    df, cols = one_hot_encoder(df, nan_as_category=False)\n",
    "\n",
    "    for f in ['feature_1','feature_2','feature_3']:\n",
    "        order_label = df.groupby([f])['outliers'].mean()\n",
    "        df[f] = df[f].map(order_label)\n",
    "\n",
    "    df['feature_sum'] = df['feature_1'] + df['feature_2'] + df['feature_3']\n",
    "    df['feature_mean'] = df['feature_sum']/3\n",
    "    df['feature_max'] = df[['feature_1', 'feature_2', 'feature_3']].max(axis=1)\n",
    "    df['feature_min'] = df[['feature_1', 'feature_2', 'feature_3']].min(axis=1)\n",
    "    df['feature_var'] = df[['feature_1', 'feature_2', 'feature_3']].std(axis=1)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fill_na(df):\n",
    "    df['category_2'].fillna(1.0,inplace=True)\n",
    "    df[\"category_2\"] = df[\"category_2\"].astype(int)\n",
    "    df['category_3'].fillna('A',inplace=True)\n",
    "    df['merchant_id'].fillna('M_ID_00a6ca8a8a',inplace=True)\n",
    "    df['installments'].replace(-1, np.nan,inplace=True)\n",
    "    df['installments'].replace(999, np.nan,inplace=True)\n",
    "    \n",
    "    return df\n",
    "\n",
    "def encode_to_numeric(df):\n",
    "    df['authorized_flag'] = df['authorized_flag'].map({'Y': 1, 'N': 0}).astype(int)\n",
    "    df['category_1'] = df['category_1'].map({'Y': 1, 'N': 0}).astype(int)\n",
    "    df['category_3'] = df['category_3'].map({'A':0, 'B':1, 'C':2})\n",
    "    # df['category_4'] = df['category_4'].map({'Y': 2, 'N': 1, 'NaN':0}).astype(int)\n",
    "    \n",
    "    return df\n",
    "\n",
    "def gen_datetime(df):\n",
    "    df['purchase_date'] = pd.to_datetime(df['purchase_date'])\n",
    "    df['month'] = df['purchase_date'].dt.month\n",
    "    df['day'] = df['purchase_date'].dt.day\n",
    "    df['hour'] = df['purchase_date'].dt.hour\n",
    "    df['weekofyear'] = df['purchase_date'].dt.weekofyear\n",
    "    df['weekday'] = df['purchase_date'].dt.weekday\n",
    "    df['weekend'] = (df['purchase_date'].dt.weekday >=5).astype(int)\n",
    "    \n",
    "    return df\n",
    "\n",
    "\n",
    "def convert_categorical(df):\n",
    "    \n",
    "    feat_list = ['category_1', 'category_2', 'category_3', 'subsector_id']\n",
    "    \n",
    "    df['category_1_2_cross'] = df['category_1'].astype(str).add('_').add(df['category_2'].astype(str))\n",
    "    df['category_1_3_cross'] = df['category_1'].astype(str).add('_').add(df['category_3'].astype(str))\n",
    "    df['category_2_3_cross'] = df['category_2'].astype(str).add('_').add(df['category_3'].astype(str))\n",
    "    # df['category_1_2_3_cross'] = df['category_1'].astype(str)\\\n",
    "    #                                .add('_').add(df['category_2'].astype(str))\\\n",
    "    #                                .add('_').add(df['category_3'].astype(str))\n",
    "    \n",
    "    for feat in feat_list:\n",
    "        df[feat + '_orig'] = df[feat]\n",
    "        \n",
    "    df = pd.get_dummies(df, columns=['category_1', 'category_2', 'category_3', \n",
    "                                     'category_1_2_cross', 'category_1_3_cross', 'category_2_3_cross',\n",
    "                                     'subsector_id'])\n",
    "    \n",
    "    drop_feat_list = ['category_2_0', 'category_3_0', 'category_4_0', \n",
    "                      'category_1_2_cross_0_0', 'category_1_2_cross_1_0', 'category_1_3_cross_0_0', \n",
    "                      'category_1_3_cross_1_0', 'category_2_3_cross_0_0', 'category_2_3_cross_0_1', \n",
    "                      'category_2_3_cross_0_2', 'category_2_3_cross_0_3', 'category_2_3_cross_1_0', \n",
    "                      'category_2_3_cross_2_0', 'category_2_3_cross_3_0', 'category_2_3_cross_4_0', \n",
    "                      'category_2_3_cross_5_0', \n",
    "                      'subsector_id_-1']\n",
    "    for feat in drop_feat_list:\n",
    "        if feat in df.columns:\n",
    "            df = df.drop([feat], axis=1)\n",
    "    \n",
    "    for feat in feat_list:\n",
    "        df = df.rename(index=str, columns={feat + '_orig': feat})\n",
    "    \n",
    "    for i in range(1, 42):\n",
    "        if i != 6:\n",
    "            df['subsector_id_'+str(i)+'_purchase_amount'] = df['subsector_id_'+str(i)] * df['purchase_amount']\n",
    "            \n",
    "    return df\n",
    "\n",
    "\n",
    "def gen_other_feat(df):\n",
    "    df['price'] = df['purchase_amount'] / df['installments']\n",
    "\n",
    "    #Christmas : December 25 2017\n",
    "    df['Christmas_Day_2017']=(pd.to_datetime('2017-12-25')-df['purchase_date']).dt.days.apply(lambda x: x if x > 0 and x < 100 else 0)\n",
    "    #Mothers Day: May 14 2017\n",
    "    df['Mothers_Day_2017']=(pd.to_datetime('2017-06-04')-df['purchase_date']).dt.days.apply(lambda x: x if x > 0 and x < 100 else 0)\n",
    "    #fathers day: August 13 2017\n",
    "    df['fathers_day_2017']=(pd.to_datetime('2017-08-13')-df['purchase_date']).dt.days.apply(lambda x: x if x > 0 and x < 100 else 0)\n",
    "    #Childrens day: October 12 2017\n",
    "    df['Children_day_2017']=(pd.to_datetime('2017-10-12')-df['purchase_date']).dt.days.apply(lambda x: x if x > 0 and x < 100 else 0)\n",
    "    #Valentine's Day : 12th June, 2017\n",
    "    df['Valentine_Day_2017']=(pd.to_datetime('2017-06-12')-df['purchase_date']).dt.days.apply(lambda x: x if x > 0 and x < 100 else 0)\n",
    "    #Black Friday : 24th November 2017\n",
    "    df['Black_Friday_2017']=(pd.to_datetime('2017-11-24') - df['purchase_date']).dt.days.apply(lambda x: x if x > 0 and x < 100 else 0)\n",
    "\n",
    "    #2018\n",
    "    #Mothers Day: May 13 2018\n",
    "    df['Mothers_Day_2018']=(pd.to_datetime('2018-05-13')-df['purchase_date']).dt.days.apply(lambda x: x if x > 0 and x < 100 else 0)\n",
    "\n",
    "    df['month_diff'] = ((datetime.datetime.today() - df['purchase_date']).dt.days)//30\n",
    "    df['month_diff'] += df['month_lag']\n",
    "    \n",
    "    df['category_1_0_month_diff'] = df['month_diff'] * df['category_1_0']\n",
    "    df['category_1_1_month_diff'] = df['month_diff'] * df['category_1_1']\n",
    "    df['category_2_1_month_diff'] = df['month_diff'] * df['category_2_1']\n",
    "    df['category_2_2_month_diff'] = df['month_diff'] * df['category_2_2']\n",
    "    df['category_2_3_month_diff'] = df['month_diff'] * df['category_2_3']\n",
    "    df['category_2_4_month_diff'] = df['month_diff'] * df['category_2_4']\n",
    "    df['category_2_5_month_diff'] = df['month_diff'] * df['category_2_5']\n",
    "    df['category_3_1_month_diff'] = df['month_diff'] * df['category_3_1']\n",
    "    df['category_3_2_month_diff'] = df['month_diff'] * df['category_3_2']\n",
    "\n",
    "    # additional features\n",
    "    df['duration'] = df['purchase_amount']*df['month_diff']\n",
    "    df['amount_month_ratio'] = df['purchase_amount']/df['month_diff']\n",
    "    \n",
    "    # duration by category\n",
    "    df['category_1_0_duration'] = df['duration'] * df['category_1_0']\n",
    "    df['category_1_1_duration'] = df['duration'] * df['category_1_1']\n",
    "    df['category_2_1_duration'] = df['duration'] * df['category_2_1']\n",
    "    df['category_2_2_duration'] = df['duration'] * df['category_2_2']\n",
    "    df['category_2_3_duration'] = df['duration'] * df['category_2_3']\n",
    "    df['category_2_4_duration'] = df['duration'] * df['category_2_4']\n",
    "    df['category_2_5_duration'] = df['duration'] * df['category_2_5']\n",
    "    df['category_3_1_duration'] = df['duration'] * df['category_3_1']\n",
    "    df['category_3_2_duration'] = df['duration'] * df['category_3_2']\n",
    "    \n",
    "    # installments by category\n",
    "    df['installments_1_0_duration'] = df['installments'] * df['category_1_0']\n",
    "    df['installments_1_1_duration'] = df['installments'] * df['category_1_1']\n",
    "    df['installments_2_1_duration'] = df['installments'] * df['category_2_1']\n",
    "    df['installments_2_2_duration'] = df['installments'] * df['category_2_2']\n",
    "    df['installments_2_3_duration'] = df['installments'] * df['category_2_3']\n",
    "    df['installments_2_4_duration'] = df['installments'] * df['category_2_4']\n",
    "    df['installments_2_5_duration'] = df['installments'] * df['category_2_5']\n",
    "    df['installments_3_1_duration'] = df['installments'] * df['category_3_1']\n",
    "    df['installments_3_2_duration'] = df['installments'] * df['category_3_2']\n",
    "    \n",
    "    return df\n",
    "\n",
    "\n",
    "def feat_agg(df):\n",
    "    col_unique =['subsector_id', 'merchant_id', 'merchant_category_id']\n",
    "    col_seas = ['month', 'hour', 'weekofyear', 'weekday', 'day']\n",
    "    col_cat = [feat for feat in list(df) if 'state_id_' in feat or \n",
    "                                                 'subsector_id_' in feat]\n",
    "    # col_cat = [feat for feat in list(df) if 'state_id_' in feat or \n",
    "    #                                             'subsector_id_' in feat or\n",
    "    #                                             'most_recent_sales_range_' in feat or\n",
    "    #                                             'most_recent_purchases_range_' in feat]\n",
    "\n",
    "    aggs = {}\n",
    "    for col in col_unique:\n",
    "        aggs[col] = ['nunique']\n",
    "\n",
    "    for col in col_seas:\n",
    "        aggs[col] = ['nunique', 'mean', 'min', 'max']\n",
    "        \n",
    "    for col in col_cat:\n",
    "        aggs[col] = ['nunique', 'mean', 'min', 'max']\n",
    "\n",
    "    aggs['purchase_amount'] = ['sum','max','min','mean','var','skew']\n",
    "    aggs['installments'] = ['sum','max','mean','var','skew']\n",
    "    aggs['installments_1_0_duration'] = ['sum','max','mean','var','skew']\n",
    "    aggs['installments_1_1_duration'] = ['sum','max','mean','var','skew']\n",
    "    aggs['installments_2_1_duration'] = ['sum','max','mean','var','skew']\n",
    "    aggs['installments_2_2_duration'] = ['sum','max','mean','var','skew']\n",
    "    aggs['installments_2_3_duration'] = ['sum','max','mean','var','skew']\n",
    "    aggs['installments_2_4_duration'] = ['sum','max','mean','var','skew']\n",
    "    aggs['installments_2_5_duration'] = ['sum','max','mean','var','skew']\n",
    "    aggs['installments_3_1_duration'] = ['sum','max','mean','var','skew']\n",
    "    aggs['installments_3_2_duration'] = ['sum','max','mean','var','skew']\n",
    "    aggs['purchase_date'] = ['max','min']\n",
    "    aggs['month_lag'] = ['max','min','mean','var','skew']\n",
    "    aggs['month_diff'] = ['max','min','mean','var','skew']\n",
    "    aggs['category_1_0_month_diff'] = ['max','min','mean','var','skew']\n",
    "    aggs['category_1_1_month_diff'] = ['max','min','mean','var','skew']\n",
    "    aggs['category_2_1_month_diff'] = ['max','min','mean','var','skew']\n",
    "    aggs['category_2_2_month_diff'] = ['max','min','mean','var','skew']\n",
    "    aggs['category_2_3_month_diff'] = ['max','min','mean','var','skew']\n",
    "    aggs['category_2_4_month_diff'] = ['max','min','mean','var','skew']\n",
    "    aggs['category_2_5_month_diff'] = ['max','min','mean','var','skew']\n",
    "    aggs['category_3_1_month_diff'] = ['max','min','mean','var','skew']\n",
    "    aggs['category_3_2_month_diff'] = ['max','min','mean','var','skew']\n",
    "    aggs['authorized_flag'] = ['mean']\n",
    "    aggs['weekend'] = ['mean'] # overwrite\n",
    "    aggs['weekday'] = ['mean'] # overwrite\n",
    "    aggs['day'] = ['nunique', 'mean', 'min'] # overwrite\n",
    "    aggs['category_1'] = ['mean']\n",
    "    aggs['category_2'] = ['mean']\n",
    "    aggs['category_3'] = ['mean']\n",
    "    aggs['card_id'] = ['size','count']\n",
    "    aggs['price'] = ['sum','mean','max','min','var']\n",
    "    aggs['Christmas_Day_2017'] = ['mean']\n",
    "    aggs['Mothers_Day_2017'] = ['mean']\n",
    "    aggs['fathers_day_2017'] = ['mean']\n",
    "    aggs['Children_day_2017'] = ['mean']\n",
    "    aggs['Valentine_Day_2017'] = ['mean']\n",
    "    aggs['Black_Friday_2017'] = ['mean']\n",
    "    aggs['Mothers_Day_2018'] = ['mean']\n",
    "    aggs['duration']=['mean','min','max','var','skew']\n",
    "    aggs['category_1_0_duration']=['mean','min','max','var','skew']\n",
    "    aggs['category_1_1_duration']=['mean','min','max','var','skew']\n",
    "    aggs['category_2_1_duration']=['mean','min','max','var','skew']\n",
    "    aggs['category_2_2_duration']=['mean','min','max','var','skew']\n",
    "    aggs['category_2_3_duration']=['mean','min','max','var','skew']\n",
    "    aggs['category_2_4_duration']=['mean','min','max','var','skew']\n",
    "    aggs['category_2_5_duration']=['mean','min','max','var','skew']\n",
    "    aggs['category_3_1_duration']=['mean','min','max','var','skew']\n",
    "    aggs['category_3_2_duration']=['mean','min','max','var','skew']\n",
    "    aggs['amount_month_ratio']=['mean','min','max','var','skew']\n",
    "\n",
    "    for col in ['category_2','category_3']:\n",
    "        df[col+'_mean'] = df.groupby([col])['purchase_amount'].transform('mean')\n",
    "        df[col+'_min'] = df.groupby([col])['purchase_amount'].transform('min')\n",
    "        df[col+'_max'] = df.groupby([col])['purchase_amount'].transform('max')\n",
    "        df[col+'_sum'] = df.groupby([col])['purchase_amount'].transform('sum')\n",
    "        aggs[col+'_mean'] = ['mean']\n",
    "\n",
    "    df = df.reset_index().groupby('card_id').agg(aggs)\n",
    "    \n",
    "    df.columns = pd.Index([e[0] + \"_\" + e[1] for e in df.columns.tolist()])\n",
    "    \n",
    "    df['purchase_date_diff'] = (df['purchase_date_max']-df['purchase_date_min']).dt.days\n",
    "    df['purchase_date_average'] = df['purchase_date_diff']/df['card_id_size']\n",
    "    df['purchase_date_uptonow'] = (datetime.datetime.today()-df['purchase_date_max']).dt.days\n",
    "    df['purchase_date_uptomin'] = (datetime.datetime.today()-df['purchase_date_min']).dt.days\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocessing historical transactions\n",
    "def transactions(source, num_rows=None):\n",
    "    # load csv\n",
    "    if source == 'hist':\n",
    "        df = pd.read_csv('../input/historical_transactions.csv', nrows=num_rows)\n",
    "    elif source == 'new':\n",
    "        df = pd.read_csv('../input/new_merchant_transactions.csv', nrows=num_rows)\n",
    "    # merchant_df = pd.read_csv('../input/merchants.csv', nrows=None)\n",
    "    # df = pd.merge(df, merchant_df.drop(['category_1', 'category_2', 'city_id', \n",
    "    #                                               'merchant_category_id', 'subsector_id',\n",
    "    #                                               'state_id'], axis=1), on='merchant_id', how='left')\n",
    "    # del merchant_df\n",
    "    # gc.collect()\n",
    "    \n",
    "    # fillna\n",
    "    df = fill_na(df)\n",
    "    \n",
    "    # trim\n",
    "    # df['purchase_amount'] = df['purchase_amount'].apply(lambda x: min(x, 0.8))\n",
    "\n",
    "    # Y/N to 1/0\n",
    "    df = encode_to_numeric(df)\n",
    "\n",
    "    # datetime features\n",
    "    df = gen_datetime(df)\n",
    "\n",
    "    # subsector_id and state_id features\n",
    "    df = convert_categorical(df)\n",
    "    \n",
    "    # additional features\n",
    "    df = gen_other_feat(df)\n",
    "\n",
    "    # reduce memory usage\n",
    "    df = reduce_mem_usage(df)\n",
    "\n",
    "    # agg features\n",
    "    df = feat_agg(df)\n",
    "\n",
    "    # change column name\n",
    "    if source == 'hist':\n",
    "        df.columns = ['hist_'+ c for c in df.columns]\n",
    "    elif source == 'new':\n",
    "        df.columns = ['new_'+ c for c in df.columns]\n",
    "\n",
    "    # reduce memory usage\n",
    "    df = reduce_mem_usage(df)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# additional features\n",
    "def additional_features(df):\n",
    "    df['hist_first_buy'] = (df['hist_purchase_date_min'] - df['first_active_month']).dt.days\n",
    "    df['hist_last_buy'] = (df['hist_purchase_date_max'] - df['first_active_month']).dt.days\n",
    "    df['new_first_buy'] = (df['new_purchase_date_min'] - df['first_active_month']).dt.days\n",
    "    df['new_last_buy'] = (df['new_purchase_date_max'] - df['first_active_month']).dt.days\n",
    "\n",
    "    date_features=['hist_purchase_date_max','hist_purchase_date_min',\n",
    "                   'new_purchase_date_max', 'new_purchase_date_min']\n",
    "\n",
    "    for f in date_features:\n",
    "        df[f] = df[f].astype(np.int64) * 1e-9\n",
    "\n",
    "    df['card_id_total'] = df['new_card_id_size']+df['hist_card_id_size']\n",
    "    df['card_id_cnt_total'] = df['new_card_id_count']+df['hist_card_id_count']\n",
    "    df['card_id_cnt_ratio'] = df['new_card_id_count']/df['hist_card_id_count']\n",
    "    df['purchase_amount_total'] = df['new_purchase_amount_sum']+df['hist_purchase_amount_sum']\n",
    "    df['purchase_amount_mean'] = df['new_purchase_amount_mean']+df['hist_purchase_amount_mean']\n",
    "    df['purchase_amount_max'] = df['new_purchase_amount_max']+df['hist_purchase_amount_max']\n",
    "    df['purchase_amount_min'] = df['new_purchase_amount_min']+df['hist_purchase_amount_min']\n",
    "    df['purchase_amount_ratio'] = df['new_purchase_amount_sum']/df['hist_purchase_amount_sum']\n",
    "    df['month_diff_mean'] = df['new_month_diff_mean']+df['hist_month_diff_mean']\n",
    "    df['month_diff_ratio'] = df['new_month_diff_mean']/df['hist_month_diff_mean']\n",
    "    df['month_lag_mean'] = df['new_month_lag_mean']+df['hist_month_lag_mean']\n",
    "    df['month_lag_max'] = df['new_month_lag_max']+df['hist_month_lag_max']\n",
    "    df['month_lag_min'] = df['new_month_lag_min']+df['hist_month_lag_min']\n",
    "    df['category_1_mean'] = df['new_category_1_mean']+df['hist_category_1_mean']\n",
    "    df['installments_total'] = df['new_installments_sum']+df['hist_installments_sum']\n",
    "    df['installments_mean'] = df['new_installments_mean']+df['hist_installments_mean']\n",
    "    df['installments_max'] = df['new_installments_max']+df['hist_installments_max']\n",
    "    df['installments_ratio'] = df['new_installments_sum']/df['hist_installments_sum']\n",
    "    df['price_total'] = df['purchase_amount_total'] / df['installments_total']\n",
    "    df['price_mean'] = df['purchase_amount_mean'] / df['installments_mean']\n",
    "    df['price_max'] = df['purchase_amount_max'] / df['installments_max']\n",
    "    df['duration_mean'] = df['new_duration_mean']+df['hist_duration_mean']\n",
    "    df['duration_min'] = df['new_duration_min']+df['hist_duration_min']\n",
    "    df['duration_max'] = df['new_duration_max']+df['hist_duration_max']\n",
    "    df['amount_month_ratio_mean']=df['new_amount_month_ratio_mean']+df['hist_amount_month_ratio_mean']\n",
    "    df['amount_month_ratio_min']=df['new_amount_month_ratio_min']+df['hist_amount_month_ratio_min']\n",
    "    df['amount_month_ratio_max']=df['new_amount_month_ratio_max']+df['hist_amount_month_ratio_max']\n",
    "    df['new_CLV'] = df['new_card_id_count'] * df['new_purchase_amount_sum'] / df['new_month_diff_mean']\n",
    "    df['hist_CLV'] = df['hist_card_id_count'] * df['hist_purchase_amount_sum'] / df['hist_month_diff_mean']\n",
    "    df['CLV_ratio'] = df['new_CLV'] / df['hist_CLV']\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def kfold_model(train_df, test_df, num_folds, model,\n",
    "                stratified=False, debug=False, saveOpt=True,\n",
    "                feat_sorted=[], num_feat=None):\n",
    "    if model == 'lgb':\n",
    "        print(\"Starting LightGBM. Train shape: {}, test shape: {}\".format(train_df.shape, test_df.shape))\n",
    "    elif model == 'xgb':\n",
    "        print(\"Starting Xgboost. Train shape: {}, test shape: {}\".format(train_df.shape, test_df.shape))\n",
    "\n",
    "    # Cross validation model\n",
    "    if stratified:\n",
    "        folds = StratifiedKFold(n_splits=num_folds, shuffle=True, random_state=326)\n",
    "    else:\n",
    "        folds = KFold(n_splits=num_folds, shuffle=True, random_state=326)\n",
    "\n",
    "    # Create arrays and dataframes to store results\n",
    "    val_preds = np.zeros(train_df.shape[0])\n",
    "    oof_preds = np.zeros(train_df.shape[0])\n",
    "    sub_preds = np.zeros(test_df.shape[0])\n",
    "    feature_importance_df = pd.DataFrame()\n",
    "    if not feat_sorted:\n",
    "        # print(\"feat_sorted is empty\")\n",
    "        feats = [f for f in train_df.columns if f not in FEATS_EXCLUDED]\n",
    "    else:\n",
    "        if num_feat == None:\n",
    "            # print(\"feat_sorted is empty, num_feat is none\")\n",
    "            feats = [f for f in train_df.columns if f not in FEATS_EXCLUDED and f in feat_sorted]\n",
    "        else:\n",
    "            # print(\"feat_sorted is empty, num_feat is not none\")\n",
    "            feats = [f for f in train_df.columns if f not in FEATS_EXCLUDED and f in feat_sorted[:num_feat]]\n",
    "    \n",
    "    print(\"Final feat size: \" + str(len(feats)))\n",
    "    # print(feats)\n",
    "    \n",
    "    # k-fold\n",
    "    for n_fold, (train_idx, valid_idx) in enumerate(folds.split(train_df[feats], train_df['outliers'])):\n",
    "        train_x, train_y = train_df[feats].iloc[train_idx], train_df['target'].iloc[train_idx]\n",
    "        valid_x, valid_y = train_df[feats].iloc[valid_idx], train_df['target'].iloc[valid_idx]\n",
    "        \n",
    "        if model == 'lgb':\n",
    "            # set data structure\n",
    "            lgb_train = lgb.Dataset(train_x, label=train_y, free_raw_data=False)\n",
    "            lgb_test = lgb.Dataset(valid_x, label=valid_y, free_raw_data=False)\n",
    "\n",
    "            # params optimized by optuna\n",
    "            params ={\n",
    "                    'task': 'train',\n",
    "                    'boosting': 'goss',\n",
    "                    'objective': 'regression',\n",
    "                    'metric': 'rmse',\n",
    "                    'learning_rate': 0.01,\n",
    "                    'subsample': 0.9855232997390695,\n",
    "                    'max_depth': 7,\n",
    "                    'top_rate': 0.9064148448434349,\n",
    "                    'num_leaves': 63,\n",
    "                    'min_child_weight': 41.9612869171337,\n",
    "                    'other_rate': 0.0721768246018207,\n",
    "                    'reg_alpha': 9.677537745007898,\n",
    "                    'colsample_bytree': 0.5665320670155495,\n",
    "                    'min_split_gain': 9.820197773625843,\n",
    "                    'reg_lambda': 8.2532317400459,\n",
    "                    'min_data_in_leaf': 21,\n",
    "                    'verbose': -1,\n",
    "                    'seed': int(2**n_fold),\n",
    "                    'bagging_seed': int(2**n_fold),\n",
    "                    'drop_seed': int(2**n_fold)\n",
    "#                     'seed': 12, #int(2**n_fold),\n",
    "#                     'bagging_seed': 34, # int(2**n_fold),\n",
    "#                     'drop_seed': 56 #int(2**n_fold)\n",
    "                    }\n",
    "\n",
    "            reg = lgb.train(\n",
    "                            params,\n",
    "                            lgb_train,\n",
    "                            valid_sets=[lgb_train, lgb_test],\n",
    "                            valid_names=['train', 'test'],\n",
    "                            num_boost_round=10000,\n",
    "                            early_stopping_rounds= 200,\n",
    "                            verbose_eval=100\n",
    "                            )\n",
    "            best_iteration = reg.best_iteration\n",
    "            val_preds[valid_idx] = valid_y\n",
    "            oof_preds[valid_idx] = reg.predict(valid_x, num_iteration=best_iteration)\n",
    "            sub_preds += reg.predict(test_df[feats], num_iteration=best_iteration) / folds.n_splits\n",
    "            \n",
    "            fold_importance_df = pd.DataFrame()\n",
    "            fold_importance_df[\"feature\"] = feats\n",
    "            fold_importance_df[\"importance\"] = np.log1p(reg.feature_importance(importance_type='gain', \n",
    "                                                                               iteration=best_iteration))\n",
    "            \n",
    "        elif model == 'xgb':\n",
    "            xgb_train = xgb.DMatrix(data=train_x, label=train_y)\n",
    "            xgb_test = xgb.DMatrix(data=valid_x, label=valid_y)\n",
    "            \n",
    "            params = {'gpu_id': 0, \n",
    "                      #'n_gpus': 2, \n",
    "                      'objective': 'reg:linear', \n",
    "                      'eval_metric': 'rmse', \n",
    "                      'silent': True, \n",
    "                      'booster': 'gbtree', \n",
    "                      'n_jobs': -1, \n",
    "                      'n_estimators': 2500, \n",
    "                      'tree_method': 'gpu_hist', \n",
    "                      'grow_policy': 'lossguide', \n",
    "                      'max_depth': 12, \n",
    "                      'seed': 538, \n",
    "                      'colsample_bylevel': 0.9, \n",
    "                      'colsample_bytree': 0.8, \n",
    "                      'gamma': 0.0001, \n",
    "                      'learning_rate': 0.006150886706231842, \n",
    "                      'max_bin': 128, \n",
    "                      'max_leaves': 47, \n",
    "                      'min_child_weight': 40, \n",
    "                      'reg_alpha': 10.0, \n",
    "                      'reg_lambda': 10.0, \n",
    "                      'subsample': 0.9}\n",
    "            \n",
    "            watchlist = [(xgb_train, 'train'), (xgb_test, 'valid')]\n",
    "            num_rounds = 10000\n",
    "            reg = xgb.train(params, \n",
    "                            xgb_train, \n",
    "                            num_rounds,\n",
    "                            watchlist, \n",
    "                            early_stopping_rounds=200,\n",
    "                            verbose_eval=100)\n",
    "            best_ntree_limit = reg.best_ntree_limit\n",
    "            \n",
    "            val_preds[valid_idx] = valid_y\n",
    "            oof_preds[valid_idx] = reg.predict(xgb_test, ntree_limit=best_ntree_limit)\n",
    "            sub_preds += reg.predict(xgb.DMatrix(test_df[feats]), ntree_limit=best_ntree_limit) / folds.n_splits\n",
    "            \n",
    "            fold_importance_df = pd.DataFrame()\n",
    "            fold_importance_df[\"feature\"] = list(reg.get_score(importance_type='gain').keys())\n",
    "            fold_importance_df[\"importance\"] = np.log1p(list(reg.get_score(importance_type='gain').values()))\n",
    "\n",
    "        fold_importance_df[\"fold\"] = n_fold + 1\n",
    "        feature_importance_df = pd.concat([feature_importance_df, fold_importance_df], axis=0)\n",
    "        print('Fold %2d RMSE : %.6f' % (n_fold + 1, rmse(valid_y, oof_preds[valid_idx])))\n",
    "        del reg, train_x, train_y, valid_x, valid_y\n",
    "        gc.collect()\n",
    "    \n",
    "    print('Final RMSE : %.6f \\n\\n\\n' % rmse(val_preds, oof_preds))\n",
    "    # display importances\n",
    "    #display_importances(feature_importance_df)\n",
    "\n",
    "    if not debug and saveOpt:\n",
    "        # save submission file\n",
    "        test_df.loc[:,'target'] = sub_preds\n",
    "        test_df = test_df.reset_index()\n",
    "        if num_feat == None:\n",
    "            if stratified:\n",
    "                test_df[['card_id', 'target']].to_csv('../result/'+submission_file_name+\"_\"+ model +\"_stratified_all_feat.csv\", index=False)\n",
    "            else:\n",
    "                test_df[['card_id', 'target']].to_csv('../result/'+submission_file_name+\"_\"+ model +\"_all_feat.csv\", index=False)\n",
    "        else:\n",
    "            if stratified:\n",
    "                test_df[['card_id', 'target']].to_csv('../result/'+submission_file_name+\"_\"+ model +\"_stratified_\"+str(num_feat)+\"_feat.csv\", index=False)\n",
    "            else:\n",
    "                test_df[['card_id', 'target']].to_csv('../result/'+submission_file_name+\"_\"+ model +\"_\"+str(num_feat)+\"_feat.csv\", index=False)\n",
    "    \n",
    "    feat_sel_summary = feature_importance_df[[\"feature\", \"importance\"]].groupby(\"feature\").mean().sort_values(by=\"importance\", ascending=False).reset_index()\n",
    "    feat_sorted = feat_sel_summary[\"feature\"].tolist()\n",
    "    \n",
    "    return feature_importance_df, sub_preds, feat_sorted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine_solution(test_df, sub_preds_list, submission_file_name):\n",
    "    sub_preds = [0.0 for _ in range(len(sub_preds_list[0]))]\n",
    "    for sol in sub_preds_list:\n",
    "        sub_preds += sol/len(sub_preds_list)\n",
    "    test_df.loc[:,'target'] = sub_preds\n",
    "    test_df = test_df.reset_index()\n",
    "    test_df[['card_id', 'target']].to_csv('../result/'+submission_file_name+'_final.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Default case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_df_cleaned size: (201917, 1154)\n",
      "test_df_cleaned size: (123623, 1154)\n",
      "Starting LightGBM. Train shape: (201917, 1154), test shape: (123623, 1154)\n",
      "Final feat size: 100\n",
      "Training until validation scores don't improve for 200 rounds.\n",
      "[100]\ttrain's rmse: 3.64944\ttest's rmse: 3.74966\n",
      "[200]\ttrain's rmse: 3.56996\ttest's rmse: 3.71189\n",
      "[300]\ttrain's rmse: 3.51694\ttest's rmse: 3.69812\n",
      "[400]\ttrain's rmse: 3.47986\ttest's rmse: 3.69116\n",
      "[500]\ttrain's rmse: 3.45245\ttest's rmse: 3.68781\n",
      "[600]\ttrain's rmse: 3.42613\ttest's rmse: 3.68518\n",
      "[700]\ttrain's rmse: 3.40409\ttest's rmse: 3.68462\n",
      "[800]\ttrain's rmse: 3.38311\ttest's rmse: 3.68418\n",
      "[900]\ttrain's rmse: 3.36348\ttest's rmse: 3.68402\n",
      "[1000]\ttrain's rmse: 3.34463\ttest's rmse: 3.68409\n",
      "[1100]\ttrain's rmse: 3.32686\ttest's rmse: 3.68393\n",
      "[1200]\ttrain's rmse: 3.3105\ttest's rmse: 3.68325\n",
      "[1300]\ttrain's rmse: 3.29456\ttest's rmse: 3.68428\n",
      "[1400]\ttrain's rmse: 3.27755\ttest's rmse: 3.68434\n",
      "Early stopping, best iteration is:\n",
      "[1201]\ttrain's rmse: 3.31021\ttest's rmse: 3.68324\n",
      "Fold  1 RMSE : 3.683242\n",
      "Training until validation scores don't improve for 200 rounds.\n",
      "[100]\ttrain's rmse: 3.63927\ttest's rmse: 3.83145\n",
      "[200]\ttrain's rmse: 3.55879\ttest's rmse: 3.79938\n",
      "[300]\ttrain's rmse: 3.50867\ttest's rmse: 3.78674\n",
      "[400]\ttrain's rmse: 3.47249\ttest's rmse: 3.78104\n",
      "[500]\ttrain's rmse: 3.44308\ttest's rmse: 3.77779\n",
      "[600]\ttrain's rmse: 3.41834\ttest's rmse: 3.77543\n",
      "[700]\ttrain's rmse: 3.39349\ttest's rmse: 3.77515\n",
      "[800]\ttrain's rmse: 3.37348\ttest's rmse: 3.77486\n",
      "[900]\ttrain's rmse: 3.35316\ttest's rmse: 3.7751\n",
      "[1000]\ttrain's rmse: 3.33353\ttest's rmse: 3.77486\n",
      "Early stopping, best iteration is:\n",
      "[816]\ttrain's rmse: 3.37005\ttest's rmse: 3.77461\n",
      "Fold  2 RMSE : 3.774614\n",
      "Training until validation scores don't improve for 200 rounds.\n",
      "[100]\ttrain's rmse: 3.64296\ttest's rmse: 3.79505\n",
      "[200]\ttrain's rmse: 3.56327\ttest's rmse: 3.76793\n",
      "[300]\ttrain's rmse: 3.51119\ttest's rmse: 3.75852\n",
      "[400]\ttrain's rmse: 3.47537\ttest's rmse: 3.75506\n",
      "[500]\ttrain's rmse: 3.44556\ttest's rmse: 3.75351\n",
      "[600]\ttrain's rmse: 3.42039\ttest's rmse: 3.75279\n",
      "[700]\ttrain's rmse: 3.39672\ttest's rmse: 3.75202\n",
      "[800]\ttrain's rmse: 3.37479\ttest's rmse: 3.75218\n",
      "[900]\ttrain's rmse: 3.35327\ttest's rmse: 3.75177\n",
      "[1000]\ttrain's rmse: 3.3338\ttest's rmse: 3.7515\n",
      "[1100]\ttrain's rmse: 3.31435\ttest's rmse: 3.75177\n",
      "Early stopping, best iteration is:\n",
      "[972]\ttrain's rmse: 3.33914\ttest's rmse: 3.7512\n",
      "Fold  3 RMSE : 3.751195\n",
      "Training until validation scores don't improve for 200 rounds.\n",
      "[100]\ttrain's rmse: 3.6548\ttest's rmse: 3.6971\n",
      "[200]\ttrain's rmse: 3.57558\ttest's rmse: 3.65794\n",
      "[300]\ttrain's rmse: 3.52581\ttest's rmse: 3.64189\n",
      "[400]\ttrain's rmse: 3.48898\ttest's rmse: 3.63657\n",
      "[500]\ttrain's rmse: 3.45952\ttest's rmse: 3.63371\n",
      "[600]\ttrain's rmse: 3.43472\ttest's rmse: 3.63255\n",
      "[700]\ttrain's rmse: 3.41175\ttest's rmse: 3.63149\n",
      "[800]\ttrain's rmse: 3.38976\ttest's rmse: 3.63142\n",
      "[900]\ttrain's rmse: 3.36741\ttest's rmse: 3.63098\n",
      "[1000]\ttrain's rmse: 3.34838\ttest's rmse: 3.63125\n",
      "[1100]\ttrain's rmse: 3.33035\ttest's rmse: 3.63189\n",
      "Early stopping, best iteration is:\n",
      "[920]\ttrain's rmse: 3.36391\ttest's rmse: 3.63087\n",
      "Fold  4 RMSE : 3.630870\n",
      "Training until validation scores don't improve for 200 rounds.\n",
      "[100]\ttrain's rmse: 3.67624\ttest's rmse: 3.50842\n",
      "[200]\ttrain's rmse: 3.59852\ttest's rmse: 3.46485\n",
      "[300]\ttrain's rmse: 3.54768\ttest's rmse: 3.44727\n",
      "[400]\ttrain's rmse: 3.50976\ttest's rmse: 3.43888\n",
      "[500]\ttrain's rmse: 3.48078\ttest's rmse: 3.43486\n",
      "[600]\ttrain's rmse: 3.45663\ttest's rmse: 3.43145\n",
      "[700]\ttrain's rmse: 3.4332\ttest's rmse: 3.43016\n",
      "[800]\ttrain's rmse: 3.41175\ttest's rmse: 3.42897\n",
      "[900]\ttrain's rmse: 3.39157\ttest's rmse: 3.4276\n",
      "[1000]\ttrain's rmse: 3.37201\ttest's rmse: 3.42694\n",
      "[1100]\ttrain's rmse: 3.35375\ttest's rmse: 3.42634\n",
      "[1200]\ttrain's rmse: 3.33628\ttest's rmse: 3.42581\n",
      "[1300]\ttrain's rmse: 3.32019\ttest's rmse: 3.42555\n",
      "[1400]\ttrain's rmse: 3.30389\ttest's rmse: 3.42525\n",
      "[1500]\ttrain's rmse: 3.28804\ttest's rmse: 3.42537\n",
      "[1600]\ttrain's rmse: 3.27276\ttest's rmse: 3.42558\n",
      "Early stopping, best iteration is:\n",
      "[1441]\ttrain's rmse: 3.29713\ttest's rmse: 3.42479\n",
      "Fold  5 RMSE : 3.424792\n",
      "Training until validation scores don't improve for 200 rounds.\n",
      "[100]\ttrain's rmse: 3.65209\ttest's rmse: 3.73098\n",
      "[200]\ttrain's rmse: 3.57212\ttest's rmse: 3.69057\n",
      "[300]\ttrain's rmse: 3.5215\ttest's rmse: 3.67569\n",
      "[400]\ttrain's rmse: 3.4854\ttest's rmse: 3.66908\n",
      "[500]\ttrain's rmse: 3.45641\ttest's rmse: 3.66704\n",
      "[600]\ttrain's rmse: 3.4318\ttest's rmse: 3.66527\n",
      "[700]\ttrain's rmse: 3.4094\ttest's rmse: 3.66361\n",
      "[800]\ttrain's rmse: 3.38682\ttest's rmse: 3.66187\n",
      "[900]\ttrain's rmse: 3.36569\ttest's rmse: 3.66106\n",
      "[1000]\ttrain's rmse: 3.34544\ttest's rmse: 3.66115\n",
      "[1100]\ttrain's rmse: 3.32761\ttest's rmse: 3.66184\n",
      "Early stopping, best iteration is:\n",
      "[966]\ttrain's rmse: 3.35183\ttest's rmse: 3.66078\n",
      "Fold  6 RMSE : 3.660782\n",
      "Training until validation scores don't improve for 200 rounds.\n",
      "[100]\ttrain's rmse: 3.65891\ttest's rmse: 3.64877\n",
      "[200]\ttrain's rmse: 3.57818\ttest's rmse: 3.617\n",
      "[300]\ttrain's rmse: 3.52729\ttest's rmse: 3.60471\n",
      "[400]\ttrain's rmse: 3.48978\ttest's rmse: 3.59816\n",
      "[500]\ttrain's rmse: 3.4621\ttest's rmse: 3.59543\n",
      "[600]\ttrain's rmse: 3.43809\ttest's rmse: 3.59325\n",
      "[700]\ttrain's rmse: 3.41456\ttest's rmse: 3.59125\n",
      "[800]\ttrain's rmse: 3.39319\ttest's rmse: 3.59036\n",
      "[900]\ttrain's rmse: 3.37326\ttest's rmse: 3.5904\n",
      "[1000]\ttrain's rmse: 3.35474\ttest's rmse: 3.58991\n",
      "[1100]\ttrain's rmse: 3.33745\ttest's rmse: 3.58985\n",
      "[1200]\ttrain's rmse: 3.31943\ttest's rmse: 3.58942\n",
      "[1300]\ttrain's rmse: 3.30276\ttest's rmse: 3.58934\n",
      "[1400]\ttrain's rmse: 3.28613\ttest's rmse: 3.58967\n",
      "[1500]\ttrain's rmse: 3.27086\ttest's rmse: 3.58867\n",
      "[1600]\ttrain's rmse: 3.25604\ttest's rmse: 3.58842\n",
      "[1700]\ttrain's rmse: 3.24119\ttest's rmse: 3.58808\n",
      "[1800]\ttrain's rmse: 3.22938\ttest's rmse: 3.58802\n",
      "[1900]\ttrain's rmse: 3.21609\ttest's rmse: 3.58878\n",
      "[2000]\ttrain's rmse: 3.20216\ttest's rmse: 3.5886\n",
      "Early stopping, best iteration is:\n",
      "[1801]\ttrain's rmse: 3.22921\ttest's rmse: 3.58793\n",
      "Fold  7 RMSE : 3.587927\n",
      "Training until validation scores don't improve for 200 rounds.\n",
      "[100]\ttrain's rmse: 3.63562\ttest's rmse: 3.86962\n",
      "[200]\ttrain's rmse: 3.55473\ttest's rmse: 3.83436\n",
      "[300]\ttrain's rmse: 3.50212\ttest's rmse: 3.82648\n",
      "[400]\ttrain's rmse: 3.46687\ttest's rmse: 3.82305\n",
      "[500]\ttrain's rmse: 3.43787\ttest's rmse: 3.82156\n",
      "[600]\ttrain's rmse: 3.41404\ttest's rmse: 3.82019\n",
      "[700]\ttrain's rmse: 3.39141\ttest's rmse: 3.82022\n",
      "[800]\ttrain's rmse: 3.36944\ttest's rmse: 3.8196\n",
      "[900]\ttrain's rmse: 3.34726\ttest's rmse: 3.81902\n",
      "[1000]\ttrain's rmse: 3.32645\ttest's rmse: 3.81967\n",
      "Early stopping, best iteration is:\n",
      "[896]\ttrain's rmse: 3.34839\ttest's rmse: 3.81893\n",
      "Fold  8 RMSE : 3.818929\n",
      "Training until validation scores don't improve for 200 rounds.\n",
      "[100]\ttrain's rmse: 3.67279\ttest's rmse: 3.53087\n",
      "[200]\ttrain's rmse: 3.59182\ttest's rmse: 3.50355\n",
      "[300]\ttrain's rmse: 3.54051\ttest's rmse: 3.49263\n",
      "[400]\ttrain's rmse: 3.50376\ttest's rmse: 3.48878\n",
      "[500]\ttrain's rmse: 3.47416\ttest's rmse: 3.48558\n",
      "[600]\ttrain's rmse: 3.44881\ttest's rmse: 3.48352\n",
      "[700]\ttrain's rmse: 3.42512\ttest's rmse: 3.48228\n",
      "[800]\ttrain's rmse: 3.40345\ttest's rmse: 3.48161\n",
      "[900]\ttrain's rmse: 3.38515\ttest's rmse: 3.48068\n",
      "[1000]\ttrain's rmse: 3.36604\ttest's rmse: 3.48109\n",
      "[1100]\ttrain's rmse: 3.34893\ttest's rmse: 3.48159\n",
      "Early stopping, best iteration is:\n",
      "[940]\ttrain's rmse: 3.37713\ttest's rmse: 3.48055\n",
      "Fold  9 RMSE : 3.480548\n",
      "Training until validation scores don't improve for 200 rounds.\n",
      "[100]\ttrain's rmse: 3.65672\ttest's rmse: 3.67906\n",
      "[200]\ttrain's rmse: 3.57766\ttest's rmse: 3.64119\n",
      "[300]\ttrain's rmse: 3.52818\ttest's rmse: 3.62559\n",
      "[400]\ttrain's rmse: 3.49254\ttest's rmse: 3.61941\n",
      "[500]\ttrain's rmse: 3.46421\ttest's rmse: 3.61576\n",
      "[600]\ttrain's rmse: 3.4386\ttest's rmse: 3.61421\n",
      "[700]\ttrain's rmse: 3.41619\ttest's rmse: 3.61324\n",
      "[800]\ttrain's rmse: 3.3944\ttest's rmse: 3.61222\n",
      "[900]\ttrain's rmse: 3.37284\ttest's rmse: 3.61183\n",
      "[1000]\ttrain's rmse: 3.35356\ttest's rmse: 3.61203\n",
      "[1100]\ttrain's rmse: 3.33458\ttest's rmse: 3.61161\n",
      "[1200]\ttrain's rmse: 3.31703\ttest's rmse: 3.61249\n",
      "Early stopping, best iteration is:\n",
      "[1079]\ttrain's rmse: 3.33891\ttest's rmse: 3.61144\n",
      "Fold 10 RMSE : 3.611441\n",
      "Final RMSE : 3.644357 \n",
      "\n",
      "\n",
      "\n",
      "Starting LightGBM. Train shape: (201917, 1154), test shape: (123623, 1154)\n",
      "Final feat size: 200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 200 rounds.\n",
      "[100]\ttrain's rmse: 3.65221\ttest's rmse: 3.69236\n",
      "[200]\ttrain's rmse: 3.56723\ttest's rmse: 3.65229\n",
      "[300]\ttrain's rmse: 3.51233\ttest's rmse: 3.63321\n",
      "[400]\ttrain's rmse: 3.47315\ttest's rmse: 3.62424\n",
      "[500]\ttrain's rmse: 3.44255\ttest's rmse: 3.61914\n",
      "[600]\ttrain's rmse: 3.41455\ttest's rmse: 3.61545\n",
      "[700]\ttrain's rmse: 3.39029\ttest's rmse: 3.61338\n",
      "[800]\ttrain's rmse: 3.3687\ttest's rmse: 3.61173\n",
      "[900]\ttrain's rmse: 3.34859\ttest's rmse: 3.61018\n",
      "[1000]\ttrain's rmse: 3.32957\ttest's rmse: 3.61008\n",
      "[1100]\ttrain's rmse: 3.31348\ttest's rmse: 3.60934\n",
      "[1200]\ttrain's rmse: 3.2971\ttest's rmse: 3.60865\n",
      "[1300]\ttrain's rmse: 3.28127\ttest's rmse: 3.60816\n",
      "[1400]\ttrain's rmse: 3.26487\ttest's rmse: 3.6075\n",
      "[1500]\ttrain's rmse: 3.24869\ttest's rmse: 3.60771\n",
      "[1600]\ttrain's rmse: 3.2343\ttest's rmse: 3.60777\n",
      "Early stopping, best iteration is:\n",
      "[1441]\ttrain's rmse: 3.25869\ttest's rmse: 3.60723\n",
      "Fold  1 RMSE : 3.607226\n",
      "Training until validation scores don't improve for 200 rounds.\n",
      "[100]\ttrain's rmse: 3.65269\ttest's rmse: 3.68235\n",
      "[200]\ttrain's rmse: 3.56978\ttest's rmse: 3.64135\n",
      "[300]\ttrain's rmse: 3.51468\ttest's rmse: 3.62605\n",
      "[400]\ttrain's rmse: 3.47332\ttest's rmse: 3.6182\n",
      "[500]\ttrain's rmse: 3.44221\ttest's rmse: 3.61565\n",
      "[600]\ttrain's rmse: 3.41334\ttest's rmse: 3.61411\n",
      "[700]\ttrain's rmse: 3.38829\ttest's rmse: 3.61248\n",
      "[800]\ttrain's rmse: 3.36528\ttest's rmse: 3.6118\n",
      "[900]\ttrain's rmse: 3.3446\ttest's rmse: 3.61124\n",
      "[1000]\ttrain's rmse: 3.32442\ttest's rmse: 3.61058\n",
      "[1100]\ttrain's rmse: 3.30456\ttest's rmse: 3.61055\n",
      "[1200]\ttrain's rmse: 3.28647\ttest's rmse: 3.61092\n",
      "[1300]\ttrain's rmse: 3.27051\ttest's rmse: 3.61114\n",
      "Early stopping, best iteration is:\n",
      "[1120]\ttrain's rmse: 3.30131\ttest's rmse: 3.6103\n",
      "Fold  2 RMSE : 3.610300\n",
      "Training until validation scores don't improve for 200 rounds.\n",
      "[100]\ttrain's rmse: 3.6454\ttest's rmse: 3.72999\n",
      "[200]\ttrain's rmse: 3.56083\ttest's rmse: 3.70539\n",
      "[300]\ttrain's rmse: 3.50532\ttest's rmse: 3.69754\n",
      "[400]\ttrain's rmse: 3.46517\ttest's rmse: 3.69462\n",
      "[500]\ttrain's rmse: 3.43508\ttest's rmse: 3.69231\n",
      "[600]\ttrain's rmse: 3.40925\ttest's rmse: 3.69094\n",
      "[700]\ttrain's rmse: 3.38611\ttest's rmse: 3.69048\n",
      "[800]\ttrain's rmse: 3.36556\ttest's rmse: 3.68974\n",
      "[900]\ttrain's rmse: 3.34652\ttest's rmse: 3.68912\n",
      "[1000]\ttrain's rmse: 3.3277\ttest's rmse: 3.68983\n",
      "Early stopping, best iteration is:\n",
      "[896]\ttrain's rmse: 3.3472\ttest's rmse: 3.68908\n",
      "Fold  3 RMSE : 3.689084\n",
      "Training until validation scores don't improve for 200 rounds.\n",
      "[100]\ttrain's rmse: 3.65034\ttest's rmse: 3.70222\n",
      "[200]\ttrain's rmse: 3.5658\ttest's rmse: 3.66482\n",
      "[300]\ttrain's rmse: 3.50917\ttest's rmse: 3.65245\n",
      "[400]\ttrain's rmse: 3.46712\ttest's rmse: 3.64788\n",
      "[500]\ttrain's rmse: 3.43573\ttest's rmse: 3.64607\n",
      "[600]\ttrain's rmse: 3.40833\ttest's rmse: 3.64435\n",
      "[700]\ttrain's rmse: 3.38319\ttest's rmse: 3.64363\n",
      "[800]\ttrain's rmse: 3.36172\ttest's rmse: 3.64335\n",
      "[900]\ttrain's rmse: 3.34152\ttest's rmse: 3.64302\n",
      "[1000]\ttrain's rmse: 3.32306\ttest's rmse: 3.64347\n",
      "[1100]\ttrain's rmse: 3.30454\ttest's rmse: 3.644\n",
      "Early stopping, best iteration is:\n",
      "[918]\ttrain's rmse: 3.33819\ttest's rmse: 3.64275\n",
      "Fold  4 RMSE : 3.642748\n",
      "Training until validation scores don't improve for 200 rounds.\n",
      "[100]\ttrain's rmse: 3.64818\ttest's rmse: 3.71533\n",
      "[200]\ttrain's rmse: 3.56294\ttest's rmse: 3.67907\n",
      "[300]\ttrain's rmse: 3.50618\ttest's rmse: 3.66784\n",
      "[400]\ttrain's rmse: 3.46709\ttest's rmse: 3.66208\n",
      "[500]\ttrain's rmse: 3.43668\ttest's rmse: 3.65804\n",
      "[600]\ttrain's rmse: 3.40858\ttest's rmse: 3.65644\n",
      "[700]\ttrain's rmse: 3.38321\ttest's rmse: 3.65434\n",
      "[800]\ttrain's rmse: 3.3605\ttest's rmse: 3.65316\n",
      "[900]\ttrain's rmse: 3.3406\ttest's rmse: 3.6525\n",
      "[1000]\ttrain's rmse: 3.32151\ttest's rmse: 3.65149\n",
      "[1100]\ttrain's rmse: 3.303\ttest's rmse: 3.6514\n",
      "[1200]\ttrain's rmse: 3.28523\ttest's rmse: 3.65097\n",
      "[1300]\ttrain's rmse: 3.26881\ttest's rmse: 3.65059\n",
      "[1400]\ttrain's rmse: 3.25283\ttest's rmse: 3.65125\n",
      "Early stopping, best iteration is:\n",
      "[1290]\ttrain's rmse: 3.27064\ttest's rmse: 3.65047\n",
      "Fold  5 RMSE : 3.650469\n",
      "Training until validation scores don't improve for 200 rounds.\n",
      "[100]\ttrain's rmse: 3.65402\ttest's rmse: 3.68146\n",
      "[200]\ttrain's rmse: 3.56988\ttest's rmse: 3.63773\n",
      "[300]\ttrain's rmse: 3.51529\ttest's rmse: 3.62162\n",
      "[400]\ttrain's rmse: 3.47573\ttest's rmse: 3.61368\n",
      "[500]\ttrain's rmse: 3.44329\ttest's rmse: 3.60942\n",
      "[600]\ttrain's rmse: 3.41762\ttest's rmse: 3.60705\n",
      "[700]\ttrain's rmse: 3.39295\ttest's rmse: 3.60659\n",
      "[800]\ttrain's rmse: 3.3696\ttest's rmse: 3.60572\n",
      "[900]\ttrain's rmse: 3.34825\ttest's rmse: 3.60532\n",
      "[1000]\ttrain's rmse: 3.32933\ttest's rmse: 3.60528\n",
      "[1100]\ttrain's rmse: 3.30996\ttest's rmse: 3.60598\n",
      "Early stopping, best iteration is:\n",
      "[928]\ttrain's rmse: 3.34292\ttest's rmse: 3.60516\n",
      "Fold  6 RMSE : 3.605158\n",
      "Training until validation scores don't improve for 200 rounds.\n",
      "[100]\ttrain's rmse: 3.6466\ttest's rmse: 3.72037\n",
      "[200]\ttrain's rmse: 3.56166\ttest's rmse: 3.69167\n",
      "[300]\ttrain's rmse: 3.5074\ttest's rmse: 3.67952\n",
      "[400]\ttrain's rmse: 3.46775\ttest's rmse: 3.67468\n",
      "[500]\ttrain's rmse: 3.43769\ttest's rmse: 3.6715\n",
      "[600]\ttrain's rmse: 3.41253\ttest's rmse: 3.66963\n",
      "[700]\ttrain's rmse: 3.38917\ttest's rmse: 3.66961\n",
      "[800]\ttrain's rmse: 3.36731\ttest's rmse: 3.66982\n",
      "Early stopping, best iteration is:\n",
      "[666]\ttrain's rmse: 3.39714\ttest's rmse: 3.66918\n",
      "Fold  7 RMSE : 3.669182\n",
      "Training until validation scores don't improve for 200 rounds.\n",
      "[100]\ttrain's rmse: 3.64477\ttest's rmse: 3.73117\n",
      "[200]\ttrain's rmse: 3.55772\ttest's rmse: 3.70466\n",
      "[300]\ttrain's rmse: 3.50063\ttest's rmse: 3.69632\n",
      "[400]\ttrain's rmse: 3.46029\ttest's rmse: 3.69317\n",
      "[500]\ttrain's rmse: 3.42978\ttest's rmse: 3.69129\n",
      "[600]\ttrain's rmse: 3.40363\ttest's rmse: 3.68944\n",
      "[700]\ttrain's rmse: 3.3784\ttest's rmse: 3.68864\n",
      "[800]\ttrain's rmse: 3.35739\ttest's rmse: 3.68827\n",
      "[900]\ttrain's rmse: 3.33808\ttest's rmse: 3.68815\n",
      "[1000]\ttrain's rmse: 3.32054\ttest's rmse: 3.68752\n",
      "[1100]\ttrain's rmse: 3.3038\ttest's rmse: 3.68737\n",
      "[1200]\ttrain's rmse: 3.28778\ttest's rmse: 3.68677\n",
      "[1300]\ttrain's rmse: 3.2711\ttest's rmse: 3.68665\n",
      "[1400]\ttrain's rmse: 3.25503\ttest's rmse: 3.68735\n",
      "Early stopping, best iteration is:\n",
      "[1281]\ttrain's rmse: 3.2745\ttest's rmse: 3.68625\n",
      "Fold  8 RMSE : 3.686252\n",
      "Training until validation scores don't improve for 200 rounds.\n",
      "[100]\ttrain's rmse: 3.65398\ttest's rmse: 3.68363\n",
      "[200]\ttrain's rmse: 3.57032\ttest's rmse: 3.6392\n",
      "[300]\ttrain's rmse: 3.51592\ttest's rmse: 3.62267\n",
      "[400]\ttrain's rmse: 3.47707\ttest's rmse: 3.61506\n",
      "[500]\ttrain's rmse: 3.44555\ttest's rmse: 3.60975\n",
      "[600]\ttrain's rmse: 3.41819\ttest's rmse: 3.60713\n",
      "[700]\ttrain's rmse: 3.39335\ttest's rmse: 3.60503\n",
      "[800]\ttrain's rmse: 3.37037\ttest's rmse: 3.60393\n",
      "[900]\ttrain's rmse: 3.35018\ttest's rmse: 3.60429\n",
      "[1000]\ttrain's rmse: 3.33173\ttest's rmse: 3.60358\n",
      "[1100]\ttrain's rmse: 3.31279\ttest's rmse: 3.6039\n",
      "Early stopping, best iteration is:\n",
      "[996]\ttrain's rmse: 3.33247\ttest's rmse: 3.60355\n",
      "Fold  9 RMSE : 3.603548\n",
      "Training until validation scores don't improve for 200 rounds.\n",
      "[100]\ttrain's rmse: 3.64543\ttest's rmse: 3.72249\n",
      "[200]\ttrain's rmse: 3.55965\ttest's rmse: 3.69444\n",
      "[300]\ttrain's rmse: 3.50372\ttest's rmse: 3.68726\n",
      "[400]\ttrain's rmse: 3.46319\ttest's rmse: 3.68428\n",
      "[500]\ttrain's rmse: 3.43227\ttest's rmse: 3.68274\n",
      "[600]\ttrain's rmse: 3.40583\ttest's rmse: 3.6821\n",
      "[700]\ttrain's rmse: 3.38155\ttest's rmse: 3.68228\n",
      "Early stopping, best iteration is:\n",
      "[570]\ttrain's rmse: 3.41324\ttest's rmse: 3.6819\n",
      "Fold 10 RMSE : 3.681901\n",
      "Final RMSE : 3.644745 \n",
      "\n",
      "\n",
      "\n",
      "***Run LightGBM with kfold - done in 707s\n",
      "***combine solutions - done in 0s\n",
      "***Full model run - done in 709s\n",
      "CPU times: user 2h 10min 32s, sys: 13.9 s, total: 2h 10min 46s\n",
      "Wall time: 11min 49s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "load_ver = 13\n",
    "write_ver = 13\n",
    "feat_gen_opt = False\n",
    "debug = False\n",
    "FEATS_EXCLUDED = ['first_active_month', 'target', 'card_id', 'outliers',\n",
    "                  'hist_purchase_date_max', 'hist_purchase_date_min', 'hist_card_id_size',\n",
    "                  'new_purchase_date_max', 'new_purchase_date_min', 'new_card_id_size',\n",
    "                  'OOF_PRED', 'month_0',\n",
    "                  'feature_1_2_cross_10','feature_1_2_cross_13','feature_2_2','feature_1_2_cross_7','feature_1_2_cross_1','feature_2_3_cross_3',\n",
    "                 ]\n",
    "\n",
    "submission_file_name = \"single_model_v\"+str(write_ver)\n",
    "with timer(\"Full model run\"):\n",
    "    # main(debug=False)\n",
    "    num_rows = 1000000 if debug else None\n",
    "    if feat_gen_opt:\n",
    "        with timer(\"train & test\"):\n",
    "            df = train_test(num_rows)\n",
    "            hist = transactions('hist', num_rows)\n",
    "            new = transactions('new', num_rows)\n",
    "        with open('../input/single_model_v'+str(write_ver)+'_data_part1.pkl', 'wb') as f:\n",
    "            pickle.dump([df, hist, new], f)\n",
    "        with timer(\"historical transactions\"):\n",
    "            df = pd.merge(df, hist, on='card_id', how='outer')\n",
    "        with timer(\"new merchants\"):\n",
    "            df = pd.merge(df, new, on='card_id', how='outer')\n",
    "        with timer(\"additional features\"):\n",
    "            df = additional_features(df)\n",
    "        with timer(\"split train & test\"):\n",
    "            train_df = df[df['target'].notnull()]\n",
    "            test_df = df[df['target'].isnull()]\n",
    "            del df\n",
    "            gc.collect()\n",
    "        with open('../input/single_model_v'+str(write_ver)+'_data_part2.pkl', 'wb') as f:\n",
    "            pickle.dump([train_df, test_df], f)\n",
    "        with timer(\"cleaning\"):\n",
    "            train_df_cleaned, test_df_cleaned = cleaning(train_df, test_df)\n",
    "        with open('../input/single_model_v'+str(write_ver)+'_data_part3.pkl', 'wb') as f:\n",
    "            pickle.dump([train_df_cleaned, test_df_cleaned], f)\n",
    "    else:\n",
    "        with open('../input/single_model_v'+str(load_ver)+'_data_part3.pkl', 'rb') as f:\n",
    "            [train_df_cleaned, test_df_cleaned] = pickle.load(f)\n",
    "            print(\"train_df_cleaned size: \" + str(train_df_cleaned.shape))\n",
    "            print(\"test_df_cleaned size: \" + str(test_df_cleaned.shape))\n",
    "    \n",
    "    with open('../input/single_model_v'+str(load_ver)+'_data_lgb_feat_sorted.pkl', 'rb') as f:\n",
    "        feat_sorted = pickle.load(f)\n",
    "    \n",
    "    with timer(\"Run LightGBM with kfold\"):\n",
    "        # feature_importance_df, sub_preds, feat_sorted = kfold_model(train_df_cleaned, test_df_cleaned, \n",
    "        #                                                             num_folds=10, model='lgb',\n",
    "        #                                                             stratified=False, debug=debug, saveOpt=True,\n",
    "        #                                                             feat_sorted=[], num_feat=None)\n",
    "        # with open('../input/single_model_v'+str(load_ver)+'_data_lgb_feat_sorted.pkl', 'wb') as f:\n",
    "        #     pickle.dump([feat_sorted], f)\n",
    "\n",
    "        feature_importance_df_1, sub_preds_1, feat_sorted_1 = kfold_model(train_df_cleaned, test_df_cleaned, \n",
    "                                                                          num_folds=10, model='lgb',\n",
    "                                                                          stratified=False, debug=debug, saveOpt=True,\n",
    "                                                                          feat_sorted=feat_sorted, num_feat=100)\n",
    "        feature_importance_df_2, sub_preds_2, feat_sorted_2 = kfold_model(train_df_cleaned, test_df_cleaned, \n",
    "                                                                          num_folds=10, model='lgb',\n",
    "                                                                          stratified=True, debug=debug, saveOpt=True,\n",
    "                                                                          feat_sorted=feat_sorted, num_feat=200)\n",
    "#         feature_importance_df_3, sub_preds_3, feat_sorted_3 = kfold_model(train_df_cleaned, test_df_cleaned, \n",
    "#                                                                           num_folds=10, model='xgb',\n",
    "#                                                                           stratified=False, debug=debug, saveOpt=True,\n",
    "#                                                                           feat_sorted=feat_sorted, num_feat=100)\n",
    "#         feature_importance_df_4, sub_preds_4, feat_sorted_4 = kfold_model(train_df_cleaned, test_df_cleaned,\n",
    "#                                                                           num_folds=10, model='xgb',\n",
    "#                                                                           stratified=True, debug=debug, saveOpt=True,\n",
    "#                                                                           feat_sorted=feat_sorted, num_feat=200)\n",
    "    with timer(\"combine solutions\"):\n",
    "        # sub_preds_list = [sub_preds_1, sub_preds_2, sub_preds_3, sub_preds_4]\n",
    "        sub_preds_list = [sub_preds_1, sub_preds_2]\n",
    "        combine_solution(test_df_cleaned, sub_preds_list, submission_file_name)\n",
    "    # with open('../input/single_model_v'+str(write_ver)+'_data_part4.pkl', 'wb') as f:\n",
    "    #    feat_summary_1 = feature_importance_df_1[[\"feature\", \"importance\"]].groupby(\"feature\").mean().sort_values(by=\"importance\", ascending=False).reset_index()\n",
    "    #    feat_summary_2 = feature_importance_df_2[[\"feature\", \"importance\"]].groupby(\"feature\").mean().sort_values(by=\"importance\", ascending=False).reset_index()\n",
    "    #    pickle.dump([feat_summary_1, feat_summary_2], f)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`single_model_v13_final.csv` got 3.694 on LB - same as the original script."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "oldHeight": 436.77078,
   "position": {
    "height": "458.969px",
    "left": "1554px",
    "right": "20px",
    "top": "139.984px",
    "width": "278.969px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "varInspector_section_display": "block",
   "window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
