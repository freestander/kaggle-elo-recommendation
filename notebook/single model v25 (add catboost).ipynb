{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Single Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "import gc\n",
    "import lightgbm as lgb\n",
    "import xgboost as xgb\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import time\n",
    "import warnings\n",
    "import xlearn as xl\n",
    "import scipy.sparse as sp\n",
    "import catboost as cb\n",
    "\n",
    "from contextlib import contextmanager\n",
    "from pandas.core.common import SettingWithCopyWarning\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import KFold, StratifiedKFold, RepeatedStratifiedKFold\n",
    "import numba\n",
    "import pickle\n",
    "\n",
    "import h2o\n",
    "from h2o.automl import H2OAutoML\n",
    "\n",
    "\n",
    "warnings.simplefilter(action='ignore', category=SettingWithCopyWarning)\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "\n",
    "pd.set_option('display.max_rows', 1000)\n",
    "pd.set_option('display.max_columns', 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "@contextmanager\n",
    "def timer(title):\n",
    "    t0 = time.time()\n",
    "    yield\n",
    "    print(\"***{} - done in {:.0f}s\".format(title, time.time() - t0))\n",
    "\n",
    "# rmse\n",
    "@numba.jit\n",
    "def rmse(y_true, y_pred):\n",
    "    return np.sqrt(mean_squared_error(y_true, y_pred))\n",
    "\n",
    "# One-hot encoding for categorical columns with get_dummies\n",
    "def one_hot_encoder(df, nan_as_category = True):\n",
    "    original_columns = list(df.columns)\n",
    "    categorical_columns = [col for col in df.columns if df[col].dtype == 'object']\n",
    "    df = pd.get_dummies(df, columns= categorical_columns, dummy_na= nan_as_category)\n",
    "    new_columns = [c for c in df.columns if c not in original_columns]\n",
    "    return df, new_columns\n",
    "    \n",
    "# Display/plot feature importance\n",
    "def display_importances(feature_importance_df_, model, straified_opt, figsize=(16, 50)):\n",
    "    # cols = feature_importance_df_[[\"feature\", \"importance\"]].groupby(\"feature\").mean().sort_values(by=\"importance\", ascending=False)[:40].index\n",
    "    cols = feature_importance_df_[[\"feature\", \"importance\"]].groupby(\"feature\").mean().sort_values(by=\"importance\", ascending=False).index\n",
    "    best_features = feature_importance_df_.loc[feature_importance_df_.feature.isin(cols)]\n",
    "\n",
    "    plt.figure(figsize=figsize)\n",
    "    sns.barplot(x=\"importance\", y=\"feature\", data=best_features.sort_values(by=\"importance\", ascending=False))\n",
    "    if model == 'lgb':\n",
    "        model_name = 'Lightgbm'\n",
    "    elif model == 'xgb':\n",
    "        model_name = 'Xgboost'\n",
    "    elif model == 'cat':\n",
    "        model_name = 'Catboost'\n",
    "    plt.title(model_name + ' Features (avg over folds)')\n",
    "    plt.tight_layout()\n",
    "    if straified_opt:\n",
    "        plt.savefig('../img/single_model_v'+str(write_ver)+'_importances_'+model+'_straified.png')\n",
    "    else:\n",
    "        plt.savefig('../img/single_model_v'+str(write_ver)+'_importances_'+model+'.png')\n",
    "\n",
    "# reduce memory\n",
    "def reduce_mem_usage(df, verbose=True):\n",
    "    numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n",
    "    start_mem = df.memory_usage().sum() / 1024**2\n",
    "    for col in df.columns:\n",
    "        col_type = df[col].dtypes\n",
    "        if col_type in numerics:\n",
    "            c_min = df[col].min()\n",
    "            c_max = df[col].max()\n",
    "            if str(col_type)[:3] == 'int':\n",
    "                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n",
    "                    df[col] = df[col].astype(np.int8)\n",
    "                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n",
    "                    df[col] = df[col].astype(np.int16)\n",
    "                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n",
    "                    df[col] = df[col].astype(np.int32)\n",
    "                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n",
    "                    df[col] = df[col].astype(np.int64)\n",
    "            else:\n",
    "                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n",
    "                    df[col] = df[col].astype(np.float16)\n",
    "                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n",
    "                    df[col] = df[col].astype(np.float32)\n",
    "                else:\n",
    "                    df[col] = df[col].astype(np.float64)\n",
    "\n",
    "    end_mem = df.memory_usage().sum() / 1024**2\n",
    "    print('Memory usage after optimization is: {:.2f} MB'.format(end_mem))\n",
    "    print('Decreased by {:.1f}%'.format(100 * (start_mem - end_mem) / start_mem))\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "def cleaning(train_df, test_df):\n",
    "    train_df = train_df.replace([np.inf, -np.inf], np.nan)\n",
    "    test_df = test_df.replace([np.inf, -np.inf], np.nan)\n",
    "\n",
    "    train_df = train_df.fillna(0)\n",
    "    test_df = test_df.fillna(0)\n",
    "    return train_df, test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocessing train & test\n",
    "def train_test(num_rows=None, debug=False):\n",
    "\n",
    "    # load csv\n",
    "    train_df = pd.read_csv('../input/train.csv', index_col=['card_id'], nrows=num_rows)\n",
    "    test_df = pd.read_csv('../input/test.csv', index_col=['card_id'], nrows=num_rows)\n",
    "\n",
    "    print(\"Train samples: {}, test samples: {}\".format(len(train_df), len(test_df)))\n",
    "\n",
    "    # outlier\n",
    "    train_df['outliers'] = 0\n",
    "    train_df.loc[train_df['target'] < -30, 'outliers'] = 1\n",
    "\n",
    "    # set target as nan\n",
    "    test_df['target'] = np.nan\n",
    "\n",
    "    # merge\n",
    "    df = train_df.append(test_df)\n",
    "\n",
    "    del train_df, test_df\n",
    "    gc.collect()\n",
    "\n",
    "    feat_list = ['feature_1', 'feature_2', 'feature_3']\n",
    "    for feat in feat_list:\n",
    "        df[feat + '_orig'] = df[feat]\n",
    "    \n",
    "    df['feature_1_2_cross'] = df['feature_1_orig'].astype(str).add('-').add(df['feature_2_orig'].astype(str))\n",
    "    df['feature_1_3_cross'] = df['feature_1_orig'].astype(str).add('-').add(df['feature_3_orig'].astype(str))\n",
    "    df['feature_2_3_cross'] = df['feature_2_orig'].astype(str).add('-').add(df['feature_3_orig'].astype(str))\n",
    "    \n",
    "    if debug:\n",
    "        print_train_test_feat_cross(df)\n",
    "    \n",
    "    df['feature_1_2_cross'] = df['feature_1_2_cross'].map({'1-1': 0, '1-2': 1, '1-3': 2,\n",
    "                                                           '2-1': 3, '2-2': 4, '2-3': 5, \n",
    "                                                           '3-1': 6, '3-2': 7, '3-3': 8, \n",
    "                                                           '4-1': 9, '4-2': 10, '4-3': 11,\n",
    "                                                           '5-1': 12, '5-2': 13\n",
    "                                                          }).astype(int)\n",
    "    df['feature_1_3_cross'] = df['feature_1_3_cross'].map({'1-0': 0, '2-0': 1, '3-1': 2, '4-0': 3, '5-1': 4\n",
    "                                                          }).astype(int)\n",
    "    df['feature_2_3_cross'] = df['feature_2_3_cross'].map({'1-0': 0, '1-1': 1, '2-0': 2, '2-1': 3, '3-0': 4, '3-1': 5\n",
    "                                                          }).astype(int)\n",
    "    \n",
    "    df = pd.get_dummies(df, columns=['feature_1', 'feature_2', 'feature_3', \n",
    "                                     'feature_1_2_cross', 'feature_1_3_cross', 'feature_2_3_cross'])\n",
    "    for feat in feat_list:\n",
    "        df[feat] = df[feat + '_orig'] \n",
    "    \n",
    "    # to datetime\n",
    "    df['first_active_month'] = pd.to_datetime(df['first_active_month'])\n",
    "\n",
    "    # datetime features\n",
    "    # df['quarter'] = df['first_active_month'].dt.quarter\n",
    "    df['elapsed_time'] = (datetime.datetime.today() - df['first_active_month']).dt.days\n",
    "\n",
    "    df['days_feature1'] = df['elapsed_time'] * df['feature_1']\n",
    "    df['days_feature2'] = df['elapsed_time'] * df['feature_2']\n",
    "    df['days_feature3'] = df['elapsed_time'] * df['feature_3']\n",
    "\n",
    "    df['days_feature1_ratio'] = df['feature_1'] / df['elapsed_time']\n",
    "    df['days_feature2_ratio'] = df['feature_2'] / df['elapsed_time']\n",
    "    df['days_feature3_ratio'] = df['feature_3'] / df['elapsed_time']\n",
    "\n",
    "    # one hot encoding\n",
    "    df, cols = one_hot_encoder(df, nan_as_category=False)\n",
    "\n",
    "    for f in ['feature_1','feature_2','feature_3']:\n",
    "        order_label = df.groupby([f])['outliers'].mean()\n",
    "        df[f] = df[f].map(order_label)\n",
    "\n",
    "    df['feature_sum'] = df['feature_1'] + df['feature_2'] + df['feature_3']\n",
    "    df['feature_mean'] = df['feature_sum']/3\n",
    "    df['feature_max'] = df[['feature_1', 'feature_2', 'feature_3']].max(axis=1)\n",
    "    df['feature_min'] = df[['feature_1', 'feature_2', 'feature_3']].min(axis=1)\n",
    "    df['feature_var'] = df[['feature_1', 'feature_2', 'feature_3']].std(axis=1)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fill_na(df):\n",
    "    df['category_2'].fillna(1.0,inplace=True)\n",
    "    df[\"category_2\"] = df[\"category_2\"].astype(int)\n",
    "    df['category_3'].fillna('A',inplace=True)\n",
    "    df['merchant_id'].fillna('M_ID_00a6ca8a8a',inplace=True)\n",
    "    df['installments'].replace(-1, np.nan,inplace=True)\n",
    "    df['installments'].replace(999, np.nan,inplace=True)\n",
    "    \n",
    "    return df\n",
    "\n",
    "def encode_to_numeric(df):\n",
    "    df['authorized_flag'] = df['authorized_flag'].map({'Y': 1, 'N': 0}).astype(int)\n",
    "    df['category_1'] = df['category_1'].map({'Y': 1, 'N': 0}).astype(int)\n",
    "    df['category_3'] = df['category_3'].map({'A':0, 'B':1, 'C':2})\n",
    "    # df['category_4'] = df['category_4'].map({'Y': 2, 'N': 1, 'NaN':0}).astype(int)\n",
    "    \n",
    "    return df\n",
    "\n",
    "def gen_datetime(df):\n",
    "    df['purchase_date'] = pd.to_datetime(df['purchase_date'])\n",
    "    df['month'] = df['purchase_date'].dt.month\n",
    "    df['day'] = df['purchase_date'].dt.day\n",
    "    df['hour'] = df['purchase_date'].dt.hour\n",
    "    df['weekofyear'] = df['purchase_date'].dt.weekofyear\n",
    "    df['weekday'] = df['purchase_date'].dt.weekday\n",
    "    df['weekend'] = (df['purchase_date'].dt.weekday >=5).astype(int)\n",
    "    \n",
    "    return df\n",
    "\n",
    "\n",
    "def convert_categorical(df):\n",
    "    \n",
    "    feat_list = ['category_1', 'category_2', 'category_3', 'subsector_id']\n",
    "    \n",
    "    df['category_1_2_cross'] = df['category_1'].astype(str).add('_').add(df['category_2'].astype(str))\n",
    "    df['category_1_3_cross'] = df['category_1'].astype(str).add('_').add(df['category_3'].astype(str))\n",
    "    df['category_2_3_cross'] = df['category_2'].astype(str).add('_').add(df['category_3'].astype(str))\n",
    "    # df['category_1_2_3_cross'] = df['category_1'].astype(str)\\\n",
    "    #                                .add('_').add(df['category_2'].astype(str))\\\n",
    "    #                                .add('_').add(df['category_3'].astype(str))\n",
    "    \n",
    "    for feat in feat_list:\n",
    "        df[feat + '_orig'] = df[feat]\n",
    "        \n",
    "    df = pd.get_dummies(df, columns=['category_1', 'category_2', 'category_3', \n",
    "                                     'category_1_2_cross', 'category_1_3_cross', 'category_2_3_cross',\n",
    "                                     'subsector_id'])\n",
    "    \n",
    "    drop_feat_list = ['category_2_0', 'category_3_0', 'category_4_0', \n",
    "                      'category_1_2_cross_0_0', 'category_1_2_cross_1_0', 'category_1_3_cross_0_0', \n",
    "                      'category_1_3_cross_1_0', 'category_2_3_cross_0_0', 'category_2_3_cross_0_1', \n",
    "                      'category_2_3_cross_0_2', 'category_2_3_cross_0_3', 'category_2_3_cross_1_0', \n",
    "                      'category_2_3_cross_2_0', 'category_2_3_cross_3_0', 'category_2_3_cross_4_0', \n",
    "                      'category_2_3_cross_5_0', \n",
    "                      'subsector_id_-1']\n",
    "    for feat in drop_feat_list:\n",
    "        if feat in df.columns:\n",
    "            df = df.drop([feat], axis=1)\n",
    "    \n",
    "    for feat in feat_list:\n",
    "        df = df.rename(index=str, columns={feat + '_orig': feat})\n",
    "    \n",
    "    for i in range(1, 42):\n",
    "        if i != 6:\n",
    "            df['subsector_id_'+str(i)+'_purchase_amount'] = df['subsector_id_'+str(i)] * df['purchase_amount']\n",
    "            \n",
    "    return df\n",
    "\n",
    "\n",
    "def gen_other_feat(df):\n",
    "    df['price'] = df['purchase_amount'] / df['installments']\n",
    "\n",
    "    \n",
    "    day_list = [7]\n",
    "    for day in day_list:\n",
    "#         #2017\n",
    "#         #New year\n",
    "#         df['new_year_2017_'+str(day)]=(pd.to_datetime('2017-01-01')-df['purchase_date']).dt.days.apply(lambda x: x if x >= 0 and x <= day else 0)\n",
    "#         #Tiradentes Day: April 21, 2017\n",
    "#         df['Tiradentes_2017_'+str(day)]=(pd.to_datetime('2017-04-21')-df['purchase_date']).dt.days.apply(lambda x: x if x >= 0 and x < day else 0)\n",
    "#         #Labor Day: May 1, 2017\n",
    "#         df['labor_2017_'+str(day)]=(pd.to_datetime('2017-05-01')-df['purchase_date']).dt.days.apply(lambda x: x if x >= 0 and x < day else 0)\n",
    "#         #Mothers Day: May 14 2017\n",
    "#         df['Mothers_Day_2017_'+str(day)]=(pd.to_datetime('2017-06-04')-df['purchase_date']).dt.days.apply(lambda x: x if x >= 0 and x < day else 0)\n",
    "#         #Valentine's Day : 12th June, 2017\n",
    "#         df['Valentine_Day_2017_'+str(day)]=(pd.to_datetime('2017-06-12')-df['purchase_date']).dt.days.apply(lambda x: x if x >= 0 and x < day else 0)\n",
    "#         #fathers day: August 13 2017\n",
    "#         df['fathers_day_2017_'+str(day)]=(pd.to_datetime('2017-08-13')-df['purchase_date']).dt.days.apply(lambda x: x if x >= 0 and x < day else 0)\n",
    "#         #Christmas : December 25 2017\n",
    "#         df['Christmas_Day_2017_'+str(day)]=(pd.to_datetime('2017-12-25')-df['purchase_date']).dt.days.apply(lambda x: x if x >= 0 and x <= day else 0)\n",
    "#         #Black Friday : 24th November 2017\n",
    "#         df['Black_Friday_2017_'+str(day)]=(pd.to_datetime('2017-11-24') - df['purchase_date']).dt.days.apply(lambda x: x if x >= 0 and x < day else 0)\n",
    "#         #Childrens day: October 12 2017\n",
    "#         df['Children_day_2017_'+str(day)]=(pd.to_datetime('2017-10-12')-df['purchase_date']).dt.days.apply(lambda x: x if x >= 0 and x < day else 0)\n",
    "#         #2018\n",
    "#         #New year\n",
    "#         df['new_year_2018_'+str(day)]=(pd.to_datetime('2018-01-01')-df['purchase_date']).dt.days.apply(lambda x: x if x >= 0 and x <= day else 0)\n",
    "#         #Mothers Day: May 13 2018\n",
    "#         df['Mothers_Day_2018_'+str(day)]=(pd.to_datetime('2018-05-13')-df['purchase_date']).dt.days.apply(lambda x: x if x > 0 and x < day else 0)\n",
    "        \n",
    "        #2017\n",
    "        #New year\n",
    "        df['new_year_2017_'+str(day)+'_amt']=(pd.to_datetime('2017-01-01')-df['purchase_date']).dt.days.apply(lambda x: 1 if x >= 0 and x <= day else 0) * df['purchase_amount']\n",
    "        #Valentine's Day : 12th June, 2017\n",
    "        df['Valentine_Day_2017_'+str(day)+'_amt']=(pd.to_datetime('2017-02-14')-df['purchase_date']).dt.days.apply(lambda x: 1 if x >= 0 and x < day else 0) * df['purchase_amount']\n",
    "        #Tiradentes Day: April 21, 2017\n",
    "        df['Tiradentes_2017_'+str(day)+'_amt']=(pd.to_datetime('2017-04-21')-df['purchase_date']).dt.days.apply(lambda x: 1 if x >= 0 and x < day else 0) * df['purchase_amount']\n",
    "        #Labor Day: May 1, 2017\n",
    "        df['labor_2017_'+str(day)+'_amt']=(pd.to_datetime('2017-05-01')-df['purchase_date']).dt.days.apply(lambda x: 1 if x >= 0 and x < day else 0) * df['purchase_amount']\n",
    "        #Mothers Day: May 14 2017\n",
    "        df['Mothers_Day_2017_'+str(day)+'_amt']=(pd.to_datetime('2017-05-14')-df['purchase_date']).dt.days.apply(lambda x: 1 if x >= 0 and x < day else 0) * df['purchase_amount']\n",
    "        #Corpus Christi\n",
    "        df['Corpus_christi_2017_'+str(day)+'_amt']=(pd.to_datetime('2017-06-20')-df['purchase_date']).dt.days.apply(lambda x: 1 if x >= 0 and x < day else 0) * df['purchase_amount']\n",
    "        #fathers day: August 13 2017\n",
    "        df['fathers_day_2017_'+str(day)+'_amt']=(pd.to_datetime('2017-08-13')-df['purchase_date']).dt.days.apply(lambda x: 1 if x >= 0 and x < day else 0) * df['purchase_amount']\n",
    "        #Independence: Sep 7 2017\n",
    "        df['independence_2017_'+str(day)+'_amt']=(pd.to_datetime('2017-09-07')-df['purchase_date']).dt.days.apply(lambda x: 1 if x >= 0 and x < day else 0) * df['purchase_amount']\n",
    "        #Childrens day: October 12 2017\n",
    "        df['Children_day_2017_'+str(day)+'_amt']=(pd.to_datetime('2017-10-12')-df['purchase_date']).dt.days.apply(lambda x: 1 if x >= 0 and x < day else 0) * df['purchase_amount']\n",
    "        #All soul's : 2th November 2017\n",
    "        df['All_souls_2017_'+str(day)+'_amt']=(pd.to_datetime('2017-11-02') - df['purchase_date']).dt.days.apply(lambda x: 1 if x >= 0 and x < day else 0) * df['purchase_amount']\n",
    "        #Republic day's : 2th November 2017\n",
    "        df['Republic_day_2017_'+str(day)+'_amt']=(pd.to_datetime('2017-11-15') - df['purchase_date']).dt.days.apply(lambda x: 1 if x >= 0 and x < day else 0) * df['purchase_amount']\n",
    "        #Black Friday : 24th November 2017\n",
    "        df['Black_Friday_2017_'+str(day)+'_amt']=(pd.to_datetime('2017-11-24') - df['purchase_date']).dt.days.apply(lambda x: 1 if x >= 0 and x < day else 0) * df['purchase_amount']\n",
    "        #Christmas : December 25 2017\n",
    "        df['Christmas_Day_2017_'+str(day)+'_amt']=(pd.to_datetime('2017-12-25')-df['purchase_date']).dt.days.apply(lambda x: 1 if x >= 0 and x <= day else 0) * df['purchase_amount']\n",
    "        #2018\n",
    "        #New year\n",
    "        df['new_year_2018_'+str(day)+'_amt']=(pd.to_datetime('2018-01-01')-df['purchase_date']).dt.days.apply(lambda x: 1 if x >= 0 and x <= day else 0) * df['purchase_amount']\n",
    "        #Valentine's Day : 12th June, 2017\n",
    "        df['Valentine_Day_2018_'+str(day)+'_amt']=(pd.to_datetime('2018-02-14')-df['purchase_date']).dt.days.apply(lambda x: 1 if x >= 0 and x < day else 0) * df['purchase_amount']\n",
    "        #Mothers Day: May 13 2018\n",
    "        df['Mothers_Day_2018_'+str(day)+'_amt']=(pd.to_datetime('2018-05-13')-df['purchase_date']).dt.days.apply(lambda x: 1 if x > 0 and x < day else 0) * df['purchase_amount']\n",
    "        \n",
    "    df['month_diff'] = ((datetime.datetime.today() - df['purchase_date']).dt.days)//30\n",
    "    df['month_diff'] += df['month_lag']\n",
    "    \n",
    "    df['category_1_0_month_diff'] = df['month_diff'] * df['category_1_0']\n",
    "    df['category_1_1_month_diff'] = df['month_diff'] * df['category_1_1']\n",
    "    df['category_2_1_month_diff'] = df['month_diff'] * df['category_2_1']\n",
    "    df['category_2_2_month_diff'] = df['month_diff'] * df['category_2_2']\n",
    "    df['category_2_3_month_diff'] = df['month_diff'] * df['category_2_3']\n",
    "    df['category_2_4_month_diff'] = df['month_diff'] * df['category_2_4']\n",
    "    df['category_2_5_month_diff'] = df['month_diff'] * df['category_2_5']\n",
    "    df['category_3_1_month_diff'] = df['month_diff'] * df['category_3_1']\n",
    "    df['category_3_2_month_diff'] = df['month_diff'] * df['category_3_2']\n",
    "\n",
    "    # additional features\n",
    "    df['duration'] = df['purchase_amount']*df['month_diff']\n",
    "    df['amount_month_ratio'] = df['purchase_amount']/df['month_diff']\n",
    "    \n",
    "    # duration by category\n",
    "    df['category_1_0_duration'] = df['duration'] * df['category_1_0']\n",
    "    df['category_1_1_duration'] = df['duration'] * df['category_1_1']\n",
    "    df['category_2_1_duration'] = df['duration'] * df['category_2_1']\n",
    "    df['category_2_2_duration'] = df['duration'] * df['category_2_2']\n",
    "    df['category_2_3_duration'] = df['duration'] * df['category_2_3']\n",
    "    df['category_2_4_duration'] = df['duration'] * df['category_2_4']\n",
    "    df['category_2_5_duration'] = df['duration'] * df['category_2_5']\n",
    "    df['category_3_1_duration'] = df['duration'] * df['category_3_1']\n",
    "    df['category_3_2_duration'] = df['duration'] * df['category_3_2']\n",
    "    \n",
    "    # installments by category\n",
    "    df['category_1_0_installments'] = df['installments'] * df['category_1_0']\n",
    "    df['category_1_1_installments'] = df['installments'] * df['category_1_1']\n",
    "    df['category_2_1_installments'] = df['installments'] * df['category_2_1']\n",
    "    df['category_2_2_installments'] = df['installments'] * df['category_2_2']\n",
    "    df['category_2_3_installments'] = df['installments'] * df['category_2_3']\n",
    "    df['category_2_4_installments'] = df['installments'] * df['category_2_4']\n",
    "    df['category_2_5_installments'] = df['installments'] * df['category_2_5']\n",
    "    df['category_3_1_installments'] = df['installments'] * df['category_3_1']\n",
    "    df['category_3_2_installments'] = df['installments'] * df['category_3_2']\n",
    "    \n",
    "    return df\n",
    "\n",
    "\n",
    "def feat_agg(df):\n",
    "    col_unique =['subsector_id', 'merchant_id', 'merchant_category_id']\n",
    "    col_seas = ['month', 'hour', 'weekofyear', 'weekday', 'day']\n",
    "    col_cat = [feat for feat in list(df) if 'state_id_' in feat or \n",
    "                                                 'subsector_id_' in feat]\n",
    "    # col_cat = [feat for feat in list(df) if 'state_id_' in feat or \n",
    "    #                                             'subsector_id_' in feat or\n",
    "    #                                             'most_recent_sales_range_' in feat or\n",
    "    #                                             'most_recent_purchases_range_' in feat]\n",
    "\n",
    "    aggs = {}\n",
    "    for col in col_unique:\n",
    "        aggs[col] = ['nunique']\n",
    "\n",
    "    for col in col_seas:\n",
    "        aggs[col] = ['nunique', 'mean', 'min', 'max']\n",
    "        \n",
    "    for col in col_cat:\n",
    "        aggs[col] = ['nunique', 'mean']\n",
    "\n",
    "    aggs['purchase_amount'] = ['sum','max','min','mean','var','skew']\n",
    "    aggs['installments'] = ['sum','max','mean','var','skew']\n",
    "    aggs['category_1_0_installments'] = ['sum','max','mean','var']\n",
    "    aggs['category_1_1_installments'] = ['sum','max','mean','var']\n",
    "    aggs['category_2_1_installments'] = ['sum','max','mean','var']\n",
    "    aggs['category_2_2_installments'] = ['sum','max','mean','var']\n",
    "    aggs['category_2_3_installments'] = ['sum','max','mean','var']\n",
    "    aggs['category_2_4_installments'] = ['sum','max','mean','var']\n",
    "    aggs['category_2_5_installments'] = ['sum','max','mean','var']\n",
    "    aggs['category_3_1_installments'] = ['sum','max','mean','var']\n",
    "    aggs['category_3_2_installments'] = ['sum','max','mean','var']\n",
    "    aggs['purchase_date'] = ['max','min']\n",
    "    aggs['month_lag'] = ['max','min','mean','var','skew']\n",
    "    aggs['month_diff'] = ['max','min','mean','var','skew']\n",
    "    aggs['category_1_0_month_diff'] = ['max','min','mean','var']\n",
    "    aggs['category_1_1_month_diff'] = ['max','min','mean','var']\n",
    "    aggs['category_2_1_month_diff'] = ['max','min','mean','var']\n",
    "    aggs['category_2_2_month_diff'] = ['max','min','mean','var']\n",
    "    aggs['category_2_3_month_diff'] = ['max','min','mean','var']\n",
    "    aggs['category_2_4_month_diff'] = ['max','min','mean','var']\n",
    "    aggs['category_2_5_month_diff'] = ['max','min','mean','var']\n",
    "    aggs['category_3_1_month_diff'] = ['max','min','mean','var']\n",
    "    aggs['category_3_2_month_diff'] = ['max','min','mean','var']\n",
    "    aggs['authorized_flag'] = ['mean']\n",
    "    aggs['weekend'] = ['mean'] # overwrite\n",
    "    aggs['weekday'] = ['mean'] # overwrite\n",
    "    aggs['day'] = ['nunique', 'mean', 'min'] # overwrite\n",
    "    aggs['category_1'] = ['mean']\n",
    "    aggs['category_2'] = ['mean']\n",
    "    aggs['category_3'] = ['mean']\n",
    "    aggs['card_id'] = ['size','count']\n",
    "    aggs['price'] = ['sum','mean','max','min','var']\n",
    "    \n",
    "    day_list = [7]\n",
    "    for day in day_list:\n",
    "#         aggs['new_year_2017_'+str(day)] = ['mean']\n",
    "#         aggs['Tiradentes_2017_'+str(day)] = ['mean']\n",
    "#         aggs['labor_2017_'+str(day)] = ['mean']\n",
    "#         aggs['Mothers_Day_2017_'+str(day)] = ['mean']\n",
    "#         aggs['Valentine_Day_2017_'+str(day)] = ['mean']\n",
    "#         aggs['fathers_day_2017_'+str(day)] = ['mean']\n",
    "#         aggs['Children_day_2017_'+str(day)] = ['mean']\n",
    "#         aggs['Black_Friday_2017_'+str(day)] = ['mean']\n",
    "#         aggs['Christmas_Day_2017_'+str(day)] = ['mean']\n",
    "#         aggs['new_year_2018_'+str(day)] = ['mean']\n",
    "#         aggs['Mothers_Day_2018_'+str(day)] = ['mean']\n",
    "#        \n",
    "        aggs['new_year_2017_'+str(day)+'_amt'] = ['mean', 'min', 'max', 'sum']\n",
    "        aggs['Valentine_Day_2017_'+str(day)+'_amt'] = ['mean', 'min', 'max', 'sum']\n",
    "        aggs['Tiradentes_2017_'+str(day)+'_amt'] = ['mean', 'min', 'max', 'sum']\n",
    "        aggs['labor_2017_'+str(day)+'_amt'] = ['mean', 'min', 'max', 'sum']\n",
    "        aggs['Mothers_Day_2017_'+str(day)+'_amt'] = ['mean', 'min', 'max', 'sum']\n",
    "        aggs['Corpus_christi_2017_'+str(day)+'_amt'] = ['mean', 'min', 'max', 'sum']\n",
    "        aggs['fathers_day_2017_'+str(day)+'_amt'] = ['mean', 'min', 'max', 'sum']\n",
    "        aggs['independence_2017_'+str(day)+'_amt'] = ['mean', 'min', 'max', 'sum']\n",
    "        aggs['Children_day_2017_'+str(day)+'_amt'] = ['mean', 'min', 'max', 'sum']\n",
    "        aggs['All_souls_2017_'+str(day)+'_amt'] = ['mean', 'min', 'max', 'sum']\n",
    "        aggs['Republic_day_2017_'+str(day)+'_amt'] = ['mean', 'min', 'max', 'sum']\n",
    "        aggs['Black_Friday_2017_'+str(day)+'_amt'] = ['mean', 'min', 'max', 'sum']\n",
    "        aggs['Christmas_Day_2017_'+str(day)+'_amt'] = ['mean', 'min', 'max', 'sum']\n",
    "        aggs['new_year_2018_'+str(day)+'_amt'] = ['mean', 'min', 'max', 'sum']\n",
    "        aggs['Valentine_Day_2018_'+str(day)+'_amt'] = ['mean', 'min', 'max', 'sum']\n",
    "        aggs['Mothers_Day_2018_'+str(day)+'_amt'] = ['mean', 'min', 'max', 'sum']\n",
    "        \n",
    "    aggs['duration']=['mean','min','max','var','skew']\n",
    "    aggs['category_1_0_duration']=['mean','min','max','var']\n",
    "    aggs['category_1_1_duration']=['mean','min','max','var']\n",
    "    aggs['category_2_1_duration']=['mean','min','max','var']\n",
    "    aggs['category_2_2_duration']=['mean','min','max','var']\n",
    "    aggs['category_2_3_duration']=['mean','min','max','var']\n",
    "    aggs['category_2_4_duration']=['mean','min','max','var']\n",
    "    aggs['category_2_5_duration']=['mean','min','max','var']\n",
    "    aggs['category_3_1_duration']=['mean','min','max','var']\n",
    "    aggs['category_3_2_duration']=['mean','min','max','var']\n",
    "    aggs['amount_month_ratio']=['mean','min','max','var']\n",
    "    \n",
    "    for i in range(1, 42):\n",
    "        if i != 6:\n",
    "            aggs['subsector_id_'+str(i)+'_purchase_amount'] = ['mean', 'min', 'max']\n",
    "\n",
    "    for col in ['category_2','category_3']:\n",
    "        df[col+'_mean'] = df.groupby([col])['purchase_amount'].transform('mean')\n",
    "        df[col+'_min'] = df.groupby([col])['purchase_amount'].transform('min')\n",
    "        df[col+'_max'] = df.groupby([col])['purchase_amount'].transform('max')\n",
    "        df[col+'_sum'] = df.groupby([col])['purchase_amount'].transform('sum')\n",
    "        aggs[col+'_mean'] = ['mean']\n",
    "\n",
    "    df = df.reset_index().groupby('card_id').agg(aggs)\n",
    "    \n",
    "    df.columns = pd.Index([e[0] + \"_\" + e[1] for e in df.columns.tolist()])\n",
    "    \n",
    "    df['purchase_date_diff'] = (df['purchase_date_max']-df['purchase_date_min']).dt.days\n",
    "    df['purchase_date_average'] = df['purchase_date_diff']/df['card_id_size']\n",
    "    df['purchase_date_uptonow'] = (datetime.datetime.today()-df['purchase_date_max']).dt.days\n",
    "    df['purchase_date_uptomin'] = (datetime.datetime.today()-df['purchase_date_min']).dt.days\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = pd.read_csv('../input/historical_transactions.csv', nrows=None)\n",
    "# train_df = pd.read_csv('../input/train.csv', index_col=['card_id'], nrows=None)\n",
    "# df = pd.merge(df, train_df, on=['card_id'], how='left')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# outlier_cutoff = -15\n",
    "# outliers = df.loc[df['target'] < outlier_cutoff]\n",
    "# non_outliers = df.loc[df['target'] >= outlier_cutoff]\n",
    "# print('{:d} outliers found (target < {:d})'.format(outliers.shape[0], outlier_cutoff))\n",
    "\n",
    "# plt.figure(figsize=[10,5])\n",
    "# plt.suptitle('Outlier vs. non-outlier feature distributions', fontsize=20, y=1.1)\n",
    "\n",
    "# for num, col in enumerate(['feature_1', 'feature_2', 'feature_3', \n",
    "#                            'category_1', 'category_2', 'category_3', \n",
    "#                            'installments', 'target']):\n",
    "#     if col is not 'target':\n",
    "#         plt.subplot(3, 3, num+1)\n",
    "#         v_c = non_outliers[col].value_counts() / non_outliers.shape[0]\n",
    "#         plt.bar(v_c.index, v_c, label=('non-outliers'), align='edge', width=-0.3, edgecolor=[0.2]*3)\n",
    "#         v_c = outliers[col].value_counts() / outliers.shape[0]\n",
    "#         plt.bar(v_c.index, v_c, label=('outliers'), align='edge', width=0.3, edgecolor=[0.2]*3)\n",
    "#         plt.title(col)\n",
    "# #         plt.legend()\n",
    "\n",
    "# plt.tight_layout()\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocessing historical transactions\n",
    "def transactions(source, num_rows=None):\n",
    "    # load csv\n",
    "    if source == 'hist':\n",
    "        df = pd.read_csv('../input/historical_transactions.csv', nrows=num_rows)\n",
    "    elif source == 'new':\n",
    "        df = pd.read_csv('../input/new_merchant_transactions.csv', nrows=num_rows)\n",
    "    # merchant_df = pd.read_csv('../input/merchants.csv', nrows=None)\n",
    "    # df = pd.merge(df, merchant_df.drop(['category_1', 'category_2', 'city_id', \n",
    "    #                                               'merchant_category_id', 'subsector_id',\n",
    "    #                                               'state_id'], axis=1), on='merchant_id', how='left')\n",
    "    # del merchant_df\n",
    "    # gc.collect()\n",
    "    \n",
    "    # fillna\n",
    "    df = fill_na(df)\n",
    "    \n",
    "    # purchase_amount processing (https://www.kaggle.com/raddar/towards-de-anonymizing-the-data-some-insights)\n",
    "    # df['purchase_amount'] = df['purchase_amount'].apply(lambda x: min(x, 0.8))\n",
    "    df['purchase_amount'] = np.round(df['purchase_amount'] / 0.00150265118 + 497.06,8)\n",
    "\n",
    "    # Y/N to 1/0\n",
    "    df = encode_to_numeric(df)\n",
    "\n",
    "    # datetime features\n",
    "    df = gen_datetime(df)\n",
    "\n",
    "    # subsector_id and state_id features\n",
    "    df = convert_categorical(df)\n",
    "    \n",
    "    # additional features\n",
    "    df = gen_other_feat(df)\n",
    "\n",
    "    # reduce memory usage\n",
    "    df = reduce_mem_usage(df)\n",
    "\n",
    "    # agg features\n",
    "    df = feat_agg(df)\n",
    "\n",
    "    # change column name\n",
    "    if source == 'hist':\n",
    "        df.columns = ['hist_'+ c for c in df.columns]\n",
    "    elif source == 'new':\n",
    "        df.columns = ['new_'+ c for c in df.columns]\n",
    "\n",
    "    # reduce memory usage\n",
    "    df = reduce_mem_usage(df)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# additional features\n",
    "def additional_features(df):\n",
    "    df['hist_first_buy'] = (df['hist_purchase_date_min'] - df['first_active_month']).dt.days\n",
    "    df['hist_last_buy'] = (df['hist_purchase_date_max'] - df['first_active_month']).dt.days\n",
    "    df['new_first_buy'] = (df['new_purchase_date_min'] - df['first_active_month']).dt.days\n",
    "    df['new_last_buy'] = (df['new_purchase_date_max'] - df['first_active_month']).dt.days\n",
    "\n",
    "    date_features=['hist_purchase_date_max','hist_purchase_date_min',\n",
    "                   'new_purchase_date_max', 'new_purchase_date_min']\n",
    "\n",
    "    for f in date_features:\n",
    "        df[f] = df[f].astype(np.int64) * 1e-9\n",
    "\n",
    "    df['card_id_total'] = df['new_card_id_size']+df['hist_card_id_size']\n",
    "    df['card_id_cnt_total'] = df['new_card_id_count']+df['hist_card_id_count']\n",
    "    df['card_id_cnt_ratio'] = df['new_card_id_count']/df['hist_card_id_count']\n",
    "    df['purchase_amount_total'] = df['new_purchase_amount_sum']+df['hist_purchase_amount_sum']\n",
    "    df['purchase_amount_mean'] = df['new_purchase_amount_mean']+df['hist_purchase_amount_mean']\n",
    "    df['purchase_amount_max'] = df['new_purchase_amount_max']+df['hist_purchase_amount_max']\n",
    "    df['purchase_amount_min'] = df['new_purchase_amount_min']+df['hist_purchase_amount_min']\n",
    "    df['purchase_amount_ratio'] = df['new_purchase_amount_sum']/df['hist_purchase_amount_sum']\n",
    "    df['month_diff_mean'] = df['new_month_diff_mean']+df['hist_month_diff_mean']\n",
    "    df['month_diff_ratio'] = df['new_month_diff_mean']/df['hist_month_diff_mean']\n",
    "    df['month_lag_mean'] = df['new_month_lag_mean']+df['hist_month_lag_mean']\n",
    "    df['month_lag_max'] = df['new_month_lag_max']+df['hist_month_lag_max']\n",
    "    df['month_lag_min'] = df['new_month_lag_min']+df['hist_month_lag_min']\n",
    "    df['category_1_mean'] = df['new_category_1_mean']+df['hist_category_1_mean']\n",
    "    df['installments_total'] = df['new_installments_sum']+df['hist_installments_sum']\n",
    "    df['installments_mean'] = df['new_installments_mean']+df['hist_installments_mean']\n",
    "    df['installments_max'] = df['new_installments_max']+df['hist_installments_max']\n",
    "    df['installments_ratio'] = df['new_installments_sum']/df['hist_installments_sum']\n",
    "    df['price_total'] = df['purchase_amount_total'] / df['installments_total']\n",
    "    df['price_mean'] = df['purchase_amount_mean'] / df['installments_mean']\n",
    "    df['price_max'] = df['purchase_amount_max'] / df['installments_max']\n",
    "    df['duration_mean'] = df['new_duration_mean']+df['hist_duration_mean']\n",
    "    df['duration_min'] = df['new_duration_min']+df['hist_duration_min']\n",
    "    df['duration_max'] = df['new_duration_max']+df['hist_duration_max']\n",
    "    df['amount_month_ratio_mean']=df['new_amount_month_ratio_mean']+df['hist_amount_month_ratio_mean']\n",
    "    df['amount_month_ratio_min']=df['new_amount_month_ratio_min']+df['hist_amount_month_ratio_min']\n",
    "    df['amount_month_ratio_max']=df['new_amount_month_ratio_max']+df['hist_amount_month_ratio_max']\n",
    "    df['new_CLV'] = df['new_card_id_count'] * df['new_purchase_amount_sum'] / df['new_month_diff_mean']\n",
    "    df['hist_CLV'] = df['hist_card_id_count'] * df['hist_purchase_amount_sum'] / df['hist_month_diff_mean']\n",
    "    df['CLV_ratio'] = df['new_CLV'] / df['hist_CLV']\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def kfold_model(train_df, test_df, num_folds, model,\n",
    "                stratified=False, debug=False, saveOpt=True,\n",
    "                feat_sorted=[], num_feat=None):\n",
    "    if model == 'lgb':\n",
    "        print(\"Starting LightGBM. Train shape: {}, test shape: {}\".format(train_df.shape, test_df.shape))\n",
    "    elif model == 'xgb':\n",
    "        print(\"Starting Xgboost. Train shape: {}, test shape: {}\".format(train_df.shape, test_df.shape))\n",
    "\n",
    "    # Cross validation model\n",
    "    if stratified:\n",
    "        n_repeats = 2\n",
    "        folds = RepeatedStratifiedKFold(n_splits=num_folds, n_repeats=n_repeats, random_state=326)\n",
    "        no_fold = num_folds * n_repeats \n",
    "    else:\n",
    "        folds = KFold(n_splits=num_folds, shuffle=True, random_state=326)\n",
    "        no_fold = num_folds\n",
    "\n",
    "    # Create arrays and dataframes to store results\n",
    "    val_preds = np.zeros(train_df.shape[0])\n",
    "    oof_preds = np.zeros(train_df.shape[0])\n",
    "    sub_preds = np.zeros(test_df.shape[0])\n",
    "    feature_importance_df = pd.DataFrame()\n",
    "    if not feat_sorted:\n",
    "        # print(\"feat_sorted is empty\")\n",
    "        feats = [f for f in train_df.columns if f not in FEATS_EXCLUDED]\n",
    "    else:\n",
    "        if num_feat == None:\n",
    "            # print(\"feat_sorted is empty, num_feat is none\")\n",
    "            feats = [f for f in train_df.columns if f not in FEATS_EXCLUDED and f in feat_sorted]\n",
    "        else:\n",
    "            # print(\"feat_sorted is empty, num_feat is not none\")\n",
    "            feats = [f for f in train_df.columns if f not in FEATS_EXCLUDED and f in feat_sorted[:num_feat]]\n",
    "    \n",
    "    print(\"Final feat size: \" + str(len(feats)))\n",
    "    # print(feats)\n",
    "    \n",
    "    # k-fold\n",
    "    for n_fold, (train_idx, valid_idx) in enumerate(folds.split(train_df[feats], train_df['outliers'])):\n",
    "        train_x, train_y = train_df[feats].iloc[train_idx], train_df['target'].iloc[train_idx]\n",
    "        valid_x, valid_y = train_df[feats].iloc[valid_idx], train_df['target'].iloc[valid_idx]\n",
    "        \n",
    "        if model == 'lgb':\n",
    "            # set data structure\n",
    "            lgb_train = lgb.Dataset(train_x, label=train_y, free_raw_data=False)\n",
    "            lgb_test = lgb.Dataset(valid_x, label=valid_y, free_raw_data=False)\n",
    "\n",
    "            # params optimized by optuna\n",
    "            params ={\n",
    "                    'task': 'train',\n",
    "                    'boosting': 'goss',\n",
    "                    'objective': 'regression',\n",
    "                    'metric': 'rmse',\n",
    "                    'learning_rate': 0.01,\n",
    "                    'subsample': 0.9855232997390695,\n",
    "                    'max_depth': 7,\n",
    "                    'top_rate': 0.9064148448434349,\n",
    "                    'num_leaves': 63,\n",
    "                    'min_child_weight': 41.9612869171337,\n",
    "                    'other_rate': 0.0721768246018207,\n",
    "                    'reg_alpha': 9.677537745007898,\n",
    "                    'colsample_bytree': 0.5665320670155495,\n",
    "                    'min_split_gain': 9.820197773625843,\n",
    "                    'reg_lambda': 8.2532317400459,\n",
    "                    'min_data_in_leaf': 21,\n",
    "                    'verbose': -1,\n",
    "                    'seed': int(2**n_fold),\n",
    "                    'bagging_seed': int(2**n_fold),\n",
    "                    'drop_seed': int(2**n_fold)\n",
    "#                     'seed': 12, #int(2**n_fold),\n",
    "#                     'bagging_seed': 34, # int(2**n_fold),\n",
    "#                     'drop_seed': 56 #int(2**n_fold)\n",
    "                    }\n",
    "\n",
    "            reg = lgb.train(\n",
    "                            params,\n",
    "                            lgb_train,\n",
    "                            valid_sets=[lgb_train, lgb_test],\n",
    "                            valid_names=['train', 'test'],\n",
    "                            num_boost_round=10000,\n",
    "                            early_stopping_rounds= 200,\n",
    "                            verbose_eval=100\n",
    "                            )\n",
    "            best_iteration = reg.best_iteration\n",
    "            val_preds[valid_idx] = valid_y\n",
    "            oof_preds[valid_idx] = reg.predict(valid_x, num_iteration=best_iteration)\n",
    "            sub_preds += reg.predict(test_df[feats], num_iteration=best_iteration) / no_fold\n",
    "            \n",
    "            fold_importance_df = pd.DataFrame()\n",
    "            fold_importance_df[\"feature\"] = feats\n",
    "            fold_importance_df[\"importance\"] = np.log1p(reg.feature_importance(importance_type='gain', \n",
    "                                                                               iteration=best_iteration))\n",
    "            \n",
    "        elif model == 'cat':\n",
    "            cat_train = cb.Pool(train_x, train_y)\n",
    "            cat_test = cb.Pool(valid_x, valid_y)\n",
    "            \n",
    "            params = {'gpu_id': 0, \n",
    "                      #'n_gpus': 2, \n",
    "                      'objective': 'reg:linear', \n",
    "                      'eval_metric': 'rmse', \n",
    "                      'silent': True, \n",
    "                      'booster': 'gbtree', \n",
    "                      'n_jobs': -1, \n",
    "                      'n_estimators': 2500, \n",
    "                      'tree_method': 'gpu_hist', \n",
    "                      'grow_policy': 'lossguide', \n",
    "                      'max_depth': 12, \n",
    "                      'seed': 538, \n",
    "                      'colsample_bylevel': 0.9, \n",
    "                      'colsample_bytree': 0.8, \n",
    "                      'gamma': 0.0001, \n",
    "                      'learning_rate': 0.006150886706231842, \n",
    "                      'max_bin': 128, \n",
    "                      'max_leaves': 47, \n",
    "                      'min_child_weight': 40, \n",
    "                      'reg_alpha': 10.0, \n",
    "                      'reg_lambda': 10.0, \n",
    "                      'subsample': 0.9}\n",
    "            \n",
    "            \n",
    "            num_round = 10\n",
    "            reg = cb.CatBoostRegressor(max_depth=11,\n",
    "                                       learning_rate=0.005, \n",
    "                                       eval_metric='RMSE', \n",
    "                                       iterations=num_round, \n",
    "                                       early_stopping_rounds=200)\n",
    "            reg.fit(cat_train, verbose_eval = 200, eval_set = cat_test)\n",
    "            val_preds[valid_idx] = valid_y\n",
    "            oof_preds[valid_idx] = reg.predict(cat_test)\n",
    "            sub_preds += reg.predict(test_df[feats]) / no_fold\n",
    "            \n",
    "            fold_importance_df = pd.DataFrame()\n",
    "            fold_importance_df = pd.DataFrame(list(zip(train_x.dtypes.index, \n",
    "                                                       reg.get_feature_importance(cb.Pool(train_x, \n",
    "                                                                                          label=train_y)))),\n",
    "                                              columns=['feature','importance'])\n",
    "\n",
    "        fold_importance_df[\"fold\"] = n_fold + 1\n",
    "        feature_importance_df = pd.concat([feature_importance_df, fold_importance_df], axis=0)\n",
    "        print('Fold %2d RMSE : %.6f' % (n_fold + 1, rmse(valid_y, oof_preds[valid_idx])))\n",
    "        del reg, train_x, train_y, valid_x, valid_y\n",
    "        gc.collect()\n",
    "    \n",
    "    final_rmse = rmse(val_preds, oof_preds)\n",
    "    print('Final RMSE : %.6f \\n\\n\\n' % final_rmse)\n",
    "    # display importances\n",
    "    #display_importances(feature_importance_df)\n",
    "\n",
    "    if not debug and saveOpt:\n",
    "        # save submission file\n",
    "        test_df.loc[:,'target'] = sub_preds\n",
    "        test_df = test_df.reset_index()\n",
    "        if num_feat == None:\n",
    "            if stratified:\n",
    "                test_df[['card_id', 'target']].to_csv('../result/'+submission_file_name+\"_\"+ model +\"_\"+ str(num_folds) +\"_fold_stratified_all_feat_\"+str(round(final_rmse, 4))+\".csv\", index=False)\n",
    "            else:\n",
    "                test_df[['card_id', 'target']].to_csv('../result/'+submission_file_name+\"_\"+ model +\"_\"+ str(num_folds) +\"_fold_all_feat_\"+str(round(final_rmse, 4))+\".csv\", index=False)\n",
    "        else:\n",
    "            if stratified:\n",
    "                test_df[['card_id', 'target']].to_csv('../result/'+submission_file_name+\"_\"+ model +\"_\"+ str(num_folds) +\"_fold_stratified_\"+str(num_feat)+\"_feat_\"+str(round(final_rmse, 4))+\".csv\", index=False)\n",
    "            else:\n",
    "                test_df[['card_id', 'target']].to_csv('../result/'+submission_file_name+\"_\"+ model +\"_\"+ str(num_folds) +\"_fold_\"+str(num_feat)+\"_feat_\"+str(round(final_rmse, 4))+\".csv\", index=False)\n",
    "    \n",
    "    feat_sel_summary = feature_importance_df[[\"feature\", \"importance\"]].groupby(\"feature\").mean().sort_values(by=\"importance\", ascending=False).reset_index()\n",
    "    feat_sorted = feat_sel_summary[\"feature\"].tolist()\n",
    "    \n",
    "    return feature_importance_df, sub_preds, feat_sorted, oof_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine_solution(test_df, sub_preds_list, submission_file_name):\n",
    "    sub_preds = [0.0 for _ in range(len(sub_preds_list[0]))]\n",
    "    for sol in sub_preds_list:\n",
    "        sub_preds += sol/len(sub_preds_list)\n",
    "    test_df.loc[:,'target'] = sub_preds\n",
    "    test_df = test_df.reset_index()\n",
    "    test_df[['card_id', 'target']].to_csv('../result/'+submission_file_name+'_final.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stacking(oof, predictions):\n",
    "    train_stack = np.vstack(oof).transpose()\n",
    "    test_stack = np.vstack(predictions).transpose()\n",
    "\n",
    "    folds = StratifiedKFold(n_splits=5, shuffle=True, random_state=15)\n",
    "    oof = np.zeros(train_stack.shape[0])\n",
    "    predictions = np.zeros(test_stack.shape[0])\n",
    "\n",
    "    for fold_, (trn_idx, val_idx) in enumerate(folds.split(train_stack, train['outliers'].values)):\n",
    "        print(\"fold nÂ°{}\".format(fold_))\n",
    "        trn_data, trn_y = train_stack[trn_idx], target.iloc[trn_idx].values\n",
    "        val_data, val_y = train_stack[val_idx], target.iloc[val_idx].values\n",
    "\n",
    "        clf = Ridge(alpha=1)\n",
    "        clf.fit(trn_data, trn_y)\n",
    "\n",
    "        oof[val_idx] = clf.predict(val_data)\n",
    "        predictions += clf.predict(test_stack) / folds.n_splits\n",
    "\n",
    "\n",
    "    print(np.sqrt(mean_squared_error(target.values, oof)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Default case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_df_cleaned size: (201917, 938)\n",
      "test_df_cleaned size: (123623, 938)\n",
      "Starting LightGBM. Train shape: (201917, 938), test shape: (123623, 938)\n",
      "Final feat size: 100\n",
      "Training until validation scores don't improve for 200 rounds.\n",
      "[100]\ttrain's rmse: 3.65076\ttest's rmse: 3.75282\n",
      "[200]\ttrain's rmse: 3.57083\ttest's rmse: 3.71365\n",
      "[300]\ttrain's rmse: 3.51802\ttest's rmse: 3.70019\n",
      "[400]\ttrain's rmse: 3.48145\ttest's rmse: 3.69447\n",
      "[500]\ttrain's rmse: 3.45408\ttest's rmse: 3.69174\n",
      "[600]\ttrain's rmse: 3.43032\ttest's rmse: 3.69008\n",
      "[700]\ttrain's rmse: 3.40871\ttest's rmse: 3.68953\n",
      "[800]\ttrain's rmse: 3.38885\ttest's rmse: 3.6896\n",
      "[900]\ttrain's rmse: 3.37142\ttest's rmse: 3.6899\n",
      "[1000]\ttrain's rmse: 3.35409\ttest's rmse: 3.69046\n",
      "Early stopping, best iteration is:\n",
      "[820]\ttrain's rmse: 3.38513\ttest's rmse: 3.68912\n",
      "Fold  1 RMSE : 3.689118\n",
      "Training until validation scores don't improve for 200 rounds.\n",
      "[100]\ttrain's rmse: 3.63903\ttest's rmse: 3.83059\n",
      "[200]\ttrain's rmse: 3.55972\ttest's rmse: 3.79922\n",
      "[300]\ttrain's rmse: 3.50656\ttest's rmse: 3.78909\n",
      "[400]\ttrain's rmse: 3.47041\ttest's rmse: 3.78365\n",
      "[500]\ttrain's rmse: 3.44286\ttest's rmse: 3.78004\n",
      "[600]\ttrain's rmse: 3.41799\ttest's rmse: 3.77849\n",
      "[700]\ttrain's rmse: 3.39607\ttest's rmse: 3.77793\n",
      "[800]\ttrain's rmse: 3.37691\ttest's rmse: 3.77765\n",
      "[900]\ttrain's rmse: 3.35964\ttest's rmse: 3.77793\n",
      "[1000]\ttrain's rmse: 3.34393\ttest's rmse: 3.77895\n",
      "Early stopping, best iteration is:\n",
      "[850]\ttrain's rmse: 3.3685\ttest's rmse: 3.77729\n",
      "Fold  2 RMSE : 3.777288\n",
      "Training until validation scores don't improve for 200 rounds.\n",
      "[100]\ttrain's rmse: 3.64387\ttest's rmse: 3.7907\n",
      "[200]\ttrain's rmse: 3.56349\ttest's rmse: 3.76101\n",
      "[300]\ttrain's rmse: 3.51154\ttest's rmse: 3.75261\n",
      "[400]\ttrain's rmse: 3.47547\ttest's rmse: 3.74854\n",
      "[500]\ttrain's rmse: 3.44685\ttest's rmse: 3.74708\n",
      "[600]\ttrain's rmse: 3.42199\ttest's rmse: 3.74696\n",
      "[700]\ttrain's rmse: 3.39959\ttest's rmse: 3.74652\n",
      "[800]\ttrain's rmse: 3.37945\ttest's rmse: 3.74598\n",
      "[900]\ttrain's rmse: 3.36127\ttest's rmse: 3.74617\n",
      "[1000]\ttrain's rmse: 3.34457\ttest's rmse: 3.74647\n",
      "Early stopping, best iteration is:\n",
      "[874]\ttrain's rmse: 3.36595\ttest's rmse: 3.74592\n",
      "Fold  3 RMSE : 3.745919\n",
      "Training until validation scores don't improve for 200 rounds.\n",
      "[100]\ttrain's rmse: 3.65662\ttest's rmse: 3.69458\n",
      "[200]\ttrain's rmse: 3.57888\ttest's rmse: 3.65605\n",
      "[300]\ttrain's rmse: 3.52763\ttest's rmse: 3.64182\n",
      "[400]\ttrain's rmse: 3.49261\ttest's rmse: 3.63579\n",
      "[500]\ttrain's rmse: 3.46453\ttest's rmse: 3.63308\n",
      "[600]\ttrain's rmse: 3.44052\ttest's rmse: 3.63104\n",
      "[700]\ttrain's rmse: 3.41931\ttest's rmse: 3.62914\n",
      "[800]\ttrain's rmse: 3.39945\ttest's rmse: 3.62886\n",
      "[900]\ttrain's rmse: 3.38079\ttest's rmse: 3.62858\n",
      "[1000]\ttrain's rmse: 3.36338\ttest's rmse: 3.62839\n",
      "[1100]\ttrain's rmse: 3.3473\ttest's rmse: 3.6291\n",
      "Early stopping, best iteration is:\n",
      "[947]\ttrain's rmse: 3.37277\ttest's rmse: 3.62805\n",
      "Fold  4 RMSE : 3.628054\n",
      "Training until validation scores don't improve for 200 rounds.\n",
      "[100]\ttrain's rmse: 3.6778\ttest's rmse: 3.50563\n",
      "[200]\ttrain's rmse: 3.90877\ttest's rmse: 3.79309\n",
      "[300]\ttrain's rmse: 3.62049\ttest's rmse: 3.50879\n",
      "[400]\ttrain's rmse: 3.55244\ttest's rmse: 3.45731\n",
      "[500]\ttrain's rmse: 3.51696\ttest's rmse: 3.44455\n",
      "[600]\ttrain's rmse: 3.48842\ttest's rmse: 3.43747\n",
      "[700]\ttrain's rmse: 3.46116\ttest's rmse: 3.43388\n",
      "[800]\ttrain's rmse: 3.43854\ttest's rmse: 3.43141\n",
      "[900]\ttrain's rmse: 3.41882\ttest's rmse: 3.43046\n",
      "[1000]\ttrain's rmse: 3.40004\ttest's rmse: 3.43039\n",
      "[1100]\ttrain's rmse: 3.38204\ttest's rmse: 3.43031\n",
      "[1200]\ttrain's rmse: 3.36652\ttest's rmse: 3.4302\n",
      "Early stopping, best iteration is:\n",
      "[1081]\ttrain's rmse: 3.38566\ttest's rmse: 3.42995\n",
      "Fold  5 RMSE : 3.429953\n",
      "Training until validation scores don't improve for 200 rounds.\n",
      "[100]\ttrain's rmse: 3.65286\ttest's rmse: 3.72617\n",
      "[200]\ttrain's rmse: 3.57362\ttest's rmse: 3.68483\n",
      "[300]\ttrain's rmse: 3.52353\ttest's rmse: 3.66854\n",
      "[400]\ttrain's rmse: 3.48703\ttest's rmse: 3.66147\n",
      "[500]\ttrain's rmse: 3.45858\ttest's rmse: 3.65929\n",
      "[600]\ttrain's rmse: 3.43492\ttest's rmse: 3.65792\n",
      "[700]\ttrain's rmse: 3.41316\ttest's rmse: 3.65665\n",
      "[800]\ttrain's rmse: 3.39226\ttest's rmse: 3.65637\n",
      "[900]\ttrain's rmse: 3.37409\ttest's rmse: 3.65574\n",
      "[1000]\ttrain's rmse: 3.35871\ttest's rmse: 3.65588\n",
      "[1100]\ttrain's rmse: 3.34341\ttest's rmse: 3.6559\n",
      "Early stopping, best iteration is:\n",
      "[914]\ttrain's rmse: 3.37168\ttest's rmse: 3.65555\n",
      "Fold  6 RMSE : 3.655550\n",
      "Training until validation scores don't improve for 200 rounds.\n",
      "[100]\ttrain's rmse: 3.65945\ttest's rmse: 3.65047\n",
      "[200]\ttrain's rmse: 3.58121\ttest's rmse: 3.61925\n",
      "[300]\ttrain's rmse: 3.53432\ttest's rmse: 3.61174\n",
      "[400]\ttrain's rmse: 3.49954\ttest's rmse: 3.60408\n",
      "[500]\ttrain's rmse: 3.47254\ttest's rmse: 3.5995\n",
      "[600]\ttrain's rmse: 3.44797\ttest's rmse: 3.59647\n",
      "[700]\ttrain's rmse: 3.42589\ttest's rmse: 3.59434\n",
      "[800]\ttrain's rmse: 3.40689\ttest's rmse: 3.59287\n",
      "[900]\ttrain's rmse: 3.3888\ttest's rmse: 3.59169\n",
      "[1000]\ttrain's rmse: 3.37182\ttest's rmse: 3.591\n",
      "[1100]\ttrain's rmse: 3.35372\ttest's rmse: 3.59083\n",
      "[1200]\ttrain's rmse: 3.33759\ttest's rmse: 3.59025\n",
      "[1300]\ttrain's rmse: 3.32128\ttest's rmse: 3.58936\n",
      "[1400]\ttrain's rmse: 3.30637\ttest's rmse: 3.58933\n",
      "[1500]\ttrain's rmse: 3.29062\ttest's rmse: 3.58973\n",
      "Early stopping, best iteration is:\n",
      "[1338]\ttrain's rmse: 3.31552\ttest's rmse: 3.58917\n",
      "Fold  7 RMSE : 3.589172\n",
      "Training until validation scores don't improve for 200 rounds.\n",
      "[100]\ttrain's rmse: 3.63622\ttest's rmse: 3.866\n",
      "[200]\ttrain's rmse: 3.55663\ttest's rmse: 3.82873\n",
      "[300]\ttrain's rmse: 3.5054\ttest's rmse: 3.81622\n",
      "[400]\ttrain's rmse: 3.47041\ttest's rmse: 3.81113\n",
      "[500]\ttrain's rmse: 3.44203\ttest's rmse: 3.80949\n",
      "[600]\ttrain's rmse: 3.41826\ttest's rmse: 3.80812\n",
      "[700]\ttrain's rmse: 3.3967\ttest's rmse: 3.80707\n",
      "[800]\ttrain's rmse: 3.37637\ttest's rmse: 3.80573\n",
      "[900]\ttrain's rmse: 3.35825\ttest's rmse: 3.80562\n",
      "[1000]\ttrain's rmse: 3.34029\ttest's rmse: 3.80529\n",
      "[1100]\ttrain's rmse: 3.3245\ttest's rmse: 3.80562\n",
      "[1200]\ttrain's rmse: 3.30917\ttest's rmse: 3.80617\n",
      "Early stopping, best iteration is:\n",
      "[1032]\ttrain's rmse: 3.33532\ttest's rmse: 3.80498\n",
      "Fold  8 RMSE : 3.804985\n",
      "Training until validation scores don't improve for 200 rounds.\n",
      "[100]\ttrain's rmse: 3.67286\ttest's rmse: 3.52711\n",
      "[200]\ttrain's rmse: 3.59846\ttest's rmse: 3.50424\n",
      "[300]\ttrain's rmse: 3.54744\ttest's rmse: 3.49171\n",
      "[400]\ttrain's rmse: 3.51208\ttest's rmse: 3.48652\n",
      "[500]\ttrain's rmse: 3.48347\ttest's rmse: 3.48382\n",
      "[600]\ttrain's rmse: 3.45906\ttest's rmse: 3.48228\n",
      "[700]\ttrain's rmse: 3.43696\ttest's rmse: 3.48118\n",
      "[800]\ttrain's rmse: 3.41745\ttest's rmse: 3.48076\n",
      "[900]\ttrain's rmse: 3.39996\ttest's rmse: 3.48068\n",
      "[1000]\ttrain's rmse: 3.3819\ttest's rmse: 3.4811\n",
      "Early stopping, best iteration is:\n",
      "[827]\ttrain's rmse: 3.4124\ttest's rmse: 3.48029\n",
      "Fold  9 RMSE : 3.480287\n",
      "Training until validation scores don't improve for 200 rounds.\n",
      "[100]\ttrain's rmse: 3.65794\ttest's rmse: 3.67797\n",
      "[200]\ttrain's rmse: 3.57927\ttest's rmse: 3.64029\n",
      "[300]\ttrain's rmse: 3.5281\ttest's rmse: 3.62407\n",
      "[400]\ttrain's rmse: 3.49352\ttest's rmse: 3.61831\n",
      "[500]\ttrain's rmse: 3.46592\ttest's rmse: 3.61407\n",
      "[600]\ttrain's rmse: 3.44218\ttest's rmse: 3.61087\n",
      "[700]\ttrain's rmse: 3.42087\ttest's rmse: 3.60878\n",
      "[800]\ttrain's rmse: 3.40133\ttest's rmse: 3.60836\n",
      "[900]\ttrain's rmse: 3.38276\ttest's rmse: 3.60734\n",
      "[1000]\ttrain's rmse: 3.36633\ttest's rmse: 3.60733\n",
      "[1100]\ttrain's rmse: 3.35157\ttest's rmse: 3.60686\n",
      "[1200]\ttrain's rmse: 3.33545\ttest's rmse: 3.60719\n",
      "[1300]\ttrain's rmse: 3.31872\ttest's rmse: 3.60726\n",
      "Early stopping, best iteration is:\n",
      "[1119]\ttrain's rmse: 3.34882\ttest's rmse: 3.60664\n",
      "Fold 10 RMSE : 3.606636\n",
      "Final RMSE : 3.642528 \n",
      "\n",
      "\n",
      "\n",
      "Starting LightGBM. Train shape: (201917, 938), test shape: (123623, 938)\n",
      "Final feat size: 200\n",
      "Training until validation scores don't improve for 200 rounds.\n",
      "[100]\ttrain's rmse: 3.65039\ttest's rmse: 3.70909\n",
      "[200]\ttrain's rmse: 3.56507\ttest's rmse: 3.67574\n",
      "[300]\ttrain's rmse: 3.50796\ttest's rmse: 3.66272\n",
      "[400]\ttrain's rmse: 3.46795\ttest's rmse: 3.65874\n",
      "[500]\ttrain's rmse: 3.43743\ttest's rmse: 3.65702\n",
      "[600]\ttrain's rmse: 3.41184\ttest's rmse: 3.65456\n",
      "[700]\ttrain's rmse: 3.38886\ttest's rmse: 3.65416\n",
      "[800]\ttrain's rmse: 3.36919\ttest's rmse: 3.65338\n",
      "[900]\ttrain's rmse: 3.35125\ttest's rmse: 3.65313\n",
      "[1000]\ttrain's rmse: 3.33337\ttest's rmse: 3.65323\n",
      "[1100]\ttrain's rmse: 3.31649\ttest's rmse: 3.65295\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1200]\ttrain's rmse: 3.30053\ttest's rmse: 3.65318\n",
      "[1300]\ttrain's rmse: 3.28434\ttest's rmse: 3.65356\n",
      "Early stopping, best iteration is:\n",
      "[1121]\ttrain's rmse: 3.3131\ttest's rmse: 3.65286\n",
      "Fold  1 RMSE : 3.652864\n",
      "Training until validation scores don't improve for 200 rounds.\n",
      "[100]\ttrain's rmse: 3.65354\ttest's rmse: 3.68102\n",
      "[200]\ttrain's rmse: 3.56927\ttest's rmse: 3.64002\n",
      "[300]\ttrain's rmse: 3.51377\ttest's rmse: 3.62547\n",
      "[400]\ttrain's rmse: 3.47458\ttest's rmse: 3.61909\n",
      "[500]\ttrain's rmse: 3.44407\ttest's rmse: 3.6161\n",
      "[600]\ttrain's rmse: 3.41875\ttest's rmse: 3.61383\n",
      "[700]\ttrain's rmse: 3.39547\ttest's rmse: 3.6124\n",
      "[800]\ttrain's rmse: 3.37447\ttest's rmse: 3.61076\n",
      "[900]\ttrain's rmse: 3.35572\ttest's rmse: 3.61014\n",
      "[1000]\ttrain's rmse: 3.33768\ttest's rmse: 3.60959\n",
      "[1100]\ttrain's rmse: 3.32083\ttest's rmse: 3.61019\n",
      "[1200]\ttrain's rmse: 3.30527\ttest's rmse: 3.61055\n",
      "Early stopping, best iteration is:\n",
      "[1000]\ttrain's rmse: 3.33768\ttest's rmse: 3.60959\n",
      "Fold  2 RMSE : 3.609591\n",
      "Training until validation scores don't improve for 200 rounds.\n",
      "[100]\ttrain's rmse: 3.64755\ttest's rmse: 3.72743\n",
      "[200]\ttrain's rmse: 3.56327\ttest's rmse: 3.69871\n",
      "[300]\ttrain's rmse: 3.50896\ttest's rmse: 3.68581\n",
      "[400]\ttrain's rmse: 3.46803\ttest's rmse: 3.67929\n",
      "[500]\ttrain's rmse: 3.43845\ttest's rmse: 3.67646\n",
      "[600]\ttrain's rmse: 3.41358\ttest's rmse: 3.67479\n",
      "[700]\ttrain's rmse: 3.3917\ttest's rmse: 3.6733\n",
      "[800]\ttrain's rmse: 3.37268\ttest's rmse: 3.67257\n",
      "[900]\ttrain's rmse: 3.35436\ttest's rmse: 3.67262\n",
      "[1000]\ttrain's rmse: 3.33722\ttest's rmse: 3.67224\n",
      "[1100]\ttrain's rmse: 3.32164\ttest's rmse: 3.67192\n",
      "[1200]\ttrain's rmse: 3.30496\ttest's rmse: 3.67217\n",
      "Early stopping, best iteration is:\n",
      "[1094]\ttrain's rmse: 3.32237\ttest's rmse: 3.67179\n",
      "Fold  3 RMSE : 3.671785\n",
      "Training until validation scores don't improve for 200 rounds.\n",
      "[100]\ttrain's rmse: 3.65206\ttest's rmse: 3.69991\n",
      "[200]\ttrain's rmse: 3.56652\ttest's rmse: 3.66263\n",
      "[300]\ttrain's rmse: 3.50945\ttest's rmse: 3.64966\n",
      "[400]\ttrain's rmse: 3.46961\ttest's rmse: 3.64442\n",
      "[500]\ttrain's rmse: 3.43896\ttest's rmse: 3.64115\n",
      "[600]\ttrain's rmse: 3.4143\ttest's rmse: 3.63855\n",
      "[700]\ttrain's rmse: 3.39174\ttest's rmse: 3.63824\n",
      "[800]\ttrain's rmse: 3.37085\ttest's rmse: 3.63843\n",
      "[900]\ttrain's rmse: 3.35086\ttest's rmse: 3.63804\n",
      "[1000]\ttrain's rmse: 3.33216\ttest's rmse: 3.63843\n",
      "Early stopping, best iteration is:\n",
      "[898]\ttrain's rmse: 3.3513\ttest's rmse: 3.63803\n",
      "Fold  4 RMSE : 3.638030\n",
      "Training until validation scores don't improve for 200 rounds.\n",
      "[100]\ttrain's rmse: 3.65215\ttest's rmse: 3.70821\n",
      "[200]\ttrain's rmse: 3.56747\ttest's rmse: 3.67352\n",
      "[300]\ttrain's rmse: 3.51284\ttest's rmse: 3.66058\n",
      "[400]\ttrain's rmse: 3.47244\ttest's rmse: 3.65416\n",
      "[500]\ttrain's rmse: 3.44176\ttest's rmse: 3.65041\n",
      "[600]\ttrain's rmse: 3.41678\ttest's rmse: 3.64712\n",
      "[700]\ttrain's rmse: 3.39429\ttest's rmse: 3.64506\n",
      "[800]\ttrain's rmse: 3.37359\ttest's rmse: 3.64383\n",
      "[900]\ttrain's rmse: 3.35454\ttest's rmse: 3.64394\n",
      "[1000]\ttrain's rmse: 3.33685\ttest's rmse: 3.6432\n",
      "[1100]\ttrain's rmse: 3.31907\ttest's rmse: 3.64343\n",
      "[1200]\ttrain's rmse: 3.30306\ttest's rmse: 3.64337\n",
      "Early stopping, best iteration is:\n",
      "[1005]\ttrain's rmse: 3.33574\ttest's rmse: 3.64312\n",
      "Fold  5 RMSE : 3.643117\n",
      "Training until validation scores don't improve for 200 rounds.\n",
      "[100]\ttrain's rmse: 3.65061\ttest's rmse: 3.70178\n",
      "[200]\ttrain's rmse: 3.56687\ttest's rmse: 3.67041\n",
      "[300]\ttrain's rmse: 3.50938\ttest's rmse: 3.66015\n",
      "[400]\ttrain's rmse: 3.46869\ttest's rmse: 3.65472\n",
      "[500]\ttrain's rmse: 3.43812\ttest's rmse: 3.65144\n",
      "[600]\ttrain's rmse: 3.4133\ttest's rmse: 3.65051\n",
      "[700]\ttrain's rmse: 3.39137\ttest's rmse: 3.64948\n",
      "[800]\ttrain's rmse: 3.36905\ttest's rmse: 3.65005\n",
      "Early stopping, best iteration is:\n",
      "[699]\ttrain's rmse: 3.39153\ttest's rmse: 3.64946\n",
      "Fold  6 RMSE : 3.649459\n",
      "Training until validation scores don't improve for 200 rounds.\n",
      "[100]\ttrain's rmse: 3.65314\ttest's rmse: 3.70891\n",
      "[200]\ttrain's rmse: 3.56879\ttest's rmse: 3.67111\n",
      "[300]\ttrain's rmse: 3.51239\ttest's rmse: 3.65601\n",
      "[400]\ttrain's rmse: 3.47133\ttest's rmse: 3.64853\n",
      "[500]\ttrain's rmse: 3.43978\ttest's rmse: 3.6434\n",
      "[600]\ttrain's rmse: 3.41524\ttest's rmse: 3.64057\n",
      "[700]\ttrain's rmse: 3.39368\ttest's rmse: 3.63799\n",
      "[800]\ttrain's rmse: 3.37267\ttest's rmse: 3.63678\n",
      "[900]\ttrain's rmse: 3.35312\ttest's rmse: 3.63602\n",
      "[1000]\ttrain's rmse: 3.33433\ttest's rmse: 3.63588\n",
      "[1100]\ttrain's rmse: 3.31635\ttest's rmse: 3.63607\n",
      "Early stopping, best iteration is:\n",
      "[952]\ttrain's rmse: 3.34296\ttest's rmse: 3.63544\n",
      "Fold  7 RMSE : 3.635445\n",
      "Training until validation scores don't improve for 200 rounds.\n",
      "[100]\ttrain's rmse: 3.65029\ttest's rmse: 3.70096\n",
      "[200]\ttrain's rmse: 3.565\ttest's rmse: 3.66591\n",
      "[300]\ttrain's rmse: 3.50871\ttest's rmse: 3.65311\n",
      "[400]\ttrain's rmse: 3.46688\ttest's rmse: 3.64825\n",
      "[500]\ttrain's rmse: 3.43572\ttest's rmse: 3.64587\n",
      "[600]\ttrain's rmse: 3.40977\ttest's rmse: 3.6442\n",
      "[700]\ttrain's rmse: 3.38763\ttest's rmse: 3.64402\n",
      "[800]\ttrain's rmse: 3.36611\ttest's rmse: 3.64358\n",
      "[900]\ttrain's rmse: 3.34722\ttest's rmse: 3.6435\n",
      "[1000]\ttrain's rmse: 3.32846\ttest's rmse: 3.64403\n",
      "Early stopping, best iteration is:\n",
      "[834]\ttrain's rmse: 3.35919\ttest's rmse: 3.6433\n",
      "Fold  8 RMSE : 3.643304\n",
      "Training until validation scores don't improve for 200 rounds.\n",
      "[100]\ttrain's rmse: 3.64873\ttest's rmse: 3.72496\n",
      "[200]\ttrain's rmse: 3.56296\ttest's rmse: 3.69838\n",
      "[300]\ttrain's rmse: 3.50638\ttest's rmse: 3.68979\n",
      "[400]\ttrain's rmse: 3.46628\ttest's rmse: 3.68573\n",
      "[500]\ttrain's rmse: 3.43434\ttest's rmse: 3.68339\n",
      "[600]\ttrain's rmse: 3.40838\ttest's rmse: 3.68187\n",
      "[700]\ttrain's rmse: 3.38478\ttest's rmse: 3.68117\n",
      "[800]\ttrain's rmse: 3.3627\ttest's rmse: 3.68111\n",
      "[900]\ttrain's rmse: 3.3428\ttest's rmse: 3.68154\n",
      "[1000]\ttrain's rmse: 3.32486\ttest's rmse: 3.68174\n",
      "Early stopping, best iteration is:\n",
      "[805]\ttrain's rmse: 3.36149\ttest's rmse: 3.68097\n",
      "Fold  9 RMSE : 3.680966\n",
      "Training until validation scores don't improve for 200 rounds.\n",
      "[100]\ttrain's rmse: 3.65264\ttest's rmse: 3.69456\n",
      "[200]\ttrain's rmse: 3.56839\ttest's rmse: 3.65996\n",
      "[300]\ttrain's rmse: 3.51293\ttest's rmse: 3.64732\n",
      "[400]\ttrain's rmse: 3.47131\ttest's rmse: 3.64098\n",
      "[500]\ttrain's rmse: 3.44276\ttest's rmse: 3.63839\n",
      "[600]\ttrain's rmse: 3.41701\ttest's rmse: 3.6362\n",
      "[700]\ttrain's rmse: 3.39284\ttest's rmse: 3.63448\n",
      "[800]\ttrain's rmse: 3.37286\ttest's rmse: 3.63472\n",
      "[900]\ttrain's rmse: 3.35383\ttest's rmse: 3.63364\n",
      "[1000]\ttrain's rmse: 3.33505\ttest's rmse: 3.63283\n",
      "[1100]\ttrain's rmse: 3.31813\ttest's rmse: 3.63227\n",
      "[1200]\ttrain's rmse: 3.30224\ttest's rmse: 3.63228\n",
      "[1300]\ttrain's rmse: 3.28622\ttest's rmse: 3.63224\n",
      "[1400]\ttrain's rmse: 3.27005\ttest's rmse: 3.63195\n",
      "[1500]\ttrain's rmse: 3.25381\ttest's rmse: 3.63211\n",
      "[1600]\ttrain's rmse: 3.23775\ttest's rmse: 3.63263\n",
      "Early stopping, best iteration is:\n",
      "[1449]\ttrain's rmse: 3.2625\ttest's rmse: 3.63173\n",
      "Fold 10 RMSE : 3.631728\n",
      "Training until validation scores don't improve for 200 rounds.\n",
      "[100]\ttrain's rmse: 3.65061\ttest's rmse: 3.70642\n",
      "[200]\ttrain's rmse: 3.56598\ttest's rmse: 3.67525\n",
      "[300]\ttrain's rmse: 3.50985\ttest's rmse: 3.66463\n",
      "[400]\ttrain's rmse: 3.46929\ttest's rmse: 3.65909\n",
      "[500]\ttrain's rmse: 3.43855\ttest's rmse: 3.65558\n",
      "[600]\ttrain's rmse: 3.41389\ttest's rmse: 3.65381\n",
      "[700]\ttrain's rmse: 3.39016\ttest's rmse: 3.65195\n",
      "[800]\ttrain's rmse: 3.36954\ttest's rmse: 3.65131\n",
      "[900]\ttrain's rmse: 3.34921\ttest's rmse: 3.65051\n",
      "[1000]\ttrain's rmse: 3.33082\ttest's rmse: 3.65066\n",
      "[1100]\ttrain's rmse: 3.31297\ttest's rmse: 3.65092\n",
      "[1200]\ttrain's rmse: 3.29659\ttest's rmse: 3.65076\n",
      "[1300]\ttrain's rmse: 3.279\ttest's rmse: 3.65121\n",
      "Early stopping, best iteration is:\n",
      "[1134]\ttrain's rmse: 3.30719\ttest's rmse: 3.65041\n",
      "Fold 11 RMSE : 3.650406\n",
      "Training until validation scores don't improve for 200 rounds.\n",
      "[100]\ttrain's rmse: 3.65321\ttest's rmse: 3.69441\n",
      "[200]\ttrain's rmse: 3.57022\ttest's rmse: 3.65239\n",
      "[300]\ttrain's rmse: 3.51425\ttest's rmse: 3.63488\n",
      "[400]\ttrain's rmse: 3.47432\ttest's rmse: 3.62657\n",
      "[500]\ttrain's rmse: 3.44347\ttest's rmse: 3.62296\n",
      "[600]\ttrain's rmse: 3.41873\ttest's rmse: 3.62025\n",
      "[700]\ttrain's rmse: 3.39707\ttest's rmse: 3.61928\n",
      "[800]\ttrain's rmse: 3.37729\ttest's rmse: 3.61797\n",
      "[900]\ttrain's rmse: 3.35962\ttest's rmse: 3.61773\n",
      "[1000]\ttrain's rmse: 3.34016\ttest's rmse: 3.61773\n",
      "[1100]\ttrain's rmse: 3.32282\ttest's rmse: 3.61786\n",
      "Early stopping, best iteration is:\n",
      "[924]\ttrain's rmse: 3.35464\ttest's rmse: 3.61751\n",
      "Fold 12 RMSE : 3.617511\n",
      "Training until validation scores don't improve for 200 rounds.\n",
      "[100]\ttrain's rmse: 3.64861\ttest's rmse: 3.71526\n",
      "[200]\ttrain's rmse: 3.56261\ttest's rmse: 3.68542\n",
      "[300]\ttrain's rmse: 3.50597\ttest's rmse: 3.67584\n",
      "[400]\ttrain's rmse: 3.46729\ttest's rmse: 3.67179\n",
      "[500]\ttrain's rmse: 3.43687\ttest's rmse: 3.66972\n",
      "[600]\ttrain's rmse: 3.41157\ttest's rmse: 3.66742\n",
      "[700]\ttrain's rmse: 3.38853\ttest's rmse: 3.66645\n",
      "[800]\ttrain's rmse: 3.36655\ttest's rmse: 3.66535\n",
      "[900]\ttrain's rmse: 3.34681\ttest's rmse: 3.66474\n",
      "[1000]\ttrain's rmse: 3.3287\ttest's rmse: 3.66498\n",
      "Early stopping, best iteration is:\n",
      "[891]\ttrain's rmse: 3.34895\ttest's rmse: 3.66466\n",
      "Fold 13 RMSE : 3.664660\n",
      "Training until validation scores don't improve for 200 rounds.\n",
      "[100]\ttrain's rmse: 3.65502\ttest's rmse: 3.69147\n",
      "[200]\ttrain's rmse: 3.57075\ttest's rmse: 3.64579\n",
      "[300]\ttrain's rmse: 3.51583\ttest's rmse: 3.62745\n",
      "[400]\ttrain's rmse: 3.47512\ttest's rmse: 3.61867\n",
      "[500]\ttrain's rmse: 3.44514\ttest's rmse: 3.61432\n",
      "[600]\ttrain's rmse: 3.42069\ttest's rmse: 3.61196\n",
      "[700]\ttrain's rmse: 3.39867\ttest's rmse: 3.60982\n",
      "[800]\ttrain's rmse: 3.37792\ttest's rmse: 3.60863\n",
      "[900]\ttrain's rmse: 3.35882\ttest's rmse: 3.60768\n",
      "[1000]\ttrain's rmse: 3.34169\ttest's rmse: 3.60664\n",
      "[1100]\ttrain's rmse: 3.32578\ttest's rmse: 3.60924\n",
      "[1200]\ttrain's rmse: 3.30065\ttest's rmse: 3.60738\n",
      "Early stopping, best iteration is:\n",
      "[1087]\ttrain's rmse: 3.32578\ttest's rmse: 3.6058\n",
      "Fold 14 RMSE : 3.605801\n",
      "Training until validation scores don't improve for 200 rounds.\n",
      "[100]\ttrain's rmse: 3.65624\ttest's rmse: 3.68155\n",
      "[200]\ttrain's rmse: 3.57251\ttest's rmse: 3.63567\n",
      "[300]\ttrain's rmse: 3.51782\ttest's rmse: 3.61767\n",
      "[400]\ttrain's rmse: 3.47901\ttest's rmse: 3.60859\n",
      "[500]\ttrain's rmse: 3.44883\ttest's rmse: 3.60425\n",
      "[600]\ttrain's rmse: 3.42205\ttest's rmse: 3.60171\n",
      "[700]\ttrain's rmse: 3.39972\ttest's rmse: 3.60068\n",
      "[800]\ttrain's rmse: 3.3781\ttest's rmse: 3.59993\n",
      "[900]\ttrain's rmse: 3.35898\ttest's rmse: 3.59907\n",
      "[1000]\ttrain's rmse: 3.34056\ttest's rmse: 3.59931\n",
      "[1100]\ttrain's rmse: 3.3232\ttest's rmse: 3.59986\n",
      "Early stopping, best iteration is:\n",
      "[924]\ttrain's rmse: 3.35449\ttest's rmse: 3.59894\n",
      "Fold 15 RMSE : 3.598937\n",
      "Training until validation scores don't improve for 200 rounds.\n",
      "[100]\ttrain's rmse: 3.64774\ttest's rmse: 3.73152\n",
      "[200]\ttrain's rmse: 3.56251\ttest's rmse: 3.70155\n",
      "[300]\ttrain's rmse: 3.50697\ttest's rmse: 3.68983\n",
      "[400]\ttrain's rmse: 3.46777\ttest's rmse: 3.68566\n",
      "[500]\ttrain's rmse: 3.43863\ttest's rmse: 3.6824\n",
      "[600]\ttrain's rmse: 3.41025\ttest's rmse: 3.68126\n",
      "[700]\ttrain's rmse: 3.38681\ttest's rmse: 3.68151\n",
      "[800]\ttrain's rmse: 3.36572\ttest's rmse: 3.68183\n",
      "Early stopping, best iteration is:\n",
      "[619]\ttrain's rmse: 3.40589\ttest's rmse: 3.68118\n",
      "Fold 16 RMSE : 3.681178\n",
      "Training until validation scores don't improve for 200 rounds.\n",
      "[100]\ttrain's rmse: 3.65164\ttest's rmse: 3.7044\n",
      "[200]\ttrain's rmse: 3.56565\ttest's rmse: 3.66962\n",
      "[300]\ttrain's rmse: 3.51106\ttest's rmse: 3.65794\n",
      "[400]\ttrain's rmse: 3.47118\ttest's rmse: 3.65289\n",
      "[500]\ttrain's rmse: 3.4407\ttest's rmse: 3.65049\n",
      "[600]\ttrain's rmse: 3.41335\ttest's rmse: 3.64895\n",
      "[700]\ttrain's rmse: 3.3895\ttest's rmse: 3.64894\n",
      "[800]\ttrain's rmse: 3.36944\ttest's rmse: 3.64853\n",
      "[900]\ttrain's rmse: 3.35114\ttest's rmse: 3.64862\n",
      "[1000]\ttrain's rmse: 3.33155\ttest's rmse: 3.6489\n",
      "Early stopping, best iteration is:\n",
      "[805]\ttrain's rmse: 3.36857\ttest's rmse: 3.64841\n",
      "Fold 17 RMSE : 3.648406\n",
      "Training until validation scores don't improve for 200 rounds.\n",
      "[100]\ttrain's rmse: 3.65117\ttest's rmse: 3.70211\n",
      "[200]\ttrain's rmse: 3.56565\ttest's rmse: 3.67235\n",
      "[300]\ttrain's rmse: 3.51057\ttest's rmse: 3.66081\n",
      "[400]\ttrain's rmse: 3.46981\ttest's rmse: 3.65719\n",
      "[500]\ttrain's rmse: 3.43895\ttest's rmse: 3.65513\n",
      "[600]\ttrain's rmse: 3.41389\ttest's rmse: 3.65344\n",
      "[700]\ttrain's rmse: 3.39185\ttest's rmse: 3.65262\n",
      "[800]\ttrain's rmse: 3.37206\ttest's rmse: 3.65186\n",
      "[900]\ttrain's rmse: 3.35258\ttest's rmse: 3.6515\n",
      "[1000]\ttrain's rmse: 3.33414\ttest's rmse: 3.65148\n",
      "[1100]\ttrain's rmse: 3.31623\ttest's rmse: 3.65233\n",
      "Early stopping, best iteration is:\n",
      "[947]\ttrain's rmse: 3.34367\ttest's rmse: 3.65126\n",
      "Fold 18 RMSE : 3.651261\n",
      "Training until validation scores don't improve for 200 rounds.\n",
      "[100]\ttrain's rmse: 3.65013\ttest's rmse: 3.71622\n",
      "[200]\ttrain's rmse: 3.56582\ttest's rmse: 3.68221\n",
      "[300]\ttrain's rmse: 3.50878\ttest's rmse: 3.67115\n",
      "[400]\ttrain's rmse: 3.46804\ttest's rmse: 3.66598\n",
      "[500]\ttrain's rmse: 3.43607\ttest's rmse: 3.66229\n",
      "[600]\ttrain's rmse: 3.40907\ttest's rmse: 3.66104\n",
      "[700]\ttrain's rmse: 3.38479\ttest's rmse: 3.65994\n",
      "[800]\ttrain's rmse: 3.36444\ttest's rmse: 3.65987\n",
      "[900]\ttrain's rmse: 3.34666\ttest's rmse: 3.65968\n",
      "[1000]\ttrain's rmse: 3.32909\ttest's rmse: 3.6601\n",
      "Early stopping, best iteration is:\n",
      "[833]\ttrain's rmse: 3.35812\ttest's rmse: 3.65947\n",
      "Fold 19 RMSE : 3.659472\n",
      "Training until validation scores don't improve for 200 rounds.\n",
      "[100]\ttrain's rmse: 3.65006\ttest's rmse: 3.714\n",
      "[200]\ttrain's rmse: 3.56354\ttest's rmse: 3.68496\n",
      "[300]\ttrain's rmse: 3.50624\ttest's rmse: 3.67458\n",
      "[400]\ttrain's rmse: 3.46553\ttest's rmse: 3.66931\n",
      "[500]\ttrain's rmse: 3.43561\ttest's rmse: 3.66614\n",
      "[600]\ttrain's rmse: 3.4103\ttest's rmse: 3.66439\n",
      "[700]\ttrain's rmse: 3.38736\ttest's rmse: 3.66278\n",
      "[800]\ttrain's rmse: 3.36721\ttest's rmse: 3.66185\n",
      "[900]\ttrain's rmse: 3.351\ttest's rmse: 3.66106\n",
      "[1000]\ttrain's rmse: 3.33488\ttest's rmse: 3.66092\n",
      "[1100]\ttrain's rmse: 3.31826\ttest's rmse: 3.66083\n",
      "[1200]\ttrain's rmse: 3.30148\ttest's rmse: 3.66088\n",
      "[1300]\ttrain's rmse: 3.28723\ttest's rmse: 3.66047\n",
      "[1400]\ttrain's rmse: 3.27208\ttest's rmse: 3.66075\n",
      "[1500]\ttrain's rmse: 3.25673\ttest's rmse: 3.66096\n",
      "Early stopping, best iteration is:\n",
      "[1309]\ttrain's rmse: 3.28595\ttest's rmse: 3.6604\n",
      "Fold 20 RMSE : 3.660399\n",
      "Final RMSE : 3.643894 \n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'feature_importance_df_3' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<timed exec>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'feature_importance_df_3' is not defined"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "load_ver = 24\n",
    "write_ver = 25\n",
    "feat_gen_opt = False\n",
    "debug = False\n",
    "FEATS_EXCLUDED = ['first_active_month', 'target', 'card_id', 'outliers',\n",
    "                  'hist_purchase_date_max', 'hist_purchase_date_min', 'hist_card_id_size',\n",
    "                  'new_purchase_date_max', 'new_purchase_date_min', 'new_card_id_size',\n",
    "                  'OOF_PRED', 'month_0',\n",
    "                  'feature_1_2_cross_10','feature_1_2_cross_13','feature_2_2','feature_1_2_cross_7','feature_1_2_cross_1','feature_2_3_cross_3',\n",
    "                 ]\n",
    "\n",
    "submission_file_name = \"single_model_v\"+str(write_ver)\n",
    "with timer(\"Full model run\"):\n",
    "    # main(debug=False)\n",
    "    num_rows = 1000000 if debug else None\n",
    "    if feat_gen_opt:\n",
    "        with timer(\"train & test\"):\n",
    "            df = train_test(num_rows)\n",
    "            hist = transactions('hist', num_rows)\n",
    "            new = transactions('new', num_rows)\n",
    "        with open('../input/single_model_v'+str(write_ver)+'_data_part1.pkl', 'wb') as f:\n",
    "            pickle.dump([df, hist, new], f)\n",
    "        with timer(\"historical transactions\"):\n",
    "            df = pd.merge(df, hist, on='card_id', how='outer')\n",
    "        with timer(\"new merchants\"):\n",
    "            df = pd.merge(df, new, on='card_id', how='outer')\n",
    "#         with timer(\"additional features\"):\n",
    "#             df = additional_features(df)\n",
    "        with timer(\"split train & test\"):\n",
    "            train_df = df[df['target'].notnull()]\n",
    "            test_df = df[df['target'].isnull()]\n",
    "            del df\n",
    "            gc.collect()\n",
    "        with open('../input/single_model_v'+str(write_ver)+'_data_part2.pkl', 'wb') as f:\n",
    "            pickle.dump([train_df, test_df], f)\n",
    "        with timer(\"cleaning\"):\n",
    "            train_df_cleaned, test_df_cleaned = cleaning(train_df, test_df)\n",
    "        with open('../input/single_model_v'+str(write_ver)+'_data_part3.pkl', 'wb') as f:\n",
    "            pickle.dump([train_df_cleaned, test_df_cleaned], f)\n",
    "    else:\n",
    "        with open('../input/single_model_v'+str(load_ver)+'_data_part3.pkl', 'rb') as f:\n",
    "            [train_df_cleaned, test_df_cleaned] = pickle.load(f)\n",
    "            print(\"train_df_cleaned size: \" + str(train_df_cleaned.shape))\n",
    "            print(\"test_df_cleaned size: \" + str(test_df_cleaned.shape))\n",
    "    \n",
    "        with open('../input/single_model_v'+str(load_ver)+'_data_lgb_feat_sorted.pkl', 'rb') as f:\n",
    "            feat_sorted = pickle.load(f)\n",
    "    \n",
    "    with timer(\"Run LightGBM with kfold\"):\n",
    "#         feature_importance_df, sub_preds, feat_sorted = kfold_model(train_df_cleaned, test_df_cleaned, \n",
    "#                                                                     num_folds=10, model='lgb',\n",
    "#                                                                     stratified=False, debug=debug, saveOpt=True,\n",
    "#                                                                     feat_sorted=[], num_feat=None)\n",
    "#         with open('../input/single_model_v'+str(load_ver)+'_data_lgb_feat_sorted.pkl', 'wb') as f:\n",
    "#             pickle.dump(feat_sorted, f)\n",
    "\n",
    "        feature_importance_df_1, sub_preds_1, \\\n",
    "            feat_sorted_1, oof_preds_1 = kfold_model(train_df_cleaned, test_df_cleaned, \n",
    "                                                     num_folds=10, model='lgb',\n",
    "                                                     stratified=False, debug=debug, saveOpt=True,\n",
    "                                                     feat_sorted=feat_sorted, num_feat=100)\n",
    "        feature_importance_df_2, sub_preds_2, \\\n",
    "            feat_sorted_2, oof_pred_2 = kfold_model(train_df_cleaned, test_df_cleaned, \n",
    "                                                    num_folds=10, model='lgb',\n",
    "                                                    stratified=True, debug=debug, saveOpt=True,\n",
    "                                                    feat_sorted=feat_sorted, num_feat=200)\n",
    "        feature_importance_df_3, sub_preds_3, \\\n",
    "#             feat_sorted_3, oof_preds_3 = kfold_model(train_df_cleaned, test_df_cleaned, \n",
    "#                                                      num_folds=10, model='cat',\n",
    "#                                                      stratified=False, debug=debug, saveOpt=True,\n",
    "#                                                      feat_sorted=feat_sorted, num_feat=100)\n",
    "#         feature_importance_df_4, sub_preds_4, feat_sorted_4 = kfold_model(train_df_cleaned, test_df_cleaned,\n",
    "#                                                                           num_folds=10, model='xgb',\n",
    "#                                                                           stratified=True, debug=debug, saveOpt=True,\n",
    "#                                                                           feat_sorted=feat_sorted, num_feat=200)\n",
    "#     with timer(\"combine solutions\"):\n",
    "#         # sub_preds_list = [sub_preds_1, sub_preds_2, sub_preds_3, sub_preds_4]\n",
    "#         sub_preds_list = [sub_preds_1, sub_preds_2]\n",
    "#         combine_solution(test_df_cleaned, sub_preds_list, submission_file_name)\n",
    "    with timer(\"stacking\"):\n",
    "        # stacking([oof_preds_1, oof_pred_2, oof_pred_3], [sub_preds_1, sub_preds_2, sub_preds_3])\n",
    "        stacking([oof_preds_1, oof_pred_2], [sub_preds_1, sub_preds_2])\n",
    "    # with open('../input/single_model_v'+str(write_ver)+'_data_part4.pkl', 'wb') as f:\n",
    "    #    feat_summary_1 = feature_importance_df_1[[\"feature\", \"importance\"]].groupby(\"feature\").mean().sort_values(by=\"importance\", ascending=False).reset_index()\n",
    "    #    feat_summary_2 = feature_importance_df_2[[\"feature\", \"importance\"]].groupby(\"feature\").mean().sort_values(by=\"importance\", ascending=False).reset_index()\n",
    "    #    pickle.dump([feat_summary_1, feat_summary_2], f)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display_importances(feature_importance_df, 'lgb', False, figsize=(16, 200))\n",
    "# display_importances(feature_importance_df_1, 'cat', False, figsize=(16, 200))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- `single_model_v19_xgb_100_feat.csv` got 3.695 on LB.\n",
    "- `single_model_v19_lgb_10_fold_stratified_100_feat_3.6425.csv` got 3.695 on LB.\n",
    "- `single_model_v13_final.csv` got 3.694 on LB.\n",
    "- `single_model_v19_lgb_10_fold_stratified_50_feat_3.6501.csv` got 3.699 on LB.\n",
    "- `single_model_v19_lgb_10_fold_100_feat_3.6436.csv` got 3.695 on LB.\n",
    "- `single_model_v20_lgb_10_fold_100_feat_3.6436.csv` got 3.694 on LB.\n",
    "- `single_model_v22_lgb_10_fold_100_feat_3.6406.csv` got 3.694 on LB.\n",
    "- `Blend2_v10.csv` got 3.691 on LB."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_importance_df_1, sub_preds_1, feat_sorted_1 = kfold_model(train_df_cleaned, test_df_cleaned, \n",
    "                                                                  num_folds=10, model='lgb',\n",
    "                                                                  stratified=False, debug=debug, saveOpt=True,\n",
    "                                                                  feat_sorted=feat_sorted, num_feat=80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "feature_importance_df_1, sub_preds_1, feat_sorted_1 = kfold_model(train_df_cleaned, test_df_cleaned, \n",
    "                                                                  num_folds=10, model='lgb',\n",
    "                                                                  stratified=False, debug=debug, saveOpt=True,\n",
    "                                                                  feat_sorted=feat_sorted, num_feat=120)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feat_sel_summary = feature_importance_df[[\"feature\", \"importance\"]].groupby(\"feature\").mean().sort_values(by=\"importance\", ascending=False).reset_index()\n",
    "feat_sel_summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# temp = train_df_cleaned[feat_sorted[:200]].corrwith(train_df['target']).reset_index()\n",
    "temp = train_df_cleaned.corrwith(train_df_cleaned['target']).reset_index()\n",
    "temp = temp.rename(columns={'index': 'feat', 0: 'corr'}).sort_values(by='corr', ascending=False)\n",
    "temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df['new_duration_var'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df_cleaned, test_df_cleaned = cleaning(train_df, test_df)\n",
    "train_df['new_duration_var'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "oldHeight": 436.969,
   "position": {
    "height": "40px",
    "left": "1563px",
    "right": "20px",
    "top": "43.9688px",
    "width": "278.969px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "varInspector_section_display": "none",
   "window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
