{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "a9ef20fd4e6471fd2c9e3022edfdddc07a970aca"
   },
   "source": [
    "## P.S. The main idea behind this notebook is inspired from FabienDaniel Kernel Elo_world.\n",
    "https://www.kaggle.com/fabiendaniel/elo-world"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd \n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import lightgbm as lgb\n",
    "from sklearn.model_selection import KFold\n",
    "import warnings\n",
    "import time\n",
    "import sys\n",
    "import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.linear_model import BayesianRidge\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "import gc\n",
    "import pickle\n",
    "from sklearn.base import BaseEstimator\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "from sklearn.metrics import log_loss\n",
    "pd.set_option('display.max_rows', 500)\n",
    "pd.set_option('display.max_columns', 500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [],
   "source": [
    "def reduce_mem_usage(df, verbose=True):\n",
    "    numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n",
    "    start_mem = df.memory_usage().sum() / 1024**2    \n",
    "    for col in df.columns:\n",
    "        col_type = df[col].dtypes\n",
    "        if col_type in numerics:\n",
    "            c_min = df[col].min()\n",
    "            c_max = df[col].max()\n",
    "            if str(col_type)[:3] == 'int':\n",
    "                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n",
    "                    df[col] = df[col].astype(np.int8)\n",
    "                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n",
    "                    df[col] = df[col].astype(np.int16)\n",
    "                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n",
    "                    df[col] = df[col].astype(np.int32)\n",
    "                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n",
    "                    df[col] = df[col].astype(np.int64)  \n",
    "            else:\n",
    "                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n",
    "                    df[col] = df[col].astype(np.float16)\n",
    "                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n",
    "                    df[col] = df[col].astype(np.float32)\n",
    "                else:\n",
    "                    df[col] = df[col].astype(np.float64)    \n",
    "    end_mem = df.memory_usage().sum() / 1024**2\n",
    "    if verbose: print('Mem. usage decreased to {:5.2f} Mb ({:.1f}% reduction)'.format(end_mem, 100 * (start_mem - end_mem) / start_mem))\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "50e1f88df96b5e525237fe120650e679b7f60654"
   },
   "outputs": [],
   "source": [
    "# new_transactions = pd.read_csv('../input/elo-merchant-category-recommendation/new_merchant_transactions.csv', parse_dates=['purchase_date'])\n",
    "# historical_transactions = pd.read_csv('../input/elo-merchant-category-recommendation/historical_transactions.csv', parse_dates=['purchase_date'])\n",
    "\n",
    "historical_transactions = pd.read_parquet('../input/hist_trans_df.parquet.gzip')\n",
    "new_transactions = pd.read_parquet('../input/new_trans_df.parquet.gzip')\n",
    "\n",
    "def binarize(df):\n",
    "    for col in ['authorized_flag', 'category_1']:\n",
    "        df[col] = df[col].map({'Y':1, 'N':0})\n",
    "    return df\n",
    "\n",
    "historical_transactions = binarize(historical_transactions)\n",
    "new_transactions = binarize(new_transactions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "a940a9ce940fa489a5a5255f6d4523c9b7c3ffdc"
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "# def read_data(input_file):\n",
    "#     df = pd.read_csv(input_file)\n",
    "#     df['first_active_month'] = pd.to_datetime(df['first_active_month'])\n",
    "#     df['elapsed_time'] = (datetime.date(2018, 2, 1) - df['first_active_month'].dt.date).dt.days\n",
    "#     return df\n",
    "\n",
    "# train = read_data('../input/elo-merchant-category-recommendation/train.csv')\n",
    "# test = read_data('../input/elo-merchant-category-recommendation/test.csv')\n",
    "\n",
    "def read_data_v2(input_file):\n",
    "    df = pd.read_parquet(input_file)\n",
    "    df['first_active_month'] = pd.to_datetime(df['first_active_month'])\n",
    "    df['elapsed_time'] = (datetime.date(2018, 2, 1) - df['first_active_month'].dt.date).dt.days\n",
    "    return df\n",
    "\n",
    "train = read_data_v2('../input/train_df.parquet.gzip')\n",
    "test = read_data_v2('../input/test_df.parquet.gzip')\n",
    "\n",
    "target = train['target']\n",
    "del train['target']\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "bd9f71f39664eef55e0b3ec3e6172e84a8e1a963"
   },
   "source": [
    "## **Feature Engineering**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_opt = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "historical_transactions[\"category_1\"] += 1\n",
    "historical_transactions[\"category_1\"].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_transactions[\"category_1\"] += 1\n",
    "new_transactions[\"category_1\"].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "historical_transactions[\"category_2\"].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "historical_transactions[\"category_2\"] = historical_transactions[\"category_2\"].fillna(0)\n",
    "historical_transactions[\"category_2\"].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_transactions[\"category_2\"].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_transactions[\"category_2\"] = new_transactions[\"category_2\"].fillna(0)\n",
    "new_transactions[\"category_2\"].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "historical_transactions[\"category_3\"].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "historical_transactions[\"category_3\"] = historical_transactions[\"category_3\"].replace({'A': 1, 'B': 2, 'C': 3, None: 0})\n",
    "historical_transactions[\"category_3\"].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_transactions[\"category_3\"].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_transactions[\"category_3\"] = new_transactions[\"category_3\"].replace({'A': 1, 'B': 2, 'C': 3, None: 0})\n",
    "new_transactions[\"category_3\"].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# historical_transactions[\"category_1_2_cross\"] = historical_transactions[\"category_1\"]*2 + historical_transactions[\"category_2\"]\n",
    "# np.sort(historical_transactions[\"category_1_2_cross\"].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "# if plot_opt:\n",
    "#     plt.scatter(new_transactions[\"category_1\"], new_transactions[\"category_2\"])\n",
    "#     plt.xlabel(\"category_1\")\n",
    "#     plt.ylabel(\"category_2\")\n",
    "#     plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# new_transactions[\"category_1_2_cross\"] = new_transactions[\"category_1\"]*2 + new_transactions[\"category_2\"]\n",
    "# np.sort(new_transactions[\"category_1_2_cross\"].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "# if plot_opt:\n",
    "#     plt.scatter(historical_transactions[\"category_1\"]+2, historical_transactions[\"category_3\"])\n",
    "#     plt.xlabel(\"category_1\")\n",
    "#     plt.ylabel(\"category_3\")\n",
    "#     plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# historical_transactions[\"category_1_3_cross\"] = (historical_transactions[\"category_1\"]+2) * historical_transactions[\"category_3\"]\n",
    "# np.sort(historical_transactions[\"category_1_3_cross\"].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "# if plot_opt:\n",
    "#     plt.scatter(new_transactions[\"category_1\"], new_transactions[\"category_3\"])\n",
    "#     plt.xlabel(\"category_1\")\n",
    "#     plt.ylabel(\"category_3\")\n",
    "#     plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# new_transactions[\"category_1_3_cross\"] = (new_transactions[\"category_1\"]+2) * new_transactions[\"category_3\"]\n",
    "# np.sort(new_transactions[\"category_1_3_cross\"].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# %%time\n",
    "# if plot_opt:\n",
    "#     plt.scatter(historical_transactions[\"category_2\"]+6, historical_transactions[\"category_3\"])\n",
    "#     plt.xlabel(\"category_2\")\n",
    "#     plt.ylabel(\"category_3\")\n",
    "#     plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# historical_transactions[\"category_2_3_cross\"] = (historical_transactions[\"category_2\"]+6) * historical_transactions[\"category_3\"]\n",
    "# np.sort(historical_transactions[\"category_2_3_cross\"].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "# if plot_opt:\n",
    "#     plt.scatter(new_transactions[\"category_2\"]+6, new_transactions[\"category_3\"])\n",
    "#     plt.xlabel(\"category_2\")\n",
    "#     plt.ylabel(\"category_3\")\n",
    "#     plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# new_transactions[\"category_2_3_cross\"] = (new_transactions[\"category_2\"]+6) * new_transactions[\"category_3\"]\n",
    "# np.sort(new_transactions[\"category_2_3_cross\"].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "def category_make_cross_feat(df):\n",
    "    df[\"category_1_3_cross\"] = (df[\"category_1\"]+2) * df[\"category_3\"]\n",
    "    df[\"category_2_3_cross\"] = (df[\"category_2\"]+6) * df[\"category_3\"]\n",
    "    return df\n",
    "\n",
    "historical_transactions = category_make_cross_feat(historical_transactions)\n",
    "new_transactions = category_make_cross_feat(new_transactions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "170f0e959ba5250b191b7cee6b586bdabd347018"
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "historical_transactions = pd.get_dummies(historical_transactions, columns=['category_2', \n",
    "                                                                           'category_3',\n",
    "                                                                           'category_1_3_cross',\n",
    "                                                                           'category_2_3_cross'])\n",
    "new_transactions = pd.get_dummies(new_transactions, columns=['category_2', \n",
    "                                                             'category_3',\n",
    "                                                             'category_1_3_cross',\n",
    "                                                             'category_2_3_cross'])\n",
    "\n",
    "historical_transactions = reduce_mem_usage(historical_transactions)\n",
    "new_transactions = reduce_mem_usage(new_transactions)\n",
    "\n",
    "agg_fun = {'authorized_flag': ['sum', 'mean', 'min', 'std', 'count']} # max is all 1's, useless\n",
    "auth_mean = historical_transactions.groupby(['card_id']).agg(agg_fun)\n",
    "auth_mean.columns = ['_'.join(col).strip() for col in auth_mean.columns.values]\n",
    "auth_mean.reset_index(inplace=True)\n",
    "\n",
    "authorized_transactions = historical_transactions[historical_transactions['authorized_flag'] == 1]\n",
    "historical_transactions = historical_transactions[historical_transactions['authorized_flag'] == 0]\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "auth_mean.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "90536f89e81fc4bb7f45a17f47e4b627f53e69b4"
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "historical_transactions['purchase_month'] = historical_transactions['purchase_date'].dt.month\n",
    "authorized_transactions['purchase_month'] = authorized_transactions['purchase_date'].dt.month\n",
    "new_transactions['purchase_month'] = new_transactions['purchase_date'].dt.month\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_transactions.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "b5de9c3bcc2ad2dfeb943e1b02fef3e433c91af3"
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "def aggregate_transactions(history):\n",
    "    \n",
    "    history.loc[:, 'purchase_date'] = pd.DatetimeIndex(history['purchase_date']).\\\n",
    "                                      astype(np.int64) * 1e-9\n",
    "    \n",
    "    agg_func = {\n",
    "        'category_1': ['sum', 'mean'],\n",
    "        'category_2_1.0': ['mean'],\n",
    "        'category_2_2.0': ['mean'],\n",
    "        'category_2_3.0': ['mean'],\n",
    "        'category_2_4.0': ['mean'],\n",
    "        'category_2_5.0': ['mean'],\n",
    "        'category_3_1': ['mean'],\n",
    "        'category_3_2': ['mean'],\n",
    "        'category_3_3': ['mean'],\n",
    "        'category_1_3_cross_3': ['mean'],\n",
    "        'category_1_3_cross_4': ['mean'],\n",
    "        'category_1_3_cross_6': ['mean'],\n",
    "        'category_1_3_cross_8': ['mean'],\n",
    "        'category_1_3_cross_9': ['mean'],\n",
    "        'category_1_3_cross_12': ['mean'],\n",
    "        'category_2_3_cross_7.0': ['mean'],\n",
    "        'category_2_3_cross_8.0': ['mean'],\n",
    "        'category_2_3_cross_9.0': ['mean'],\n",
    "        'category_2_3_cross_10.0': ['mean'],\n",
    "        'category_2_3_cross_11.0': ['mean'],\n",
    "        'category_2_3_cross_12.0': ['mean'],\n",
    "        'category_2_3_cross_14.0': ['mean'],\n",
    "        'category_2_3_cross_16.0': ['mean'],\n",
    "        'category_2_3_cross_20.0': ['mean'],\n",
    "        'category_2_3_cross_21.0': ['mean'],\n",
    "        'category_2_3_cross_22.0': ['mean'],\n",
    "        'category_2_3_cross_24.0': ['mean'],\n",
    "        'category_2_3_cross_27.0': ['mean'],\n",
    "        'category_2_3_cross_30.0': ['mean'],\n",
    "        'category_2_3_cross_33.0': ['mean'],\n",
    "        'merchant_id': ['nunique'],\n",
    "        'merchant_category_id': ['nunique'],\n",
    "        'state_id': ['nunique'],\n",
    "        'city_id': ['nunique'],\n",
    "        'subsector_id': ['nunique'],\n",
    "        'purchase_amount': ['sum', 'mean', 'max', 'min', 'std', 'count'], #one count is enough, others are just the same\n",
    "        'installments': ['sum', 'mean', 'max', 'min', 'std'],\n",
    "        'purchase_month': ['mean', 'max', 'min', 'std'],\n",
    "        'purchase_date': [np.ptp, 'min', 'max'],\n",
    "        'month_lag': ['min', 'max']\n",
    "        }\n",
    "    \n",
    "    agg_history = history.groupby(['card_id']).agg(agg_func)\n",
    "    agg_history.columns = ['_'.join(col).strip() for col in agg_history.columns.values]\n",
    "    agg_history.reset_index(inplace=True)\n",
    "    \n",
    "    df = (history.groupby('card_id')\n",
    "          .size()\n",
    "          .reset_index(name='transactions_count'))\n",
    "    \n",
    "    agg_history = pd.merge(df, agg_history, on='card_id', how='left')\n",
    "    \n",
    "    return agg_history\n",
    "\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "bf1f963f61841ef34f79fd16364bbf59b7f4e8a4"
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "history = aggregate_transactions(historical_transactions)\n",
    "history.columns = ['hist_' + c if c != 'card_id' else c for c in history.columns]\n",
    "history[:5]\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # history[[\"hist_purchase_amount_count\", \"hist_installments_count\", \"hist_purchase_month_count\"]].head(500)\n",
    "# history.head(500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "4c1bdbaa7c16e22436893354b31d47e1846330ab"
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "authorized = aggregate_transactions(authorized_transactions)\n",
    "authorized.columns = ['auth_' + c if c != 'card_id' else c for c in authorized.columns]\n",
    "authorized[:5]\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "4cc2775c29c17cb7438f3f5c45777b894afa94fc"
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "new = aggregate_transactions(new_transactions)\n",
    "new.columns = ['new_' + c if c != 'card_id' else c for c in new.columns]\n",
    "new[:5]\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "fb443fb1e2a47f5a0bef02921a2cba1d4e95e673"
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "def aggregate_per_month(history):\n",
    "    grouped = history.groupby(['card_id', 'month_lag'])\n",
    "\n",
    "    agg_func = {\n",
    "#             'purchase_amount': ['count', 'sum', 'mean', 'min', 'max', 'std'],\n",
    "#             'installments': ['count', 'sum', 'mean', 'min', 'max', 'std'],\n",
    "            'purchase_amount': ['count', 'sum'],\n",
    "            'installments': ['count', 'sum'],\n",
    "            }\n",
    "\n",
    "    intermediate_group = grouped.agg(agg_func)\n",
    "    intermediate_group.columns = ['_'.join(col).strip() for col in intermediate_group.columns.values]\n",
    "    intermediate_group.reset_index(inplace=True)\n",
    "\n",
    "    final_group = intermediate_group.groupby('card_id').agg(['mean', 'std'])\n",
    "    final_group.columns = ['_'.join(col).strip() for col in final_group.columns.values]\n",
    "    final_group.reset_index(inplace=True)\n",
    "    \n",
    "    return final_group\n",
    "#___________________________________________________________\n",
    "final_group =  aggregate_per_month(historical_transactions) \n",
    "final_group[:10]\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "5e5e313f605160783127a6f0798f176eaf6a55a4"
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "train = pd.merge(train, history, on='card_id', how='left')\n",
    "test = pd.merge(test, history, on='card_id', how='left')\n",
    "\n",
    "train = pd.merge(train, authorized, on='card_id', how='left')\n",
    "test = pd.merge(test, authorized, on='card_id', how='left')\n",
    "\n",
    "train = pd.merge(train, new, on='card_id', how='left')\n",
    "test = pd.merge(test, new, on='card_id', how='left')\n",
    "\n",
    "train = pd.merge(train, final_group, on='card_id', how='left')\n",
    "test = pd.merge(test, final_group, on='card_id', how='left')\n",
    "\n",
    "train = pd.merge(train, auth_mean, on='card_id', how='left')\n",
    "test = pd.merge(test, auth_mean, on='card_id', how='left')\n",
    "\n",
    "print(\"Train Shape:\", train.shape)\n",
    "print(\"Test Shape:\", test.shape)\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_feat(df):\n",
    "    # Feature crosses\n",
    "    df[\"feature_1_2_cross\"] = df[\"feature_1\"] + (df[\"feature_2\"]-1)*5\n",
    "    df[\"feature_1_3_cross\"] = df[\"feature_1\"] + df[\"feature_3\"]*3\n",
    "    df[\"feature_2_3_cross\"] = df[\"feature_2\"] + df[\"feature_3\"]*3\n",
    "    df = pd.get_dummies(df, columns=[\"feature_1_2_cross\", \"feature_1_3_cross\", \"feature_2_3_cross\"])\n",
    "    \n",
    "    return df\n",
    "\n",
    "train = generate_feat(train)\n",
    "test = generate_feat(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def power_2_3_feat(df, feat_list):\n",
    "    for feat in feat_list:\n",
    "        df[feat+\"_power2\"] = df[feat]**2\n",
    "#         df[feat+\"_power3\"] = df[feat]**3\n",
    "    return df\n",
    "\n",
    "feat_list = [\"elapsed_time\", \"hist_purchase_date_ptp\"]\n",
    "train = power_2_3_feat(train, feat_list)\n",
    "test = power_2_3_feat(test, feat_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_feat(df, feat_list):\n",
    "    for feat in feat_list:\n",
    "        df[feat+\"_log\"] = np.log(df[feat])\n",
    "#         df[feat+\"_power3\"] = df[feat]**3\n",
    "    return df\n",
    "\n",
    "feat_list = [\"elapsed_time\"]\n",
    "train = log_feat(train, feat_list)\n",
    "test = log_feat(test, feat_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corrmat = train.corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "f, ax = plt.subplots(figsize=(100, 100))\n",
    "sns.heatmap(corrmat, vmax=1.0, square=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f.savefig(\"../img/corr.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "remove_feat_list = ['authorized_flag_min', \n",
    "                    'hist_purchase_amount_sum',\n",
    "                    'hist_purchase_amount_max',\n",
    "                    'hist_purchase_amount_min',\n",
    "                    'hist_purchase_amount_std',\n",
    "                    'hist_installments_sum',\n",
    "                    'hist_installments_max',\n",
    "                    'hist_installments_min',\n",
    "                    'hist_installments_std',\n",
    "                   ]\n",
    "train = train.drop(remove_feat_list, axis=1)\n",
    "test = test.drop(remove_feat_list, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corrmat = train.corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f, ax = plt.subplots(figsize=(100, 100))\n",
    "sns.heatmap(corrmat, vmax=1.0, square=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f.savefig(\"../img/corr_after_feat_removal.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open('../input/feat_list.pkl', 'rb') as f:\n",
    "#     feat_list = pickle.load(f)\n",
    "\n",
    "# train = train[feat_list]\n",
    "# test = test[feat_list]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### save train, test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../input/train_test_target.pkl', 'wb') as f:\n",
    "    pickle.dump([train, target, test], f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../input/train_test_target.pkl', 'rb') as f:\n",
    "    [train, target, test] = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://www.kaggle.com/pmarcelino/comprehensive-data-exploration-with-python\n",
    "# saleprice correlation matrix\n",
    "\n",
    "# k = train.shape[1] #number of variables for heatmap\n",
    "# cols = corrmat.nlargest(k, 'SalePrice')['SalePrice'].index\n",
    "# cm = np.corrcoef(df_train[cols].values.T)\n",
    "# sns.set(font_scale=1.25)\n",
    "# hm = sns.heatmap(cm, cbar=True, annot=True, square=True, fmt='.2f', annot_kws={'size': 10}, yticklabels=cols.values, xticklabels=cols.values)\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #scatterplot\n",
    "# sns.set()\n",
    "# # cols = ['SalePrice', 'OverallQual', 'GrLivArea', 'GarageCars', 'TotalBsmtSF', 'FullBath', 'YearBuilt']\n",
    "# # sns.pairplot(df_train[cols], size = 2.5)\n",
    "# sns.pairplot(train, size = 2.5)\n",
    "# plt.show();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = [c for c in train.columns if c not in ['card_id', 'first_active_month']]\n",
    "categorical_feats = [c for c in features if 'feature_' in c]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = train.fillna(0)\n",
    "test = test.fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Outlier removal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12,160))\n",
    "sns.distplot(target.values, bins=50, kde=False, color='blue')\n",
    "plt.title('Histogram of Loyalty Score before removal')\n",
    "plt.xlabel('Loyalty score', fontsize=12)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import norm\n",
    "from scipy import stats\n",
    "sns.distplot(target, fit=norm)\n",
    "fig = plt.figure()\n",
    "res = stats.probplot(target, plot=plt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#skewness and kurtosis\n",
    "print(\"Skewness: %f\" % pd.DataFrame(target).skew())\n",
    "print(\"Kurtosis: %f\" % pd.DataFrame(target).kurt())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_target = min(value for value in target if value > -20)\n",
    "min_target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx_between_20_30 = [value for value in target if value >= -30 and value <=-20]\n",
    "len(idx_between_20_30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx_lessThan_30 = [value for value in target if value < -30]\n",
    "len(idx_lessThan_30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://www.kaggle.com/nottold/naive-ensemble-model-ridge-lasso\n",
    "class OutlierDetection(BaseEstimator):\n",
    "    def __init__(self, alpha, dims, std, mean, median):\n",
    "        self.alpha = alpha\n",
    "        self.dims = dims\n",
    "        self.std = std\n",
    "        self.mean = mean\n",
    "        self.median = median\n",
    "    def fit(self, X):\n",
    "        # std, mean, median = X.std(), X.mean(), X.median()\n",
    "        X[\"outliers\"] = 0\n",
    "        for col in X.columns:\n",
    "#             print(col)\n",
    "            if not col == \"outliers\":\n",
    "                # outlier_idx = (abs(X[col]) > (self.alpha * std[col] + mean[col]))\n",
    "                outlier_idx = (np.abs(X[col]) > (self.alpha * self.std[col] + self.mean[col]))\n",
    "                X.set_value(outlier_idx, \"outliers\", X[outlier_idx][\"outliers\"] + 1)\n",
    "        outliers = X[X[\"outliers\"] > self.dims]\n",
    "        X.drop(\"outliers\", axis=1, inplace=True)\n",
    "        outlier_idx = outliers.index.tolist()\n",
    "        # return outliers.index\n",
    "        return set(list(range(X.shape[0]))) - set(outlier_idx), outlier_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_df = pd.DataFrame(target)\n",
    "\n",
    "outlier_removal = OutlierDetection(alpha=3, \n",
    "                                   dims=0, \n",
    "                                   std=target_df.std().astype('float'), \n",
    "                                   mean=target_df.mean().astype('float'), \n",
    "                                   median=target_df.median().astype('float'))\n",
    "normal_idx, outlier_idx = outlier_removal.fit(target_df)\n",
    "# samples = target_df.shape[0] - len(outlier)\n",
    "# xtrain = xtrain.drop(outlier_index).reset_index(drop=True)\n",
    "# y = y.drop(outlier_index).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "796d4e17b4c4cc3121fc833669fc2eaf07bf4fba"
   },
   "outputs": [],
   "source": [
    "train[\"outliers\"] = 0\n",
    "train.at[outlier_idx, \"outliers\"] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train[\"outliers\"].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../input/train_test_target_with_target.pkl', 'wb') as f:\n",
    "    pickle.dump([train, target, test, normal_idx, outlier_idx], f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../input/train_test_target_with_target.pkl', 'rb') as f:\n",
    "    [train, target, test, normal_idx, outlier_idx] = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Training Model Without Outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = train[train['outliers'] == 0]\n",
    "df_target = target[normal_idx]\n",
    "features = [c for c in df_train.columns if c not in ['card_id', 'first_active_month','outliers']]\n",
    "categorical_feats = [c for c in features if 'feature_' in c]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((199644, 212), (199644,))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.shape, df_target.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lgbm_regression_train(train, target, test, param, features, categorical_feats):\n",
    "    folds = KFold(n_splits=5, shuffle=True, random_state=15)\n",
    "    oof = np.zeros(len(train))\n",
    "    predictions = np.zeros(len(test))\n",
    "    start = time.time()\n",
    "    feature_importance_df = pd.DataFrame()\n",
    "\n",
    "    for fold_, (trn_idx, val_idx) in enumerate(folds.split(train.values, target.values)):\n",
    "        print(\"fold n°{}\".format(fold_))\n",
    "        trn_data = lgb.Dataset(train.iloc[trn_idx][features], label=target.iloc[trn_idx], categorical_feature=categorical_feats)\n",
    "        val_data = lgb.Dataset(train.iloc[val_idx][features], label=target.iloc[val_idx], categorical_feature=categorical_feats)\n",
    "\n",
    "        num_round = 10000\n",
    "        clf = lgb.train(param, trn_data, num_round, valid_sets = [trn_data, val_data], verbose_eval=100, early_stopping_rounds = 200)\n",
    "        oof[val_idx] = clf.predict(train.iloc[val_idx][features], num_iteration=clf.best_iteration)\n",
    "\n",
    "        fold_importance_df = pd.DataFrame()\n",
    "        fold_importance_df[\"feature\"] = features\n",
    "        fold_importance_df[\"importance\"] = clf.feature_importance()\n",
    "        fold_importance_df[\"fold\"] = fold_ + 1\n",
    "        feature_importance_df = pd.concat([feature_importance_df, fold_importance_df], axis=0)\n",
    "\n",
    "        predictions += clf.predict(test[features], num_iteration=clf.best_iteration) / folds.n_splits\n",
    "\n",
    "    print(\"CV score: {:<8.5f}\".format(mean_squared_error(oof, target)**0.5))\n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fold n°0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/frank/miniconda2/envs/python35/lib/python3.5/site-packages/lightgbm/basic.py:1158: UserWarning: Using categorical_feature in Dataset.\n",
      "  warnings.warn('Using categorical_feature in Dataset.')\n",
      "/home/frank/miniconda2/envs/python35/lib/python3.5/site-packages/lightgbm/basic.py:725: UserWarning: categorical_feature in param dict is overridden.\n",
      "  warnings.warn('categorical_feature in param dict is overridden.')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 200 rounds.\n",
      "[100]\ttraining's rmse: 1.5885\tvalid_1's rmse: 1.6088\n",
      "[200]\ttraining's rmse: 1.55768\tvalid_1's rmse: 1.58216\n",
      "[300]\ttraining's rmse: 1.54258\tvalid_1's rmse: 1.57098\n",
      "[400]\ttraining's rmse: 1.53261\tvalid_1's rmse: 1.56486\n",
      "[500]\ttraining's rmse: 1.52505\tvalid_1's rmse: 1.56149\n",
      "[600]\ttraining's rmse: 1.51892\tvalid_1's rmse: 1.55916\n",
      "[700]\ttraining's rmse: 1.51377\tvalid_1's rmse: 1.5577\n",
      "[800]\ttraining's rmse: 1.50921\tvalid_1's rmse: 1.55672\n",
      "[900]\ttraining's rmse: 1.50498\tvalid_1's rmse: 1.55607\n",
      "[1000]\ttraining's rmse: 1.50102\tvalid_1's rmse: 1.55558\n",
      "[1100]\ttraining's rmse: 1.49723\tvalid_1's rmse: 1.55517\n",
      "[1200]\ttraining's rmse: 1.49361\tvalid_1's rmse: 1.5549\n",
      "[1300]\ttraining's rmse: 1.49009\tvalid_1's rmse: 1.55471\n",
      "[1400]\ttraining's rmse: 1.48662\tvalid_1's rmse: 1.55447\n",
      "[1500]\ttraining's rmse: 1.48328\tvalid_1's rmse: 1.55427\n",
      "[1600]\ttraining's rmse: 1.47998\tvalid_1's rmse: 1.55411\n",
      "[1700]\ttraining's rmse: 1.47671\tvalid_1's rmse: 1.55401\n",
      "[1800]\ttraining's rmse: 1.47344\tvalid_1's rmse: 1.55386\n",
      "[1900]\ttraining's rmse: 1.47029\tvalid_1's rmse: 1.55375\n",
      "[2000]\ttraining's rmse: 1.46719\tvalid_1's rmse: 1.55363\n",
      "[2100]\ttraining's rmse: 1.46418\tvalid_1's rmse: 1.55357\n",
      "[2200]\ttraining's rmse: 1.46127\tvalid_1's rmse: 1.55355\n",
      "[2300]\ttraining's rmse: 1.45826\tvalid_1's rmse: 1.55344\n",
      "[2400]\ttraining's rmse: 1.45528\tvalid_1's rmse: 1.55343\n",
      "[2500]\ttraining's rmse: 1.45235\tvalid_1's rmse: 1.5534\n",
      "[2600]\ttraining's rmse: 1.4494\tvalid_1's rmse: 1.55332\n",
      "[2700]\ttraining's rmse: 1.44665\tvalid_1's rmse: 1.55332\n",
      "[2800]\ttraining's rmse: 1.44386\tvalid_1's rmse: 1.55325\n",
      "[2900]\ttraining's rmse: 1.44117\tvalid_1's rmse: 1.55325\n",
      "[3000]\ttraining's rmse: 1.43846\tvalid_1's rmse: 1.55323\n",
      "[3100]\ttraining's rmse: 1.43562\tvalid_1's rmse: 1.55318\n",
      "[3200]\ttraining's rmse: 1.43285\tvalid_1's rmse: 1.55324\n",
      "Early stopping, best iteration is:\n",
      "[3072]\ttraining's rmse: 1.43642\tvalid_1's rmse: 1.55316\n",
      "fold n°1\n",
      "Training until validation scores don't improve for 200 rounds.\n",
      "[100]\ttraining's rmse: 1.59281\tvalid_1's rmse: 1.59353\n",
      "[200]\ttraining's rmse: 1.56239\tvalid_1's rmse: 1.56642\n",
      "[300]\ttraining's rmse: 1.54728\tvalid_1's rmse: 1.55437\n",
      "[400]\ttraining's rmse: 1.53734\tvalid_1's rmse: 1.54791\n",
      "[500]\ttraining's rmse: 1.52983\tvalid_1's rmse: 1.544\n",
      "[600]\ttraining's rmse: 1.52384\tvalid_1's rmse: 1.54173\n",
      "[700]\ttraining's rmse: 1.5186\tvalid_1's rmse: 1.54031\n",
      "[800]\ttraining's rmse: 1.51391\tvalid_1's rmse: 1.5394\n",
      "[900]\ttraining's rmse: 1.5096\tvalid_1's rmse: 1.53871\n",
      "[1000]\ttraining's rmse: 1.50555\tvalid_1's rmse: 1.53825\n",
      "[1100]\ttraining's rmse: 1.50178\tvalid_1's rmse: 1.53796\n",
      "[1200]\ttraining's rmse: 1.49817\tvalid_1's rmse: 1.53779\n",
      "[1300]\ttraining's rmse: 1.49455\tvalid_1's rmse: 1.53767\n",
      "[1400]\ttraining's rmse: 1.49116\tvalid_1's rmse: 1.5375\n",
      "[1500]\ttraining's rmse: 1.48784\tvalid_1's rmse: 1.53733\n",
      "[1600]\ttraining's rmse: 1.48453\tvalid_1's rmse: 1.53718\n",
      "[1700]\ttraining's rmse: 1.48124\tvalid_1's rmse: 1.53703\n",
      "[1800]\ttraining's rmse: 1.47808\tvalid_1's rmse: 1.5369\n",
      "[1900]\ttraining's rmse: 1.47489\tvalid_1's rmse: 1.53678\n",
      "[2000]\ttraining's rmse: 1.47181\tvalid_1's rmse: 1.53662\n",
      "[2100]\ttraining's rmse: 1.46883\tvalid_1's rmse: 1.5366\n",
      "[2200]\ttraining's rmse: 1.46583\tvalid_1's rmse: 1.53657\n",
      "[2300]\ttraining's rmse: 1.46275\tvalid_1's rmse: 1.53652\n",
      "[2400]\ttraining's rmse: 1.45988\tvalid_1's rmse: 1.53645\n",
      "[2500]\ttraining's rmse: 1.45691\tvalid_1's rmse: 1.53635\n",
      "[2600]\ttraining's rmse: 1.45385\tvalid_1's rmse: 1.53633\n",
      "[2700]\ttraining's rmse: 1.45098\tvalid_1's rmse: 1.5363\n",
      "[2800]\ttraining's rmse: 1.44809\tvalid_1's rmse: 1.53637\n",
      "[2900]\ttraining's rmse: 1.44522\tvalid_1's rmse: 1.53637\n",
      "Early stopping, best iteration is:\n",
      "[2725]\ttraining's rmse: 1.45026\tvalid_1's rmse: 1.53628\n",
      "fold n°2\n",
      "Training until validation scores don't improve for 200 rounds.\n",
      "[100]\ttraining's rmse: 1.59057\tvalid_1's rmse: 1.60131\n",
      "[200]\ttraining's rmse: 1.56017\tvalid_1's rmse: 1.57376\n",
      "[300]\ttraining's rmse: 1.5451\tvalid_1's rmse: 1.56178\n",
      "[400]\ttraining's rmse: 1.53514\tvalid_1's rmse: 1.55538\n",
      "[500]\ttraining's rmse: 1.5276\tvalid_1's rmse: 1.55181\n",
      "[600]\ttraining's rmse: 1.52153\tvalid_1's rmse: 1.5497\n",
      "[700]\ttraining's rmse: 1.51629\tvalid_1's rmse: 1.54831\n",
      "[800]\ttraining's rmse: 1.51163\tvalid_1's rmse: 1.54745\n",
      "[900]\ttraining's rmse: 1.50741\tvalid_1's rmse: 1.54693\n",
      "[1000]\ttraining's rmse: 1.50338\tvalid_1's rmse: 1.54645\n",
      "[1100]\ttraining's rmse: 1.49948\tvalid_1's rmse: 1.54613\n",
      "[1200]\ttraining's rmse: 1.49583\tvalid_1's rmse: 1.54581\n",
      "[1300]\ttraining's rmse: 1.49223\tvalid_1's rmse: 1.54562\n",
      "[1400]\ttraining's rmse: 1.48873\tvalid_1's rmse: 1.54546\n",
      "[1500]\ttraining's rmse: 1.4853\tvalid_1's rmse: 1.54531\n",
      "[1600]\ttraining's rmse: 1.48199\tvalid_1's rmse: 1.54519\n",
      "[1700]\ttraining's rmse: 1.47869\tvalid_1's rmse: 1.54508\n",
      "[1800]\ttraining's rmse: 1.47538\tvalid_1's rmse: 1.545\n",
      "[1900]\ttraining's rmse: 1.47216\tvalid_1's rmse: 1.54495\n",
      "[2000]\ttraining's rmse: 1.46909\tvalid_1's rmse: 1.54487\n",
      "[2100]\ttraining's rmse: 1.46605\tvalid_1's rmse: 1.54485\n",
      "[2200]\ttraining's rmse: 1.46294\tvalid_1's rmse: 1.5448\n",
      "[2300]\ttraining's rmse: 1.45992\tvalid_1's rmse: 1.5448\n",
      "[2400]\ttraining's rmse: 1.45694\tvalid_1's rmse: 1.5447\n",
      "[2500]\ttraining's rmse: 1.45392\tvalid_1's rmse: 1.54472\n",
      "[2600]\ttraining's rmse: 1.45105\tvalid_1's rmse: 1.54472\n",
      "[2700]\ttraining's rmse: 1.44819\tvalid_1's rmse: 1.54467\n",
      "[2800]\ttraining's rmse: 1.44532\tvalid_1's rmse: 1.54463\n",
      "[2900]\ttraining's rmse: 1.44239\tvalid_1's rmse: 1.54466\n",
      "Early stopping, best iteration is:\n",
      "[2795]\ttraining's rmse: 1.44545\tvalid_1's rmse: 1.54462\n",
      "fold n°3\n",
      "Training until validation scores don't improve for 200 rounds.\n",
      "[100]\ttraining's rmse: 1.59246\tvalid_1's rmse: 1.59516\n",
      "[200]\ttraining's rmse: 1.56159\tvalid_1's rmse: 1.56842\n",
      "[300]\ttraining's rmse: 1.54625\tvalid_1's rmse: 1.55746\n",
      "[400]\ttraining's rmse: 1.53626\tvalid_1's rmse: 1.55166\n",
      "[500]\ttraining's rmse: 1.52865\tvalid_1's rmse: 1.54807\n",
      "[600]\ttraining's rmse: 1.52249\tvalid_1's rmse: 1.54599\n",
      "[700]\ttraining's rmse: 1.51728\tvalid_1's rmse: 1.54488\n",
      "[800]\ttraining's rmse: 1.51254\tvalid_1's rmse: 1.54409\n",
      "[900]\ttraining's rmse: 1.5082\tvalid_1's rmse: 1.54368\n",
      "[1000]\ttraining's rmse: 1.50426\tvalid_1's rmse: 1.54323\n",
      "[1100]\ttraining's rmse: 1.50045\tvalid_1's rmse: 1.54299\n",
      "[1200]\ttraining's rmse: 1.4968\tvalid_1's rmse: 1.54286\n",
      "[1300]\ttraining's rmse: 1.49318\tvalid_1's rmse: 1.54265\n",
      "[1400]\ttraining's rmse: 1.48978\tvalid_1's rmse: 1.54256\n",
      "[1500]\ttraining's rmse: 1.48638\tvalid_1's rmse: 1.54241\n",
      "[1600]\ttraining's rmse: 1.48302\tvalid_1's rmse: 1.54232\n",
      "[1700]\ttraining's rmse: 1.47974\tvalid_1's rmse: 1.54222\n",
      "[1800]\ttraining's rmse: 1.47657\tvalid_1's rmse: 1.54223\n",
      "[1900]\ttraining's rmse: 1.4735\tvalid_1's rmse: 1.54217\n",
      "[2000]\ttraining's rmse: 1.47035\tvalid_1's rmse: 1.54204\n",
      "[2100]\ttraining's rmse: 1.46724\tvalid_1's rmse: 1.54202\n",
      "[2200]\ttraining's rmse: 1.46409\tvalid_1's rmse: 1.5419\n",
      "[2300]\ttraining's rmse: 1.46109\tvalid_1's rmse: 1.54182\n",
      "[2400]\ttraining's rmse: 1.45802\tvalid_1's rmse: 1.54176\n",
      "[2500]\ttraining's rmse: 1.45503\tvalid_1's rmse: 1.5418\n",
      "Early stopping, best iteration is:\n",
      "[2397]\ttraining's rmse: 1.4581\tvalid_1's rmse: 1.54175\n",
      "fold n°4\n",
      "Training until validation scores don't improve for 200 rounds.\n",
      "[100]\ttraining's rmse: 1.59561\tvalid_1's rmse: 1.58115\n",
      "[200]\ttraining's rmse: 1.56465\tvalid_1's rmse: 1.5557\n",
      "[300]\ttraining's rmse: 1.54937\tvalid_1's rmse: 1.54485\n",
      "[400]\ttraining's rmse: 1.53944\tvalid_1's rmse: 1.53897\n",
      "[500]\ttraining's rmse: 1.53196\tvalid_1's rmse: 1.53554\n",
      "[600]\ttraining's rmse: 1.52591\tvalid_1's rmse: 1.53356\n",
      "[700]\ttraining's rmse: 1.52072\tvalid_1's rmse: 1.5323\n",
      "[800]\ttraining's rmse: 1.51612\tvalid_1's rmse: 1.53154\n",
      "[900]\ttraining's rmse: 1.51194\tvalid_1's rmse: 1.53109\n",
      "[1000]\ttraining's rmse: 1.50788\tvalid_1's rmse: 1.53074\n",
      "[1100]\ttraining's rmse: 1.50405\tvalid_1's rmse: 1.53046\n",
      "[1200]\ttraining's rmse: 1.50032\tvalid_1's rmse: 1.53013\n",
      "[1300]\ttraining's rmse: 1.49666\tvalid_1's rmse: 1.52991\n",
      "[1400]\ttraining's rmse: 1.49305\tvalid_1's rmse: 1.52968\n",
      "[1500]\ttraining's rmse: 1.48975\tvalid_1's rmse: 1.52954\n",
      "[1600]\ttraining's rmse: 1.4864\tvalid_1's rmse: 1.52936\n",
      "[1700]\ttraining's rmse: 1.48309\tvalid_1's rmse: 1.52934\n",
      "[1800]\ttraining's rmse: 1.47982\tvalid_1's rmse: 1.52923\n",
      "[1900]\ttraining's rmse: 1.47657\tvalid_1's rmse: 1.5292\n",
      "[2000]\ttraining's rmse: 1.47333\tvalid_1's rmse: 1.52905\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2100]\ttraining's rmse: 1.47022\tvalid_1's rmse: 1.52895\n",
      "[2200]\ttraining's rmse: 1.46725\tvalid_1's rmse: 1.529\n",
      "[2300]\ttraining's rmse: 1.46424\tvalid_1's rmse: 1.52885\n",
      "[2400]\ttraining's rmse: 1.46125\tvalid_1's rmse: 1.52874\n",
      "[2500]\ttraining's rmse: 1.45826\tvalid_1's rmse: 1.52871\n",
      "[2600]\ttraining's rmse: 1.45523\tvalid_1's rmse: 1.52875\n",
      "[2700]\ttraining's rmse: 1.45225\tvalid_1's rmse: 1.52875\n",
      "Early stopping, best iteration is:\n",
      "[2522]\ttraining's rmse: 1.45755\tvalid_1's rmse: 1.5287\n",
      "CV score: 1.54092 \n"
     ]
    }
   ],
   "source": [
    "param = {'num_leaves': 31,\n",
    "         'min_data_in_leaf': 32, \n",
    "         'objective':'regression',\n",
    "         'max_depth': -1,\n",
    "         'learning_rate': 0.01, #default: 0.005 (3.66940)   /   0.005(3.67032), 0.01 (3.67152), 0.05 ()\n",
    "         \"min_child_samples\": 20,\n",
    "         \"boosting\": \"gbdt\",\n",
    "         \"feature_fraction\": 0.9,\n",
    "         \"bagging_freq\": 1,\n",
    "         \"bagging_fraction\": 0.9 ,\n",
    "         \"bagging_seed\": 11,\n",
    "         \"metric\": 'rmse',\n",
    "         \"lambda_l1\": 0.1,\n",
    "         \"nthread\": -1,\n",
    "         \"verbosity\": -1}\n",
    "\n",
    "normal_predictions = lgbm_regression_train(df_train, \n",
    "                                           df_target, \n",
    "                                           test, \n",
    "                                           param, \n",
    "                                           features, \n",
    "                                           categorical_feats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_without_outliers = pd.DataFrame({\"card_id\":test[\"card_id\"].values})\n",
    "model_without_outliers[\"target\"] = normal_predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Training Model For Outliers Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lgbm_classification_train(df_train, target, df_test, param, features, categorical_feats):\n",
    "    folds = KFold(n_splits=5, shuffle=True, random_state=15)\n",
    "    oof = np.zeros(len(df_train))\n",
    "    predictions = np.zeros(len(df_test))\n",
    "    feature_importance_df = pd.DataFrame()\n",
    "\n",
    "    start = time.time()\n",
    "    for fold_, (trn_idx, val_idx) in enumerate(folds.split(df_train.values, target.values)):\n",
    "        print(\"fold n°{}\".format(fold_))\n",
    "        trn_data = lgb.Dataset(df_train.iloc[trn_idx][features], label=target.iloc[trn_idx], categorical_feature=categorical_feats)\n",
    "        val_data = lgb.Dataset(df_train.iloc[val_idx][features], label=target.iloc[val_idx], categorical_feature=categorical_feats)\n",
    "\n",
    "        num_round = 10000\n",
    "        clf = lgb.train(param, trn_data, num_round, valid_sets = [trn_data, val_data], verbose_eval=100, early_stopping_rounds = 200)\n",
    "        oof[val_idx] = clf.predict(df_train.iloc[val_idx][features], num_iteration=clf.best_iteration)\n",
    "\n",
    "        fold_importance_df = pd.DataFrame()\n",
    "        fold_importance_df[\"feature\"] = features\n",
    "        fold_importance_df[\"importance\"] = clf.feature_importance()\n",
    "        fold_importance_df[\"fold\"] = fold_ + 1\n",
    "        feature_importance_df = pd.concat([feature_importance_df, fold_importance_df], axis=0)\n",
    "\n",
    "        predictions += clf.predict(df_test[features], num_iteration=clf.best_iteration) / folds.n_splits\n",
    "        \n",
    "    print(\"CV score: {:<8.5f}\".format(log_loss(target, oof)))\n",
    "\n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fold n°0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/frank/miniconda2/envs/python35/lib/python3.5/site-packages/lightgbm/basic.py:1158: UserWarning: Using categorical_feature in Dataset.\n",
      "  warnings.warn('Using categorical_feature in Dataset.')\n",
      "/home/frank/miniconda2/envs/python35/lib/python3.5/site-packages/lightgbm/basic.py:725: UserWarning: categorical_feature in param dict is overridden.\n",
      "  warnings.warn('categorical_feature in param dict is overridden.')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 200 rounds.\n",
      "[100]\ttraining's binary_logloss: 0.0462551\tvalid_1's binary_logloss: 0.048348\n",
      "[200]\ttraining's binary_logloss: 0.046259\tvalid_1's binary_logloss: 0.048371\n",
      "Early stopping, best iteration is:\n",
      "[4]\ttraining's binary_logloss: 0.0465531\tvalid_1's binary_logloss: 0.0482194\n",
      "fold n°1\n",
      "Training until validation scores don't improve for 200 rounds.\n",
      "[100]\ttraining's binary_logloss: 0.0467073\tvalid_1's binary_logloss: 0.0471337\n",
      "[200]\ttraining's binary_logloss: 0.0466989\tvalid_1's binary_logloss: 0.0471148\n",
      "Early stopping, best iteration is:\n",
      "[47]\ttraining's binary_logloss: 0.04661\tvalid_1's binary_logloss: 0.0471143\n",
      "fold n°2\n",
      "Training until validation scores don't improve for 200 rounds.\n",
      "[100]\ttraining's binary_logloss: 0.0465446\tvalid_1's binary_logloss: 0.0466949\n",
      "[200]\ttraining's binary_logloss: 0.0465202\tvalid_1's binary_logloss: 0.0466656\n",
      "Early stopping, best iteration is:\n",
      "[47]\ttraining's binary_logloss: 0.0465115\tvalid_1's binary_logloss: 0.0466884\n",
      "fold n°3\n",
      "Training until validation scores don't improve for 200 rounds.\n",
      "[100]\ttraining's binary_logloss: 0.0455343\tvalid_1's binary_logloss: 0.0516146\n",
      "[200]\ttraining's binary_logloss: 0.0455053\tvalid_1's binary_logloss: 0.0515663\n",
      "Early stopping, best iteration is:\n",
      "[47]\ttraining's binary_logloss: 0.0454667\tvalid_1's binary_logloss: 0.0516315\n",
      "fold n°4\n",
      "Training until validation scores don't improve for 200 rounds.\n",
      "[100]\ttraining's binary_logloss: 0.046385\tvalid_1's binary_logloss: 0.0469847\n",
      "[200]\ttraining's binary_logloss: 0.0463759\tvalid_1's binary_logloss: 0.0470099\n",
      "Early stopping, best iteration is:\n",
      "[47]\ttraining's binary_logloss: 0.0463362\tvalid_1's binary_logloss: 0.0468652\n",
      "CV score: 0.04810 \n"
     ]
    }
   ],
   "source": [
    "param = {'num_leaves': 31,\n",
    "         'min_data_in_leaf': 30, \n",
    "         'objective':'binary',\n",
    "         'max_depth': 6,\n",
    "         'learning_rate': 0.01,\n",
    "         \"boosting\": \"rf\",\n",
    "         \"feature_fraction\": 0.9,\n",
    "         \"bagging_freq\": 1,\n",
    "         \"bagging_fraction\": 0.9 ,\n",
    "         \"bagging_seed\": 11,\n",
    "         \"metric\": 'binary_logloss',\n",
    "         \"lambda_l1\": 0.1,\n",
    "         \"verbosity\": -1,\n",
    "         \"random_state\": 2333}\n",
    "\n",
    "with open('../input/train_test_target_with_target.pkl', 'rb') as f:\n",
    "    [train, target, test, normal_idx, outlier_idx] = pickle.load(f)\n",
    "\n",
    "target = train['outliers']\n",
    "del train['outliers']\n",
    "features = [c for c in train.columns if c not in ['card_id', 'first_active_month']]\n",
    "categorical_feats = [c for c in features if 'feature_' in c]\n",
    "outlier_label = lgbm_classification_train(train, target, test, param, features, categorical_feats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>card_id</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>C_ID_0ab67a22ab</td>\n",
       "      <td>0.022946</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>C_ID_130fd0cbdd</td>\n",
       "      <td>0.001735</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>C_ID_b709037bc5</td>\n",
       "      <td>0.010461</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>C_ID_d27d835a9f</td>\n",
       "      <td>0.001735</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>C_ID_2b5e3df5c2</td>\n",
       "      <td>0.001735</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           card_id    target\n",
       "0  C_ID_0ab67a22ab  0.022946\n",
       "1  C_ID_130fd0cbdd  0.001735\n",
       "2  C_ID_b709037bc5  0.010461\n",
       "3  C_ID_d27d835a9f  0.001735\n",
       "4  C_ID_2b5e3df5c2  0.001735"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_outlier_prob = pd.DataFrame({\"card_id\":test[\"card_id\"].values})\n",
    "df_outlier_prob[\"target\"] = outlier_label\n",
    "df_outlier_prob.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Combining Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "outlier_id = pd.DataFrame(\\\n",
    "                          df_outlier_prob.sort_values(by='target',\n",
    "                                                      ascending = False)\n",
    "                          .head(25000)['card_id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_submission = pd.read_csv('../result/Blend2_v2.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "123623\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>card_id</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>C_ID_0ab67a22ab</td>\n",
       "      <td>-2.346967</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>C_ID_130fd0cbdd</td>\n",
       "      <td>-0.354020</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>C_ID_b709037bc5</td>\n",
       "      <td>-0.932773</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>C_ID_d27d835a9f</td>\n",
       "      <td>-0.148607</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>C_ID_2b5e3df5c2</td>\n",
       "      <td>-1.090599</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           card_id    target\n",
       "0  C_ID_0ab67a22ab -2.346967\n",
       "1  C_ID_130fd0cbdd -0.354020\n",
       "2  C_ID_b709037bc5 -0.932773\n",
       "3  C_ID_d27d835a9f -0.148607\n",
       "4  C_ID_2b5e3df5c2 -1.090599"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(best_submission.shape[0])\n",
    "best_submission.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25000\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>card_id</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>C_ID_0ab67a22ab</td>\n",
       "      <td>-2.346967</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>C_ID_6d8dba8475</td>\n",
       "      <td>-0.881375</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>C_ID_4859ac9ed5</td>\n",
       "      <td>-0.641561</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>C_ID_7f1041e8e1</td>\n",
       "      <td>-5.193301</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>C_ID_22e4a47c72</td>\n",
       "      <td>0.341024</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           card_id    target\n",
       "0  C_ID_0ab67a22ab -2.346967\n",
       "1  C_ID_6d8dba8475 -0.881375\n",
       "2  C_ID_4859ac9ed5 -0.641561\n",
       "3  C_ID_7f1041e8e1 -5.193301\n",
       "4  C_ID_22e4a47c72  0.341024"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(outlier_id.shape[0])\n",
    "most_likely_liers = best_submission.merge(outlier_id,how='right')\n",
    "most_likely_liers.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 4min 19s, sys: 36.1 ms, total: 4min 19s\n",
      "Wall time: 4min 19s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "for card_id in most_likely_liers['card_id']:\n",
    "    model_without_outliers.loc[model_without_outliers['card_id']==card_id,'target']\\\n",
    "    = most_likely_liers.loc[most_likely_liers['card_id']==card_id,'target'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_without_outliers.to_csv(\"../result/Blend2_v3.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Result from `Blend2_v3.csv` is 3.692 (lower than v2 result). Not cool."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "0b926c1b3a703d8ed6c7c609e7c892b503e3c511"
   },
   "source": [
    "## LightGBM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "28ac4d7abc8117671e8e79caaabcbf41dbb328c0"
   },
   "outputs": [],
   "source": [
    "param = {'num_leaves': 31,\n",
    "         'min_data_in_leaf': 32, \n",
    "         'objective':'regression',\n",
    "         'max_depth': -1,\n",
    "         'learning_rate': 0.01, #default: 0.005 (3.66940)   /   0.005(3.67032), 0.01 (3.67152), 0.05 ()\n",
    "         \"min_child_samples\": 20,\n",
    "         \"boosting\": \"gbdt\",\n",
    "         \"feature_fraction\": 0.9,\n",
    "         \"bagging_freq\": 1,\n",
    "         \"bagging_fraction\": 0.9 ,\n",
    "         \"bagging_seed\": 11,\n",
    "         \"metric\": 'rmse',\n",
    "         \"lambda_l1\": 0.1,\n",
    "         \"nthread\": -1,\n",
    "         \"verbosity\": -1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "92bf1d3418327cb759937b8f32f40d75e73108e9",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "folds = KFold(n_splits=5, shuffle=True, random_state=15)\n",
    "oof = np.zeros(len(train))\n",
    "predictions = np.zeros(len(test))\n",
    "start = time.time()\n",
    "feature_importance_df = pd.DataFrame()\n",
    "\n",
    "for fold_, (trn_idx, val_idx) in enumerate(folds.split(train.values, target.values)):\n",
    "    print(\"fold n°{}\".format(fold_))\n",
    "    trn_data = lgb.Dataset(train.iloc[trn_idx][features], label=target.iloc[trn_idx], categorical_feature=categorical_feats)\n",
    "    val_data = lgb.Dataset(train.iloc[val_idx][features], label=target.iloc[val_idx], categorical_feature=categorical_feats)\n",
    "\n",
    "    num_round = 10000\n",
    "    clf = lgb.train(param, trn_data, num_round, valid_sets = [trn_data, val_data], verbose_eval=100, early_stopping_rounds = 200)\n",
    "    oof[val_idx] = clf.predict(train.iloc[val_idx][features], num_iteration=clf.best_iteration)\n",
    "    \n",
    "    fold_importance_df = pd.DataFrame()\n",
    "    fold_importance_df[\"feature\"] = features\n",
    "    fold_importance_df[\"importance\"] = clf.feature_importance()\n",
    "    fold_importance_df[\"fold\"] = fold_ + 1\n",
    "    feature_importance_df = pd.concat([feature_importance_df, fold_importance_df], axis=0)\n",
    "    \n",
    "    predictions += clf.predict(test[features], num_iteration=clf.best_iteration) / folds.n_splits\n",
    "\n",
    "print(\"CV score: {:<8.5f}\".format(mean_squared_error(oof, target)**0.5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "98eb235d712c857ebc3dc3fff312e049c811267a",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "cols = (feature_importance_df[[\"feature\", \"importance\"]]\n",
    "        .groupby(\"feature\")\n",
    "        .mean()\n",
    "        .sort_values(by=\"importance\", ascending=False)[:1000].index)\n",
    "\n",
    "best_features = feature_importance_df.loc[feature_importance_df.feature.isin(cols)]\n",
    "\n",
    "plt.figure(figsize=(8,50))\n",
    "sns.barplot(x=\"importance\",\n",
    "            y=\"feature\",\n",
    "            data=best_features.sort_values(by=\"importance\",\n",
    "                                           ascending=False))\n",
    "plt.title('LightGBM Features (avg over folds)')\n",
    "plt.tight_layout()\n",
    "plt.savefig('lgbm_importances.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "acdaccc5c862736b097ae9e5bcdcdae5083b5d33"
   },
   "source": [
    "## LightGBM-1 with Repeated kfold approach\n",
    "\n",
    "#### RepeatedKFold repeats K-Fold n times. It can be used when one requires to run KFold n times, producing different splits in each repetition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "91043a8b0a7b1d335672f35d39f1621b02befc88"
   },
   "outputs": [],
   "source": [
    "lgbparam = {'num_leaves': 31,\n",
    "            'boosting_type': 'rf',\n",
    "             'min_data_in_leaf': 30, \n",
    "             'objective':'regression',\n",
    "             'max_depth': -1,\n",
    "             'learning_rate': 0.005,\n",
    "             \"min_child_samples\": 20,\n",
    "             \"boosting\": \"gbdt\",\n",
    "             \"feature_fraction\": 0.9,\n",
    "             \"bagging_freq\": 1,\n",
    "             \"bagging_fraction\": 0.9 ,\n",
    "             \"bagging_seed\": 11,\n",
    "             \"metric\": 'rmse',\n",
    "             \"lambda_l1\": 0.1,\n",
    "             \"verbosity\": -1,\n",
    "             \"nthread\": 4,\n",
    "             \"random_state\": 4590}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "17b17b5d967ccf2912d9f7b229cd06cb74819d85",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import RepeatedKFold\n",
    "folds = RepeatedKFold(n_splits=5, n_repeats=2, random_state=4520)\n",
    "\n",
    "oof_lgb = np.zeros(len(train))\n",
    "predictions_lgb = np.zeros(len(test))\n",
    "start = time.time()\n",
    "feature_importance_df = pd.DataFrame()\n",
    "\n",
    "for fold_, (trn_idx, val_idx) in enumerate(folds.split(train.values, target.values)):\n",
    "    print(\"fold n°{}\".format(fold_))\n",
    "    trn_data = lgb.Dataset(train.iloc[trn_idx][features], label=target.iloc[trn_idx], categorical_feature=categorical_feats)\n",
    "    val_data = lgb.Dataset(train.iloc[val_idx][features], label=target.iloc[val_idx], categorical_feature=categorical_feats)\n",
    "\n",
    "    num_round = 11000\n",
    "    clf = lgb.train(lgbparam, trn_data, num_round, valid_sets = [trn_data, val_data], verbose_eval=100, early_stopping_rounds = 100)\n",
    "    oof_lgb[val_idx] = clf.predict(train.iloc[val_idx][features], num_iteration=clf.best_iteration)\n",
    "    \n",
    "    fold_importance_df = pd.DataFrame()\n",
    "    fold_importance_df[\"feature\"] = features\n",
    "    fold_importance_df[\"importance\"] = clf.feature_importance()\n",
    "    fold_importance_df[\"fold\"] = fold_ + 1\n",
    "    feature_importance_df = pd.concat([feature_importance_df, fold_importance_df], axis=0)\n",
    "    \n",
    "    predictions_lgb += clf.predict(test[features], num_iteration=clf.best_iteration) / (5 * 2)\n",
    "\n",
    "print(\"CV score: {:<8.5f}\".format(mean_squared_error(oof_lgb, target)**0.5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "b84242fc4c57f96b36b860d5728cddaf3b8c2943",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "cols = (feature_importance_df[[\"feature\", \"importance\"]]\n",
    "        .groupby(\"feature\")\n",
    "        .mean()\n",
    "        .sort_values(by=\"importance\", ascending=False)[:1000].index)\n",
    "\n",
    "best_features = feature_importance_df.loc[feature_importance_df.feature.isin(cols)]\n",
    "\n",
    "plt.figure(figsize=(14,50))\n",
    "sns.barplot(x=\"importance\",\n",
    "            y=\"feature\",\n",
    "            data=best_features.sort_values(by=\"importance\",\n",
    "                                           ascending=False))\n",
    "plt.title('LightGBM Features (avg over folds)')\n",
    "plt.tight_layout()\n",
    "plt.savefig('../img/lgbm_importances.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_features.sort_values(by=\"importance\", ascending=False)\n",
    "feat_list = best_features[\"feature\"].loc[best_features[\"importance\"] > 0]\n",
    "\n",
    "with open('../input/feat_list.pkl', 'wb') as f:\n",
    "    pickle.dump([feat_list], f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "0362877a4fb45006383dc3bc03b5642aba0f47dc"
   },
   "outputs": [],
   "source": [
    "sub_df = pd.DataFrame({\"card_id\":test[\"card_id\"].values})\n",
    "sub_df[\"target\"] = predictions\n",
    "sub_df.to_csv(\"../result/submit_lgb.csv\", index=False)\n",
    "\n",
    "sub_df1 = pd.DataFrame({\"card_id\":test[\"card_id\"].values})\n",
    "sub_df1[\"target\"] = predictions_lgb\n",
    "sub_df1.to_csv(\"../result/submit_lgb1.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "2dfa7a5fcabd78829678fa4283e99862f1c4a0d7"
   },
   "source": [
    "## Stacking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "8c9afce8b59b5f51024150a997a9d2eb702ab85d"
   },
   "outputs": [],
   "source": [
    "train_stack = np.vstack([oof,oof_lgb]).transpose()\n",
    "test_stack = np.vstack([predictions,predictions_lgb]).transpose()\n",
    "\n",
    "folds = RepeatedKFold(n_splits=5,n_repeats=1,random_state=4520)\n",
    "oof_stack = np.zeros(train_stack.shape[0])\n",
    "predictions_stack = np.zeros(test_stack.shape[0])\n",
    "\n",
    "for fold_, (trn_idx, val_idx) in enumerate(folds.split(train_stack, target)):\n",
    "    print(\"fold n°{}\".format(fold_))\n",
    "    trn_data, trn_y = train_stack[trn_idx], target.iloc[trn_idx].values\n",
    "    val_data, val_y = train_stack[val_idx], target.iloc[val_idx].values\n",
    "\n",
    "    print(\"-\" * 10 + \"Stacking \" + str(fold_) + \"-\" * 10)\n",
    "#     cb_model = CatBoostRegressor(iterations=3000, learning_rate=0.1, depth=8, l2_leaf_reg=20, bootstrap_type='Bernoulli',  eval_metric='RMSE', metric_period=50, od_type='Iter', od_wait=45, random_seed=17, allow_writing_files=False)\n",
    "#     cb_model.fit(trn_data, trn_y, eval_set=(val_data, val_y), cat_features=[], use_best_model=True, verbose=True)\n",
    "    clf = BayesianRidge()\n",
    "    clf.fit(trn_data, trn_y)\n",
    "    \n",
    "    oof_stack[val_idx] = clf.predict(val_data)\n",
    "    predictions_stack += clf.predict(test_stack) / 5\n",
    "\n",
    "\n",
    "np.sqrt(mean_squared_error(target.values, oof_stack))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "e871862de58a3ce43c148f78f152c1d6385a0a08"
   },
   "outputs": [],
   "source": [
    "sample_submission = pd.read_csv('../input/sample_submission.csv')\n",
    "sample_submission['target'] = predictions_stack\n",
    "sample_submission.to_csv('../result/Bayesian_Ridge_Stacking.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "b65e47686b49b31f0dc47171ab2aece4a4d0a941"
   },
   "outputs": [],
   "source": [
    "sample_submission = pd.read_csv('../input/sample_submission.csv')\n",
    "sample1 = pd.read_csv(\"../result/3.695.csv\")\n",
    "sample2 = pd.read_csv(\"../result/combining_submission (1).csv\")\n",
    "sample_submission['target'] = predictions * 0.5 + predictions_lgb * 0.5\n",
    "sample_submission.to_csv(\"../result/Blend1.csv\", index = False)\n",
    "sample_submission['target'] = sample_submission['target'] * 0.2 + sample1['target'] * 0.2 + sample2['target'] * 0.6\n",
    "sample_submission.to_csv('../result/Blend2.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "oldHeight": 122.4,
   "position": {
    "height": "40px",
    "left": "1259px",
    "right": "20px",
    "top": "14px",
    "width": "250px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "varInspector_section_display": "none",
   "window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
